{"hash":"84cf81435dbe3789963c971487c433de70805743","data":{"thisPage":{"id":"fab130dfedfaab931b1fa670a960c27f","name":"OperationSeven","mockData":"{\"실습 스크립트\":\"Frozen\"}","source":"<template>\n    <div>\n        <mark-down class=\"content\" source='\n## 실습 스크립트 - 컨테이너 오케스트레이션 Lab. Guide \n\n### 구글 (Google) Cloud\n<details>\n<summary>Google Cloud Platform 기반의 Container Orchestration Lab. Scripts</summary>\n<p>\n\n<details>\n  <summary><b>자주 사용되는 GCP Cloud 명령어</b></summary>\n\n  - <b>Referencing URL: <a href=\"https://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters?hl=ko\" target=\"_blank\">https://cloud.google.com/kubernetes-engine/docs/how-to/managing-clusters?hl=ko</a></b>\n  - <b>GCP 인증</b>\n    `gcloud init`\n\n  - <b>GCP 클러스터 생성</b>\n    `gcloud container clusters create (Cluster Name) --zone asia-northeast1-a --enable-cloud-logging --enable-cloud-monitoring --subnetwork default`\n\n  - <b>GCP 클러스터 토큰 가져오기</b>\n    `gcloud container clusters get-credentials (Cluster Name) --zone asia-northeast1-a --project (PROJECT-NAME)`\n\n  - <b>GCP GCR(Google Container Registry) 인증</b>\n    `gcloud auth configure-docker`\n  - <b>GCR 레지스트리에 이미지 Push</b>\n    `docker push gcr.io/PROJECT-NAME/IMAGE-NAME:v1`\n    - (docker  version 18.03 이하)\n    `gcloud docker --push gcr.io/PROJECT-NAME/IMAGE-NAME:v1`\n    - 오류(unknown flag: --password-stdin) 발생 시,\n      `docker login --username AWS -p $(aws ecr get-login-password --region (Region-Code)) (Account-Id).dkr.ecr.(Region-Code).amazonaws.com/`\n  - <b>GCP 클러스터 삭제</b>\n    `gcloud container clusters delete [CLUSTER_NAME]`\n\n    \n</details> \n    \n### Docker\n\n<details>\n<summary><b>Docker Hands-on</b></summary>\n\n  - <b>Lab. image</b>\n    - 이미지 Pull\n      - docker pull hello-world\n      - docker images \n      - docker pull nginx\n      - docker pull nginx:latest\n      - docker pull docker.io/library/nginx:latest\n      - docker pull nginx:1.16.1\n      - docker images\n    - 도커허브 (Docker Hub, 가입) \n      - <a href=\"http://hub.docker.com\" target=\"_blank\">접속 후, nginx 검색</a>\n    - 이미지 Tagging\n      `docker image tag nginx my-nginx # Create 태그`\n    - 이미지 삭제\n      - docker image rm my-nginx\n      - docker image rm hello-world\n      - docker image rm $(docker images -q) # 한번에 모든 도커 이미지 지우기\n      \n  - <b>Lab. container</b>\n    - 컨테이너 생성\n      - docker run hello-world # 컨테이너 만들기\n      - docker run --name hello hello-world # 이름 지정, 미지정시 임의의 이름으로 생성\n      - docker run --name my-nginx -d -p 8080:80 nginx\n      - docker ps\n    - 컨테이너 시작/종료\n      - docker stop my-nginx\n      - docker start my-nginx\n    - 컨테이너 포트 노출\n      - http://localhost 에서 nginx index.html 확인\n      - docker container rm my-nginx\n      - docker run --name my-nginx -d -p 8080:80 nginx\n      - http://localhost:8080 에서 nginx index.html 확인\n    - 컨테이너 접근\n      - docker exec my-nginx cat /usr/share/nginx/html/index.html #실행 중 컨테이너 접근\n      - docker exec -i -t  my-nginx /bin/bash\n        - apt-get update\n        - apt-get install curl\n        - curl localhost\n        - exit\n    - 컨테이너 삭제\n      - docker container rm my-nginx # 실행 중 컨테이너  삭제 시, 오류\n      - docker container rm $(docker ps -a -q) # 한번에 모든 컨테이너 지우기\n  - <b>Lab. Docker Build & Push</b>\n    - Dockerfile 로부터 이미지 생성\n      - Dockerfile & 리소스 생성\n        - mkdir Dockerfile\n        - cd Dockerfile\n        - nano index.html\n          <pre style=\"white-space: pre-wrap\">\"Hi~ My name is Park Yong Joo..\"</pre>\n        - 저장 및 종료 (Ctrl + X, y 입력 후 엔터)\n        - nano Dockerfile\n          <pre style=\"white-space: pre-wrap\">\n          FROM nginx\n          COPY index.html /usr/share/nginx/html/\n          </pre>\n        - 저장 및 종료 (Ctrl + x, Y 입력 후 엔터)\n      - 도커라이징 & Docker Hub에 Push\n        - docker build -t (Docker-ID)/my-nginx . \n        - docker images\n        - docker push (Docker-ID)/my-nginx\n          \"denied: 권한오류 생성 시, docker login 명령으로 Docker Hub에 로그인해 준다.\"\n      - http://hub.docker.com 에서 이미지 확인\n      - Docker Hub 이미지로부터 컨테이너 실행\n        - docker run --name new-nginx -d -p 8081:80 (Docker-ID)/my-nginx\n      - Browser에서 실행 애플리케이션 확인\n        - http://localhost:8081\n  - <b>Clear</b>\n    - docker container rm $(docker ps -a -q)\n      - container 삭제 전, 실행 중인 컨테이너를 정지시켜 준다.\n      - docker container stop new-nginx\n    - docker image rm -f $(docker images -q)\n    \n</details> \n\n### Kubernetes\n\n<details>\n<summary><b>Kubernetes Hands-on</b></summary>\n\n- <b>Cloud Shell에서 Docker hub 토큰 생성 (docker hub의 접속정보 입력)</b>\n  - docker login \n- <b>Kubernetes Hands-on</b>\n  - Lab에 필요한 리소스 내려받기\n    - git clone https://github.com/acmexii/mall.git\n    - git clone https://github.com/event-storming/container-orchestration.git\n    - cd container-orchestration\n    - cd yaml\n  - Lab. K8s Sample App 생성\n    - 어플리케이션 생성/ 확인\n      - Docker hub에 올린 이미지를 통한 컨테이너 생성\n        <pre style=\"white-space: pre-wrap\">kubectl create deploy my-nginx --image=apexacme/my-nginx:v1\"</pre>\n      - 클러스터 외부에 노출하기\n        <pre style=\"white-space: pre-wrap\">kubectl expose deploy my-nginx --type=LoadBalancer --port=80\"</pre>\n    - 서비스 확인하기\n      - kubectl get svc의 EXTERNAL-IP 복사\n      - Browser에서 EXTERNAL-IP:80 접속\n\n  - Lab. Pod & 기본명령\n    - kubectl get nodes\n      - 쿠버네티스에 제대로 접속했는지 확인\n      - 현 클러스터의 워크노드를 리스트업\n      - 접속 결과 안나오는 경우\n        - kubectl config current-context 명령으로 Cluster 접속 확인\n    - 객체의 검색\n      - kubectl get [object type]\n      - kubectl get pods   # pods = pod = po\n      - kubectl get deployments   # deploy\n      - kubectl get services    # svc\n      - kubectl get replicaset    # rs\n    - 객체의 모니터링\n      - watch kubectl get all\n      - kubectl get pod -w\n      - watch kubectl get pod\n    - 객체의 유형\n      - Service \n        - types\n          - LoadBalancer\n            - 클라우드 제공자에 의해 제공된 Loadbalancer 로 노출\n            - front-end 혹은 ingress (api gateway)\n          - ClusterIP(default) / NodePort\n            - 클러스터 내부 IP\n            - 내부 마이크로 서비스\n      - Deployment\n        - ReplicaSet (하나이상 생성)\n          - Pod (하나이상 생성)\n            - Container (docker ) 하나이상.\n      - Pod\n      - ReplicaSet\n      - Ingress\n      - Secret\n      - ConfigMap\n      - ServiceAccount = sa\n      - statefulset\n      - daemonset\n    - 설정파일(YAML)을 통한 Pod 배포 (직접 타이핑)\n      - nano declarative-pod.yaml\n        <pre style=\"white-space: pre-wrap\">\n        apiVersion: v1\n        kind: Pod\n        metadata:\n          name: declarative-pod\n          labels:\n            env: test\n        spec:\n          containers:\n          - name: my-first-\n            image: nginx\n        </pre>\n      - 저장 및 종료 (ctrl + X, Y, 엔터)\n      - kubectl create -f declarative-pod.yaml\n      - kubectl get pods\n    - 원하는 Node 타입에 Pod 생성\n      - pwd 명령으로 현 위치가 /container-orchestration/yaml/pod 인지 확인\n      - kubectl create -f pod-with-nodeselector.yaml\n      - kubectl get po -o wide\n        - Pod가 찾는 노드가 없어 pending 상태\n      - 노드에 라벨 추가\n        - kubectl label nodes [your-node-name] disktype=ssd\n        - kubectl get nodes --show-labels | grep ssd\n      - kubectl get po -o wide\n    - 생성된 Pod 및 오브젝트 삭제\n      - kubectl delete pod [pod명]\n      - kubectl delete service,deploy --all\n  - Lab. Label\n    - kubectl run nginx --image=nginx\n    - kubectl get pods -l app=nginx\n    - kubectl get pods --selector app=nginx\n    - <pre style=\"white-space: pre-wrap\">kubectl get pods --selector \"app in (nginx, test)\"</pre>\n  - Lab. ReplicaSet\n    - pwd 로 현 위치가 /container-orchestration/yaml/replicaset 인지 확인\n    - kubectl create -f replicaset.yaml\n    - kubectl get all\n    - #replica 개수 조정\n      - kubectl scale replicaset/frontend --replicas=5\n      - kubectl get po\n  - Lab. Deployment & 기본명령\n    - 기본 nginx 서버의 배포\n    - kubectl create deploy nginx --image=nginx\n    - kubectl get deploy nginx\n    - kubectl get replicaset -l app=nginx\n    - kubectl get po -l app=nginx  # \"-l\" 옵션은 label의 key/value 로 객체를 필터링\n    - kubectl get pods --selector app=nginx\n    - <pre style=\"white-space: pre-wrap\">kubectl get pods --selector \"app in (nginx, test)\"</pre>\n    - kubectl describe po (검색한 pod name)\n    - (pod 제거)\n    - kubectl delete po --all   #\n    - (pod 를 제거해도 재생됨을 확인)\n    - kubectl get po\n    - (scale out)\n    - kubectl scale deploy nginx --replicas=3\n    - kubectl get po   # pod 개수가 3개로 늘어남을 확인\n    - kubectl delete po --all   # pod 를 모두 지움\n    - kubectl get po   # pod 를 모두 지워도 결국 3개로 복원됨을 확인\n    - ( 제거하기 위해서는 deployment 를 제거해야만 함)\n    - kubectl delete deploy nginx\n  - Lab. Rollout & Back\n    - (pwd 로 현 위치가 /container-orchestration/yaml/ 인지 확인)\n    - kubectl create -f nginx.yaml\n    - (배포주석 확인, 주석이 없을 경우 아래 명령으로 주석 추가( Rollback시, 필요) )\n    - <pre style=\"white-space: pre-wrap\">kubectl annotate deploy nginx-deployment kubernetes.io/change-cause=\"v1 is nginx:1.7.9\"</pre>\n    - (설정의 변경)\n    - 파일을 편집기에서 열어(nano nginx.yaml) spec > replicas 부분을 3-->5 로 수정\n    - kubectl apply -f nginx.yaml\n    - (스케일 아웃 결과 확인)\n    - kubectl get deploy     # DESIRED :3 --> 5 로 수정되고 곧 ACTIVE 도 5가 됨\n    - (무정지 버전업)\n    - 파일을 편집기에 열어서(nano nginx.yaml) spec > template > image 부분을 nginx:1.7.9 -> nginx:1.9.1 로 수정\n    - kubectl apply -f nginx.yaml\n    - (배포주석 달기)\n    - <pre style=\"white-space: pre-wrap\">kubectl annotate deploy nginx-deployment kubernetes.io/change-cause=\"v2 is nginx:1.9.1\"</pre>\n    - (결과확인)\n    - kubectl describe po <해당 deployment 의 pod 중 하나의 이름>    # 내용의 image 부분이 1.791 인지 확인\n    - (무정지 재배포 히스토리 확인)\n    - kubectl rollout history deploy nginx-deployment\n    - (다음과 같이 출력됨을 확인)\n      \"REVISION  CHANGE-CAUSE\n      1         v1 is nginx:1.7.9\n      2         v2 is nginx:1.9.1\"\n    - (롤백하기)\n    - kubectl rollout undo deploy nginx-deployment\n    - kubectl rollout undo deployment nginx --to-revision 1\n  - Lab. Service\n    - kubectl delete service,deploy --all  # 기존 이력 삭제\n    - (다시 생성)\n    - kubectl create deploy nginx --image=nginx\n    - (서비스로 노출)\n    - <pre style=\"white-space: pre-wrap\">kubectl expose deploy nginx --type=\"LoadBalancer\" --port=80</pre>\n    - (웹 브라우저를 열고 생성된 external ip 로 접속, Nginx welcome 메시지 확인)\n    - kubectl exec -it (pod name) --/bin/bash   # 생성된 nginx 서버 linux 의 shell 에 접근\n  - Auto Scale-Out\n    - pwd 로 현 위치가 /container-orchestration/yaml/ 인지 확인\n    - (모든 객체 지우기)\n    - kubectl delete deploy,service,pod --all\n    - (대상 서비스 배포 및 모니터링)\n    - kubectl apply -f https://k8s.io/examples/application/php-apache.yaml\n      - NOTE : 서비스가 Auto Scaling되기 위해서는 컨테이너 Spec에 Resources : 설정이 있어야 함\n        \" resources:\n            limits:\n              cpu: 500m\n            requests:\n              cpu: 200m\"\n    - (오토 스케일링 설정, hpa: HorizontalPodAutoscaler )\n      - kubectl autoscale deployment php-apache --cpu-percent=20 --min=1 --max=10\n        cpu-percent=50 : Pod 들의 요청 대비 평균 CPU 사용율 (여기서는  요청이 200 milli-cores이므로, 모든 Pod의 평균 CPU 사용율이 100 milli-cores(50%)를 넘게되면 HPA 발생)\"\n      - kubectl get hpa php-apache -o yaml\n    - 로드 제너레이터(siege)가 설치된 컨테이너 생성\n      - cat siege.yaml\n      - kubectl create -f siege.yaml\n    - 로드 생성\n      - siege -c30 -t30S -v http://php-apache\n    - (오토 스케일링이 되지 않을 때 :  kubectl get hpa의 TARGETS 부분에 cpu 사용률이 <unknown>으로 출력될 때)\n      - metrics-server가 제대로 실행중인지 kubectl top pods 명령으로 포드 cpu 사용률이 모니터링 되는지 확인\n      - 디플로이먼트의 컨테이너 옵션에 cpu request 옵션이 제대로 걸려 있는지 확인\n      - cpu request옵션이 없으면 hpa가 cpu사용량에 필요한 계산을 할 수 없음\n  - Lab. Volume\n    - (pwd 로 현 위치가 /container-orchestration/yaml/volume 인지 확인)\n    - (emptyDir 마운트)\n    - kubectl create -f volume-emptydir.yaml\n      - kubectl exec -it shared-volumes --container redis --/bin/bash\n      - cd /data/shared\n      - echo test… > test.txt\n      - exit\n      - kubectl exec -it shared-volumes --container nginx --/bin/bash\n      - cd /data/shared\n      - ls\n    - (GitRepository를 볼륨으로 마운트)\n    - kubectl create -f volume-gitrepo.yaml\n    - (PersistentVolumeClaim 생성)\n    - kubectl create -f volume-pvc.yaml\n    - kubectl get pvc\n    - kubectl describe pvc azure-managed-disk\n    - (생성된 PersistentVolueClaim으로 Pod 생성하기)\n    - kubectl create -f pod-with-pvc.yaml\n    - kubectl describe pod mypod\n    - (포드 접속)\n    - kubectl exec -it mypod --/bin/bash\n    - (마운트 및 사이즈 확인)\n    - df -k\n  - Lab. ConfigMap\n    - (pwd 로 현 위치가 /container-orchestration/yaml/configmap/ 인지 확인)\n    - (컨피그 맵 생성)\n    - kubectl create configmap hello-cm --from-literal=language=java\n    - kubectl get cm\n    - kubectl get cm hello-cm -o yaml\n    - 클라우드에서 배포 이미지 확인\n    - nano cm-deployment.yaml 파일 편집(나의 Registry명으로 수정)\n    - (배포 및 서비스 생성)\n      - kubectl create -f cm-deployment.yaml\n      - kubectl create -f cm-service.yaml\n    - (서비스 확인)\n      - Service의 External-IP 접속\n  - Lab. Secret\n    - (pwd 로 현 위치가 /container-orchestration/yaml/secret/ 인지 확인)\n    - (Pod에서 Secret 파일 마운트 사용하기 내용을 참고하여 배포 및 서비스 확인해 보기)\n  - Lab. Liveness & Readiness Probe\n    - (pwd 로 현 위치가 /container-orchestration/yaml/liveness/ 인지 확인)\n    - (Liveness Command Probe 실습)\n      - kubectl create -f exec-liveness.yaml\n      - (컨테이너가 Running 상태로 보이나, Liveness Probe 실패로 계속 재시작)\n      - (kubectl describe로 실패 메시지 확인)\n      - kubectl describe po liveness-exec\n    - (Liveness HTTP Probe 실습)\n      - kubectl create -f http-liveness.yaml\n      - (kubectl describe로 실패 메시지 확인)\n      - kubectl describe po liveness-http\n    - (Liveness 와 readiness probe 동시 적용 실습)\n      - kubectl create -f tcp-liveness-readiness.yaml\n      - (8080포트에 대해 정상적으로 Liveness 와 readiness Probe를 통과해 서비스가 실행됨)\n      - kubectl describe po goproxy\n  - Lab. Ingress\n    - Helm 명령으로 설치 여부 확인\n    - Helm 이 설치되어 있지 않은 경우, Helm(패키지 인스톨러) 설치\n      - Helm 3.x 설치(권장)\n        - curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 > get_helm.sh\n        - chmod 700 get_helm.sh\n        - ./get_helm.sh\n      - Helm 2.x 설치\n        - curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash\n        - (설치 중, sudo를 위한 비밀번호 입력)\n        - kubectl --namespace kube-system create sa tiller\n        - kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller\n        - helm init --service-account tiller\n    - Helm으로 Ingress Controller 설치\n      - helm repo add stable https://charts.helm.sh/stable\n      - helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n      - helm repo update\n      - kubectl create namespace ingress-basic\n      - helm version 확인\n      - (helm version 2.x 일때)\n        - helm install --name nginx-ingress stable/nginx-ingress  --namespace=ingress-basic\n      - (helm version 3.x 일때)\n        - helm install nginx-ingress ingress-nginx/ingress-nginx --namespace=ingress-basic\n      - (설치확인)\n        - kubectl get all --namespace=ingress-basic\n        - (Ingress Controller의 EXTERNAL-IP가 API Gateway 엔드포인트: 메모 必)\n    - Ingress 대상 서비스(BLUE, GREEN) 생성\n      - (pwd 로 현 위치가 /container-orchestration/yaml/ingress/blue-svc/ 인지 확인)\n        - (도커라이징 & 이미지 Push)\n        - az acr build --registry [acr-레지트스리명] --image [acr레지스트리명].azurecr.io/nginx-blue:latest .\n        - (배포 전 yaml을 열어 image URL을 나의 ACR이름으로 수정)\n        - nano nginx-blue-deployment.yaml\n        - (저장 ctrl + X)\n        - (배포 및 서비스 생성)\n        - kubectl create -f nginx-blue-deployment.yaml\n      - (pwd 로 현 위치가 /container-orchestration/yaml/ingress/green-svc/ 인지 확인)\n        - (도커라이징 & 이미지 Push)\n        - az acr build --registry [acr-레지트스리명] --image [acr레지스트리명].azurecr.io/nginx-green:latest .\n        - (배포 전 yaml을 열어 image URL을 나의 ACR이름으로 수정)\n        - nano nginx-green-deployment.yaml\n        - (저장 ctrl + X)\n        - (배포 및 서비스 생성)\n        - kubectl create -f nginx-green-deployment.yaml\n      - (서비스 생성 확인)\n        - kubectl get deploy,service -n ingress-basic\n    - Ingress 생성\n      - (pwd 로 현 위치가 /container-orchestration/yaml/ingress/ 인지 확인)\n      - kubectl create -f web-ingress.yaml\n      - (Ingress 생성확인)\n      - kubectl get ingress -n ingress-basic\n    - Ingress 테스트\n      - API Gateway 주소를 Local 시스템에 등록\n        - 관리자권한으로 CMD 실행\n        - cd c:\\\n        - cd windows\n        - cd system32\n        - cd drivers\n        - cd etc\n        - notepad hosts\n        - Windows - hosts 파일 맨 하단에 Ingress Controller의 External-IP 등록\n          \"예시. 51.243.10.185\t blue.example.com  green.example.com\"\n      - 인그레이스 리소스 삭제\n      - kubectl delete namespace ingress-basic\n\n</details>\n\n<details>\n<summary><b>Kubernetes Advanced Hands-on</b></summary>\n\n- <b>Lab. Ingress</b>\n  - Helm 명령으로 설치 여부 확인\n  - Helm 이 설치되어 있지 않은 경우, Helm(패키지 인스톨러) 설치\n    - Helm 3.x 설치(권장)\n      - curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 > get_helm.sh\n      - chmod 700 get_helm.sh\n      - ./get_helm.sh\n    - Helm 2.x 설치\n      - curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash\n      - (설치 중, sudo를 위한 비밀번호 입력)\n      - kubectl --namespace kube-system create sa tiller\n      - kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller\n      - helm init --service-account tiller\n  - Helm으로 Ingress Controller 설치\n    - helm repo add stable https://charts.helm.sh/stable\n    - helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n    - helm repo update\n    - kubectl create namespace ingress-basic\n    - helm version 확인\n    - (helm version 2.x 일때)\n      - helm install --name nginx-ingress stable/nginx-ingress  --namespace=ingress-basic\n    - (helm version 3.x 일때)\n      - helm install nginx-ingress ingress-nginx/ingress-nginx --namespace=ingress-basic\n    - (설치확인)\n      - kubectl get all --namespace=ingress-basic\n      - (Ingress Controller의 EXTERNAL-IP가 API Gateway 엔드포인트: 메모 必)\n  - Ingress 대상 서비스(BLUE, GREEN) 생성\n    - (pwd 로 현 위치가 /container-orchestration/yaml/ingress/blue-svc/ 인지 확인)\n      - (도커라이징 & 이미지 Push)\n      - az acr build --registry [acr-레지트스리명] --image [acr레지스트리명].azurecr.io/nginx-blue:latest .\n      - (배포 전 yaml을 열어 image URL을 나의 ACR이름으로 수정)\n      - nano nginx-blue-deployment.yaml\n      - (저장 ctrl + X)\n      - (배포 및 서비스 생성)\n      - kubectl create -f nginx-blue-deployment.yaml\n    - (pwd 로 현 위치가 /container-orchestration/yaml/ingress/green-svc/ 인지 확인)\n      - (도커라이징 & 이미지 Push)\n      - az acr build --registry [acr-레지트스리명] --image [acr레지스트리명].azurecr.io/nginx-green:latest .\n      - (배포 전 yaml을 열어 image URL을 나의 ACR이름으로 수정)\n      - nano nginx-green-deployment.yaml\n      - (저장 ctrl + X)\n      - (배포 및 서비스 생성)\n      - kubectl create -f nginx-green-deployment.yaml\n    - (서비스 생성 확인)\n      - kubectl get deploy,service -n ingress-basic\n  - Ingress Routing Rule 생성\n    - (pwd 로 현 위치가 /container-orchestration/yaml/ingress/ 인지 확인)\n    - kubectl create -f path-based-ingress.yaml\n    - kubectl get ingress -n ingress-basic\n  - Ingress 테스트\n    - 인그레이스 리소스 삭제\n      - kubectl delete namespace ingress-basic\n\n</details>\n\n<details>\n<summary><b>Service Mesh, Istio Hands-on</b></summary>\n\n  - <b>Lab. Istio Install</b>\n    - Istio 설치\n    - curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.7.1 TARGET_ARCH=x86_64 sh -\n    - cd istio-1.7.1\n    - <pre style=\"white-space: pre-wrap\">export PATH=$PWD/bin:$PATH</pre>\n    - istioctl install --set profile=demo --set hub=gcr.io/istio-release\n      \"note : there are other profiles for production or performance testing.\"\n    - Istio 모니터링 툴(Telemetry Applications) 설치\n      - vi samples/addons/kiali.yaml\n      - 4라인의 apiVersion: apiextensions.k8s.io/v1beta1을 apiVersion: apiextensions.k8s.io/v1으로 수정\n      - kubectl apply -f samples/addons\n      - kiali.yaml 오류발생시, 아래 명령어 실행\n        > kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.7/samples/addons/kiali.yaml\n\n      - 모니터링(Tracing & Monitoring) 툴 설정\n        - Monitoring Server - Kiali\n          - 기본 ServiceType 변경 : ClusterIP를 LoadBalancer 로..\n            - kubectl edit svc kiali -n istio-system\n            - :%s/ClusterIP/LoadBalancer/g\n            - :wq!\n          - 모니터링 시스템(kiali) 접속 : EXTERNAL-IP:20001 (admin/admin)\n        - Tracing Server - Jaeger\n          - 기본 ServiceType 변경 : ClusterIP를 LoadBalancer 로..\n            - kubectl edit svc tracing -n istio-system\n            - :%s/ClusterIP/LoadBalancer/g\n            - :wq!\n          - 분산추적 시스템(tracing) 접속 : EXTERNAL-IP:80\n    - 설치확인\n      - kubectl get all -n istio-system\n  - <b>How to enable Istio</b>\n    - 1. Whenever deploying to Cluster, Using pre-processing command \"Istio kube-inject\"\n      - kubectl apply -f <(istioctl kube-inject -f Deployment.yml) -n istio-test-ns\n    - 2. Using Istio-enabled Namespace.\n      - e.g. kubectl label namespace tutorial istio-injection=enabled\n\n\n  - <b>Lab. Istio Tutorial 셋업</b>\n    - Git repository에서 Tutorial 리소스 가져오기\n      - cd ~\n      - git clone https://github.com/redhat-developer-demos/istio-tutorial\n      - cd istio-tutorial\n    - 네임스페이스 생성\n      - kubectl create namespace tutorial\n    - Customer Service 배포\n      - kubectl apply -f <(istioctl kube-inject -f customer/kubernetes/Deployment.yml) -n tutorial\n        - kubectl describe pod (Customer Pod) -n tutorial 로 생성확인\n      - kubectl create -f customer/kubernetes/Service.yml -n tutorial\n    - Istio Gateway 설치 및 Customer 서비스 라우팅(VirtualService) 설정\n      - cat customer/kubernetes/Gateway.yml\n      - kubectl create -f customer/kubernetes/Gateway.yml -n tutorial\n      - (Istio-IngressGateway를 통한 Customer 서비스 확인)\n        - kubectl get service/istio-ingressgateway -n istio-system\n        - 해당 EXTERNAL-IP가 Istio Gateway 주소\n        - Customer 서비스 호출 :\n          <pre style=\"white-space: pre-wrap\">\"http://istio-ingressgateway IP)/customer\"</pre>\n    - Preference, Recommendation-v1 Service 배포\n      - kubectl apply -f <(istioctl kube-inject -f preference/kubernetes/Deployment.yml)  -n tutorial\n      - kubectl create -f preference/kubernetes/Service.yml -n tutorial\n      - kubectl apply -f <(istioctl kube-inject -f recommendation/kubernetes/Deployment.yml) -n tutorial\n      - kubectl create -f recommendation/kubernetes/Service.yml -n tutorial\n  - <b>Lab. Istio - Traffic Routing</b>\n    - Simple Routing\n      - (pwd 로 현 위치가 /istio-tutorial/ 인지 확인)\n      - (recommendation 서비스 추가 배포: v2)\n         - kubectl apply -f <(istioctl kube-inject -f recommendation/kubernetes/Deployment-v2.yml) -n tutorial\n      - 서비스 호출\n        - 브라우저에서 Customer 서비스(Externl-IP:8080 접속) 호출\n        - F5(새로고침)를 10회 이상 클릭하여 다수의 요청 생성\n      - Routing 결과 확인 - Kiali(Externl-IP:20001) 접속\n      - (Recommendation v.2 서비스 Scale Out)\n      - (서비스의 v2 의 replica 를 2로 설정)\n        - kubectl scale --replicas=2 deployment/recommendation-v2 -n tutorial\n        - kubectl get po -n tutorial\n      - Customer 서비스를 10회 이상 F5(새로고침)하여 서비스 호출\n      - Routing 결과 확인 - Kiali(Externl-IP:20001) 접속\n    - Advanced Routing\n      - 정책(VirtualService, DestinationRule) 설정\n        - (현, 정책 확인)\n          - kubectl get VirtualService -n tutorial -o yaml\n          - kubectl get DestinationRule -n tutorial -o yaml\n        - (사용자 선호도에 따른 추천 서비스 라우팅 정책 설정)\n        - (VirtualService, DestinationRule 설정, v2로 100% 라우팅)\n          - kubectl create -f istiofiles/destination-rule-recommendation-v1-v2.yml -n tutorial\n          - kubectl create -f istiofiles/virtual-service-recommendation-v2.yml -n tutorial\n        - (설정정책 확인)\n          - kubectl get VirtualService -n tutorial -o yaml\n          - kubectl get DestinationRule -n tutorial -o yaml\n        - (서비스 확인)\n          - 브라우저에서 Customer 서비스(Externl-IP:8080 접속)호출\n          - Kiali(Externl-IP:20001), Jaeger(External-IP:80)에서 모니터링\n      - 가중치 기반 스마트 라우팅 (Canary Deployment)\n        - (recommendation 서비스 v1의 가중치를 100으로 변경)\n          - kubectl replace -f istiofiles/virtual-service-recommendation-v1.yml -n tutorial\n        - (서비스 호출 및 Kiali(Externl-IP:20001)에서 모니터링)\n        - (VirtualService 삭제 시, Round-Robin 방식으로 동작)\n          - kubectl delete -f istiofiles/virtual-service-recommendation-v1.yml -n tutorial\n        - Canary 라우팅 비율별 배포 정책 예시\n          - (90 : 10)\n          - kubectl apply -f istiofiles/virtual-service-recommendation-v1_and_v2.yml -n tutorial\n          - (75 : 25)\n          - kubectl replace -f istiofiles/virtual-service-recommendation-v1_and_v2_75_25.yml -n tutorial\n        - 삭제\n          - kubectl delete dr recommendation -n tutorial\n          - #kubectl delete vs recommendation -n tutorial\n          - kubectl scale --replicas=1 deployment/recommendation-v2 -n tutorial\n      - Client 브라우저 유형별 스마트 라우팅\n        - Firefox 브라우저로 접속 시, v2로 라우팅되도록 설정\n          - kubectl apply -f istiofiles/destination-rule-recommendation-v1-v2.yml -n tutorial\n          - kubectl apply -f istiofiles/virtual-service-firefox-recommendation-v2.yml -n tutorial\n        - (Firefox 브라우저와 다른 브라우저에서 접속 확인)\n        - (Browser 환경이 지원되지 않을 경우,)\n          - curl -A Safari Externl-IP:8080\n          - curl -A Firefox Externl-IP:8080\n        - 삭제\n          - kubectl delete dr recommendation -n tutorial\n          - kubectl delete vs recommendation -n tutorial\n  - <b>Lab. Istio - Timeout & Retry</b>\n    - Lab에 필요한 모듈(Message Queue) 설치\n      - kubectl get svc my-kafka -n kafka\n      - 미설치시, 설치 링크 (https://workflowy.com/s/msa/27a0ioMCzlpV04Ib#/a7018fb8c629)\n    - tutorial  네임스페이스에 Istio 기능 추가\n      - kubectl label namespace tutorial istio-injection=enabled --overwrite\n      - 네임스페이스가 없을 시, 생성 후 실행\n    - Lab. Timeout : Fail-Fast를 통한 서비스 Caller 자원 보호\n      - Timeout 테스트를 위해 CNA 과정에서 구현한 Order 마이크로서비스의  코드 보완 및 tutorial 네임스페이스에 배포\n        - Service time delay를 위해, Order Aggregate(Order.java)에 저장전 Thread.sleep 코드 삽입\n          <pre style=\"white-space: pre-wrap\">\n          @PrePersist\n\t          public void onPrePersist(){\n\t          try {\n\t            Thread.currentThread().sleep((long) (800 + Math.random() * 220));\n\t          } catch (InterruptedException e) {\n\t             e.printStackTrace();\n\t          }\n\t        }\n          </pre>\n        - Docker image Build & Push\n        - tutorial 네임스페이스에 Order 서비스 배포\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n            apiVersion: apps/v1\n            kind: Deployment\n            metadata:\n              name: order\n              namespace: tutorial\n              labels:\n                app: order\n            spec:\n              replicas: 1\n              selector:\n                matchLabels:\n                  app: order\n              template:\n                metadata:\n                  labels:\n                    app: order\n                spec:\n                  containers:\n                    - name: order\n                      image: IMAGE_FULL_REPOSITORY_URL/order:v1\n                      ports:\n                        - containerPort: 8080\n                      resources:\n                        limits:\n                          cpu: 500m\n                        requests:\n                          cpu: 200m\n        EOF\n        </pre>\n        - Order 서비스 생성\n          - kubectl expose deploy order --port=8080 -n tutorial\n        - Order 서비스 Timeout 설정 (Istio Gateway에서 Order 서비스로 라우팅 시)\n          - (pwd 로 현 위치가 /istio-tutorial/ 인지 확인)\n          - nano customer/kubernetes/Gateway.yaml 오픈 후 마지막 행 다음에 타임아웃 설정이 포함된 아래 내용 추가\n            <pre style=\"white-space: pre-wrap\">\n            - match:\n              - uri:\"\n              prefix: /orders\n              route:\n            - destination:\n              host: order\n              port:\n              number: 8080\n              timeout: 3s\n            </pre>\n          - (변경 내용 적용)\n          - kubectl apply -f customer/kubernetes/Gateway.yml -n tutorial\n        -  Order 서비스 Timeout 설정 (클라우드 내에서 Order 서비스로 라우팅시)\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n            apiVersion: networking.istio.io/v1alpha3\n            kind: VirtualService\n            metadata:\n              name: vs-order-network-rule\n              namespace: tutorial\n            spec:\n              hosts:\n              - order\n              http:\n              - route:\n                - destination:\n                    host: order\n                timeout: 3s\n          EOF\n          </pre>\n        - 부하테스트 툴(Siege) 설치 및 Order 서비스 Load Testing\n          - kubectl run siege --image=apexacme/siege-nginx -n tutorial\n          - kubectl exec -it siege -c siege -n tutorial &#45;&#45; /bin/bash\n          - <pre style=\"white-space: pre-wrap\">siege -c30 -t20S -v --content-type \"application/json\" &#39;http://order:8080/orders POST {\"productId\": \"1001\", \"qty\":5}&#39;</pre>\n      - Order 서비스에 설정된 Timeout을 임계치를 초과하는 순간, Istio에서 서비스로의 연결을 자동 차단하는 것을 확인\n    - Lab. Retry : 5xx 오류를 리턴받게 되면, Envoy Proxy에서 설정한 횟수만큼 대상 서비스를 재호출하여 일시적인 장애였는지를 다시 확인하는 Rule\n      - Retry 테스트를 위해 CNA 과정에서 구현한 Delivery 마이크로서비스를 tutorial 네임스페이스에 배포\n      - Docker image Build & Push\n      - tutorial 네임스페이스에 Delivery 서비스 배포\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: apps/v1\n          kind: Deployment\n          metadata:\n            name: delivery\n            namespace: tutorial\n            labels:\n              app: delivery\n          spec:\n            replicas: 1\n            selector:\n              matchLabels:\n                app: delivery\n            template:\n              metadata:\n                labels:\n                  app: delivery\n              spec:\n                containers:\n                  - name: delivery\n                    image: IMAGE_FULL_REPOSITORY_URL/delivery:v1\n                    ports:\n                      - containerPort: 8080\n                    resources:\n                      limits:\n                        cpu: 500m\n                      requests:\n                        cpu: 200m\n        EOF\n        </pre>\n      - Delivery 서비스 생성\n        - kubectl expose deploy delivery --port=8080 -n tutorial\n      - Order 서비스에 Retry Rule 추가 적용\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: networking.istio.io/v1alpha3\n          kind: VirtualService\n          metadata:\n            name: vs-order-network-rule\n            namespace: tutorial\n          spec:\n            hosts:\n            - order\n            http:\n            - route:\n              - destination:\n                  host: order\n              timeout: 3s\n              retries:\n                attempts: 3\n                perTryTimeout: 2s\n                retryOn: 5xx,retriable-4xx,gateway-error,connect-failure,refused-stream\n        EOF\n        </pre>\n      - Delivery 서비스를 정지하고, 이를 동기호출하는 Order 서비스 API 호출\n        - kubectl scale deploy delivery --replicas=0 -n tutorial\n        - kubectl exec -it siege -c siege -n tutorial --/bin/bash\n        - http http://order:8080/orders/ productId=1001 qty=5\n          - httpie가 없을 시,\n          - apt-get update\n          - apt-get install httpie\n        - http DELETE http://order:8080/orders/1\n      - Jaeger 접속(http://tracing svc EXTERNAL-IP :80) 후, Retry 횟수 확인하기\n        \"< 검색조건 >\n          Service : order.tutorial, Operation : delivery.tutorial.svc.cluster.local:8080/*\n          검색결과 : 총 Retry 횟수 + 1 의 Requests 로깅\"\n  - <b>Lab. Istio - Circuit Breaker</b>\n    - Circuit Breaker : 장애 인스턴스를 회피하는 기능으로 5xx 오류를 리턴한 인스턴스를  라우팅 대상에서 일정시간 만큼 제외 (Pool Ejection)\n    - Namespace 생성 및 Istio 활성\n      - kubectl create namespace istio-cb-ns\n      - kubectl label namespace istio-cb-ns istio-injection=enabled\n    - Istio Retry 디폴트 동작 확인\n      - 테스트 어플리케이션 배포\n        - hello-server-1, hello-server-2 Pods, Service\n        - hello-server 앱은 env:RANDOM_ERROR 값의 확률로 랜덤하게 503 에러를 발생하는 로직이 포함.\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60; EOF\n            apiVersion: v1\n            kind: Pod\n            metadata:\n              name: hello-server-1\n              namespace: istio-cb-ns\n              labels:\n                app: hello\n            spec:\n              containers:\n                - name: hello-server-1\n                image: docker.io/honester/hello-server:latest\n                imagePullPolicy: IfNotPresent\n                env:\n                - name: VERSION\n                  value: \"v1\"\n                - name: LOG\n                  value: \"1\"\n            &#45;&#45;&#45;\n            apiVersion: v1\n            kind: Pod\n            metadata:\n              name: hello-server-2\n              namespace: istio-cb-ns\n              labels:\n                app: hello\n            spec:\n              containers:\n                - name: hello-server-2\n                image: docker.io/honester/hello-server:latest\n                imagePullPolicy: IfNotPresent\n                env:\n                - name: VERSION\n                  value: \"v2\"\n                - name: LOG\n                  value: \"1\"\n                - name: RANDOM_ERROR\n                  value: \"0.2\"\n            &#45;&#45;&#45;\n            apiVersion: v1\n            kind: Service\n            metadata:\n              name: svc-hello\n              namespace: istio-cb-ns\n              labels:\n                app: hello\n            spec:\n              selector:\n                app: hello\n              ports:\n                - name: http\n                protocol: TCP\n                port: 8080\n          EOF\n          </pre>\n        - 클라이언트용 서비스(httpbin) 배포\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n            apiVersion: apps/v1\n            kind: Deployment\n            metadata:\n              name: httpbin\n              namespace: istio-cb-ns\n            spec:\n              replicas: 1\n              selector:\n                matchLabels:\n                  app: httpbin\n              template:\n                metadata:\n                  labels:\n                    app: httpbin\n                spec:\n                  containers:\n                  - name: httpbin\n                    image: docker.io/honester/httpbin:latest\n                    imagePullPolicy: IfNotPresent\n                    ports:\n                    - containerPort: 80\n            &#45;&#45;&#45;\n            apiVersion: v1\n            kind: Service\n            metadata:\n              name: httpbin\n              namespace: istio-cb-ns\n              labels:\n                app: httpbin\n            spec:\n              selector:\n                app: httpbin\n              ports:\n                - name: http\n                port: 8000\n                targetPort: 80\n          EOF\n          </pre>\n      - Retry 디폴트 동작 테스트\n        - hello-server-2의 로그 모니터 걸기\n          - kubectl logs -f hello-server-2 -c hello-server-2 -n istio-cb-ns\n        - 클라이언트에서 svc-hello 서비스 10번 호출하기\n          - for i in {1..10}; do kubectl exec -it httpbin -c httpbin -n istio-cb-ns --curl http://svc-hello.istio-cb-ns:8080; sleep 0.1; done\n      - 결과 확인/분석\n        \"1) 서비스 호출은 Round Robin으로 로드 밸런싱되나, 프로세싱 시간에 따라 동일한 서비스가 연속 2회 로깅 될 수 있음\n        2) 핵심포인트는, Server-2가 5xx 오류를 리턴할 경우, 자동으로 Retry되어 Server-1 로그가 연달아 출력된다는 점임. (Default Retry : 2회)\"\n    - Circuit Breaker 설정\n      - 대기 쓰레드수 기반 Circuit Breaker\n        - 클라이언트용 서비스(httpbin)에 쓰레드 기반 Circuit Breaker 설정\n        - (Pending Thread가 많을수록 경우, 오랫동안 큐잉된 요청은 Response time이 증가하게 되므로, 적절한 대기 쓰레드를 풀을 적용하여 Circuit Breaking)\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n            apiVersion: networking.istio.io/v1alpha3\n            kind: DestinationRule\n            metadata:\n              name: dr-httpbin\n              namespace: istio-cb-ns\n            spec:\n              host: httpbin\n              trafficPolicy:\n                connectionPool:\n                  http:\n                    http1MaxPendingRequests: 1\n                    maxRequestsPerConnection: 1\n          EOF\n          </pre>\n        - Circuit Breaker 동작 확인\n          - 부하테스트 툴(Siege) 설치 및  Load Testing\n            - kubectl run siege --image=apexacme/siege-nginx -n istio-cb-ns\n            - kubectl exec -it siege -c siege -n istio-cb-ns --/bin/bash\n              - siege -c1 -t10S -v http://httpbin:8000/get  # 100% availability\n              - siege -c2 -t10S -v http://httpbin:8000/get  # 87% availability\n          - Kiali(Externl-IP:20001) 모니터링\n      - 로드 밸런싱 풀(pool) 인스턴스의 Health Status 기반 Circuit Breaker\n        - Hello 서비스의 로드 밸런싱 풀(pool)의 인스턴스 상태기반 Circuit Breaker 설정\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n            apiVersion: networking.istio.io/v1alpha3\n            kind: DestinationRule\n            metadata:\n              name: dr-hello-server\n              namespace: istio-cb-ns\n            spec:\n              host: svc-hello\n              trafficPolicy:\n                outlierDetection:\n                  interval: 1s\n                  consecutiveErrors: 1\n                  baseEjectionTime: 3m\n                  maxEjectionPercent: 100\n          EOF\n          </pre>\n        - Circuit Breaker 동작 확인\n          - 클라이언트(httpbin Pod)에서 svc-hello 호출\n            - hello-server-2의 로그 모니터 걸기\n              - kubectl logs -f hello-server-2 -c hello-server-2 -n istio-cb-ns\n            - 클라이언트에서 svc-hello 서비스 10번 호출하기\n              - for i in {1..10}; do kubectl exec -it httpbin -c httpbin -n istio-cb-ns --curl http://svc-hello.istio-cb-ns:8080; sleep 0.1; done\n            - 결과 확인/분석\n              \"1) 5초 동안 5xx 에러가 2번 발생할 경우, Server-2로는 5분 동안 트래픽이 라우팅 되지 않는다.\n              2) 모니터링 시스템(Kiali) : EXTERNAL-IP:20001 에서 Circuit Breaker 뱃지 발생 확인\"\n    - 동기호출 Target인 배송(Delivery) 서비스에 Circuit Breaker 설정하기\n      - Thread 부하 및 5XX 오류에 대해 서비스를 차단하는 Circuit Breaker 생성\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: networking.istio.io/v1alpha3\n          kind: DestinationRule\n          metadata:\n            name: dr-delivery\n            namespace: tutorial\n          spec:\n            host: delivery\n            trafficPolicy:\n              connectionPool:\n                http:\n                  http1MaxPendingRequests: 30\n                  maxRequestsPerConnection: 100\n              outlierDetection:\n                interval: 5s\n                consecutiveErrors: 1\n                baseEjectionTime: 5m\n                maxEjectionPercent: 100\n        EOF\n        </pre>\n      - 설정 내용\n        - 최대 활성 연결 갯수 30개와 최대 요청 대기 수를 100개로 지정하고, 이 임계점을 넘어가는 추가 요청은 거부(circuit break)\n        - 5초 동안 2번 5xx을 리턴한 서비스는 5분 동안 라우팅 대상에서 제외(Ejection)\n        - 또한, 모든 대상 서비스 인스턴스가 방출(제외)될 수 있음\n    - Clean-up\n      - kubectl delete pod/hello-server-1 pod/hello-server-2 pod/httpbin service/svc-hello dr/dr-hello -n istio-cb-ns\n  - <b>Clear Istio </b>\n    - kubectl delete ns tutorial istio-cb-ns istio-system\n\n\n\n\n\n</details>\n\n</p>\n</details>\n\n<hr />\n\n\n### 마이크로소프트 Azure\n<details>\n<summary>Azure Cloud 기반의 Container Orchestration Lab. Scripts</summary>\n<p>\n\n\n<details>\n<summary><b>자주 사용되는 Azure Cloud 명령어</b></summary>\n\n  - <b>구독(종량제) 설정</b>\n    - ex) az account set --subscription \"종량제2\"\n  - <b>Azure 클러스터 생성</b>\n    - az aks create --resource-group user01_resource_group --name user01_cluster --node-count 2 --enable-addons monitoring --generate-ssh-keys\n  - <b>Azure 클러스터 토큰 가져오기</b>\n    - az aks get-credentials --resource-group (user01_resource_group) --name (user01-cluster)\n  - <b>Azure 컨테이너 레지스트리 생성</b>\n    - az acr create --resource-group (user01_resource_group) --name (user01) --sku Basic\n  - <b>Azure 컨테이너 레지스트리 로그인</b>\n    - az acr login --name (user01)\n  - <b>Azure 클러스터(AKS)에 레지스트리(ACR) 붙이기</b>\n    - az aks update -n (user01_cluster) -g (user01_resource_group) --attach-acr (user01_registry)\n  - <b>Azure 레지스트리(ACR)에 도커 이미지 푸시하기</b>\n    - az acr build --registry [acr-레지트스리명] --image [acr레지스트리명].azurecr.io/[이미지명]:latest .\n  - <b>Azure 클러스터 삭제</b>\n    - az aks delete --name MyManagedCluster --resource-group MyResourceGroup\n  - <b>Azure 리소스그룹 삭제</b>\n    - az group delete --name rsrcgroup\n  - <b>Azure AKS 모니터링</b>\n    - admin-user 서비스 계정 및 클러스터 롤 바인딩 생성\n      <pre style=\"white-space: pre-wrap\">\n      cat &#60;&#60;EOF | kubectl apply -f -\n      &#45;&#45;&#45;\n      apiVersion: v1\n      kind: ServiceAccount\n      metadata:\n        name: admin-user\n        namespace: kube-system\n      &#45;&#45;&#45;\n      apiVersion: rbac.authorization.k8s.io/v1beta1\n      kind: ClusterRoleBinding\n      metadata:\n        name: admin-user\n      roleRef:\n        apiGroup: rbac.authorization.k8s.io\n        kind: ClusterRole\n        name: cluster-admin\n      subjects:\n      - kind: ServiceAccount\n        name: admin-user\n        namespace: kube-system\n      EOF\n      </pre>\n    - 인증 토큰 조회\n      - <pre style=\"white-space: pre-wrap\">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk \"{print $1}\")</pre>\n    - az aks browse --resource-group (resource-group-name) --name (cluster-name)\n    - 복사된 토큰값 붙여넣기 및 로그인\n\n</details>\n\n### Docker\n\n<details>\n<summary><b>Setup</b></summary>    </b>\n\n- <b>관리자 권한으로 PowerShell 실행</b>\n  - Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\n- <b>리눅스설치 및 실행</b>\n  - (설치전 확인 사항)\n    - Windows 기능 켜기/끄기에서 \"Linux용 Windows 하위시스템 활성화\" 확인\n    - 개발자 기능사용에서 \"개발자 모드\" 활성화 확인\n  - Ubuntu의 Archive Repository Server를 (빠른 패키지 설치를 위해) 국내로 설정\n    - sudo vi /etc/apt/sources.list\n    - :%s/archive.ubuntu.com/ftp.daumkakao.com/g\n    - :wq!\n    - sudo apt-get update\n- <b>Linux에 JDK 설치</b>\n  - (설치 명령)\n  - sudo apt-get update\n  - sudo apt install default-jdk\n  - (bash에 환경변수 추가)\n  -  cd ~\n  - nano .bashrc\n  - (맨아래로 이동)\n  - (JAVA_HOME 설정 및 실행 Path 추가)\n    <pre style=\"white-space: pre-wrap\">\n    export JAVA_HOME=‘/usr/lib/jvm/java-11-openjdk-amd64\n    export PATH=$PATH:$JAVA_HOME/bin:.\n    </pre>\n  - (수정사항 반영)\n    - ctrl + x, y 입력, 종료\n    - source ~/.bashrc\n  - (설치 확인)\n  - echo $JAVA_HOME\n  - java -version\n- <b>Windows에 도커 데몬 설치</b>\n  - https://www.docker.com/products/docker-desktop\n- <b>도커허브 계정생성</b>\n  -  http://hub.docker.com 접속 후, Sign Up (회원가입)\n- <b>리눅스에 도커 Client 설치</b>\n  - sudo apt-get update\n  - 비밀번호 입력창에 skadmin1234\n  - sudo apt install apt-transport-https ca-certificates curl software-properties-common\n  - curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add\n  - <pre style=\"white-space: pre-wrap\">sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\"</pre>\n  - sudo apt update\n  - sudo apt install docker-ce\n  - #리눅스 설치시 생성한 사용자 명 입력\n  - sudo usermod -aG docker skccadmin\n- <b>도커 데몬과 도커 Client 연결 </b>\n  - cd\n  - nano .bashrc\n  - 맨아래 줄에 아래 환경변수 추가\n    - 방향키로 맨 아래까지 내린 다음, 새로운 행에 아래 내용 입력\n    - export DOCKER_HOST=tcp://0.0.0.0:2375\n    - 저장 & 종료 : Ctrl + x, 입력 후, y 입력  후 엔터\n  - source ~/.bashrc\n  - 연결 확인\n    - docker images\n    - docker run --name nginx -d -p 80:80 nginx\n    - docker images\n</details>\n\n <details>\n<summary><b>Docker Hands-On</b></summary>\n\n- <b>Lab. image</b>\n  - 이미지 Pull\n    - docker pull hello-world\n    - docker images\n    - docker pull nginx\n    - docker pull nginx:latest\n    - docker pull docker.io/library/nginx:latest\n    - docker pull nginx:1.16.1\n    - docker images\n  - 도커허브 (Docker Hub)\n    - http://hub.docker.com # 접속 후, nginx 검색\n  - 이미지 Tagging\n    - docker image tag nginx my-nginx # Create 태그\n  - 이미지 삭제\n    - docker image rm my-nginx\n    - docker image rm hello-world\n    - docker image rm $(docker images -q) # 한번에 모든 도커 이미지 지우기\n- <b>Lab. container</b>\n  - 컨테이너 생성\n    - docker run hello-world # 컨테이너 만들기\n    - docker run --name hello hello-world # 이름 지정, 미지정시 임의의 이름으로 생성\n    - docker run --name my-nginx -d -p 80:80 nginx\n    - docker ps\n  - 컨테이너 시작/종료\n    - docker stop my-nginx\n    - docker start my-nginx\n  - 컨테이너 포트 노출\n    - http://localhost 에서 nginx index.html 확인\n    - docker container rm my-nginx\n    - docker run --name my-nginx -d -p 8080:80 nginx\n    - http://localhost:8080 에서 nginx index.html 확인\n  - 컨테이너 접근\n    - docker exec my-nginx cat /usr/share/nginx/html/index.html #실행 중 컨테이너 접근\n    - docker exec -i -t  my-nginx /bin/bash\n      - apt-get update\n      - apt-get install curl\n      - curl localhost\n      - exit\n  - 컨테이너 삭제\n    - docker container rm my-nginx # 실행 중 컨테이너  삭제 시, 오류\n    - docker container rm $(docker ps -a -q) # 한번에 모든 컨테이너 지우기\n- <b>Lab. Docker Build & Push</b>\n  - Dockerfile로부터 이미지 생성\n    - Dockerfile & 리소스 생성\n      - mkdir Dockerfile\n      - cd Dockerfile\n      - nano index.html\n        <pre style=\"white-space: pre-wrap\">\"Hi~ My name is Park Yong Joo..\"</pre>\n        - 저장 및 종료 (Ctrl + X, y 입력 후 엔터)\n        - nano Dockerfile\n          <pre style=\"white-space: pre-wrap\">\n          FROM nginx\n          COPY index.html /usr/share/nginx/html/\n          </pre>\n      - 저장 및 종료 (Ctrl + x, Y 입력 후 엔터)\n    - 도커라이징 & Push\n      - docker build -t (Docker-ID)/my-nginx:v1 .\n      - docker images\n      - docker push (Docker-ID)/my-nginx:v1\n        \"denied: 권한오류 생성 시, docker login 명령으로 Docker Hub에 로그인해 준다.\"\n    - http://hub.docker.com 에서 이미지 확인\n    - Docker Hub 이미지로부터 컨테이너 실행\n      - docker run --name new-nginx -d -p 80:80 (Docker-ID)/my-nginx\n    - Browser에서 실행 애플리케이션 확인\n      - http://localhost:8080\n- <b>Clear</b>\n  - docker container rm $(docker ps -a -q)\n    - container 삭제 전, 실행 중인 컨테이너를 정지시켜 준다.\n    - docker container stop new-nginx\n  - docker image rm -f $(docker images -q)\n\n</details>\n\n### Kubernetes\n\n <details>\n<summary><b>Cloud Setup</b></summary>\n\n  - <b>Cloud Shell 에서 Setup 확인 명령</b>\n      - az aks get-credentials --resource-group My_Resource_Group --name My-cluster\n      - kubectl config current-context\n\n  - <b>Cloud Client Setup</b>\n    - Kubectl 설치 (ubuntu 18.04)\n      - sudo apt-get update && sudo apt-get install -y apt-transport-https\n      - curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n      - <pre style=\"white-space: pre-wrap\">echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list</pre>\n      - sudo apt-get update\n      - sudo apt-get install -y kubectl\n    - Azure-Cli 설치\n      - curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n      - az login -u ~~ -p ~~\n    - Local에 AKS(Azure Kubernetes Service) 클러스터 접속정보 설정\n      - az aks get-credentials --resource-group [azure-resource-name] --name [azure-cluster-name]\n      - kubectl config current-context\n      - kubectl get all\n    - Local에 ACR(Azure Container Registry) 접속정보 설정\n      - az acr login --name [azure-registry-name]\n    - Azure AKS와 ACR 연결 (4\")\n      - az aks update -n [azure-cluster-name] -g [azure-resource-Group-name] --attach-acr [azure-acr-name]\n\n</details>\n\n<details>\n<summary><b>Kubernetes Hands-on</b></summary>\n\n- <b>Lab에 필요한 리소스 내려받기</b>\n  - git clone https://github.com/acmexii/mall.git\n  - git clone https://github.com/event-storming/container-orchestration.git\n  - cd container-orchestration\n  - cd yaml\n\n- <b>Lab. K8s Sample App 생성</b>\n  - 어플리케이션 생성/ 확인\n    - Docker hub에 올린 이미지를 통한 컨테이너 생성\n      kubectl create deploy my-nginx --image=apexacme/my-nginx:v1\"\n    - 클러스터 외부에 노출하기\n      kubectl expose deploy my-nginx --type=LoadBalancer --port=80\"\n  - 서비스 확인하기\n    - kubectl get svc의 EXTERNAL-IP 복사\n    - Browser에서 EXTERNAL-IP:80 접속\n- <b>Lab. Pod & 기본명령</b>\n  - kubectl get nodes\n    - 쿠버네티스에 제대로 접속했는지 확인\n    - 현 클러스터의 워크노드를 리스트업\n    - 접속 결과 안나오는 경우\n      - kubectl config current-context 명령으로 Cluster 접속 확인\n  - 객체의 검색\n    - kubectl get [object type]\n    - kubectl get pods   # pods = pod = po\n    - kubectl get deployments   # deploy\n    - kubectl get services    # svc\n    - kubectl get replicaset    # rs\n  - 객체의 모니터링\n    - watch kubectl get all\n    - kubectl get pod -w\n    - watch kubectl get pod\n  - 객체의 유형\n    - Service\n      - types\n        - LoadBalancer\n          - 클라우드 제공자에 의해 제공된 Loadbalancer 로 노출\n          - front-end 혹은 ingress (api gateway)\n        - ClusterIP(default) / NodePort\n          - 클러스터 내부 IP\n          - 내부 마이크로 서비스\n    - Deployment\n      - ReplicaSet (하나이상 생성)\n        - Pod (하나이상 생성)\n          - Container (docker ) 하나이상.\n    - Pod\n    - ReplicaSet\n    - Ingress\n    - Secret\n    - ConfigMap\n    - ServiceAccount = sa\n    - statefulset\n    - daemonset\n  - 설정파일(YAML)을 통한 Pod 배포 (직접 타이핑)\n    - nano declarative-pod.yaml\n      <pre style=\"white-space: pre-wrap\">\n      apiVersion: v1\n      kind: Pod\n      metadata:\n        name: declarative-pod\n        labels:\n          env: test\n      spec:\n        containers:\n        - name: memory-demo-ctr\n          image: nginx\n      </pre>\n    - 저장 및 종료 (ctrl + X, Y, 엔터)\n    - kubectl create -f declarative-pod.yaml\n    - kubectl get pods\n  - 원하는 Node 타입에 Pod 생성\n    - #pwd 로 현 위치가 /container-orchestration/yaml/pod 인지 확인\n    - kubectl create -f pod-with-nodeselector.yaml\n    - kubectl get po -o wide\n      - Pod가 찾는 노드가 없어 pending 상태\n    - 노드에 라벨 추가\n      - kubectl label nodes [your-node-name] disktype=ssd\n      - kubectl get nodes --show-labels | grep ssd\n    - kubectl get po -o wide\n  - Pod 생성 전 초기화\n    - kubectl create -f pod-initialize.yaml\n    - kubectl get po\n    - #생성된 Pod 내로 접근\n    - kubectl exec -it init-demo --/bin/bash\n    - cd /usr/share/nginx/html\n    - ls\n  - 생성된 Pod 및 오브젝트 삭제\n    - kubectl delete pod [pod명]\n    - kubectl delete service,deploy --all\n- <b>Lab. Label </b>\n  - kubectl run nginx --image=nginx\n  - kubectl get pods -l run=nginx\n  - kubectl get pods --selector run=nginx\n  - <pre style=\"white-space: pre-wrap\">kubectl get pods --selector \"run in (nginx, test)\"</pre>\n- <b>Lab. ReplicaSet</b>\n  - pwd 로 현 위치가 /container-orchestration/yaml/replicaset 인지 확인\n  - kubectl create -f replicaset.yaml\n  - kubectl get all\n  - #replica 개수 조정\n    - kubectl scale replicaset/frontend --replicas=5\n    - kubectl get po\n- <b>Lab. Deployment & 기본명령 </b>\n  - 기본 nginx 서버의 배포\n  - kubectl create deploy nginx --image=nginx\n  - kubectl get deploy nginx\n  - kubectl get replicaset -l app=nginx\n  - kubectl get po -l app=nginx  # \"-l\" 옵션은 label의 key/value 로 객체를 필터링\n  - kubectl get pods --selector app=nginx\n  - <pre style=\"white-space: pre-wrap\">kubectl get pods --selector \"app in (nginx, test)\"</pre>\n  - kubectl describe po (검색한 pod name)\n  - (pod 제거)\n  - kubectl delete po --all   #\n  - (pod 를 제거해도 재생됨을 확인)\n  - kubectl get po\n  - (scale out)\n  - kubectl scale deploy nginx --replicas=3\n  - kubectl get po   # pod 개수가 3개로 늘어남을 확인\n  - kubectl delete po --all   # pod 를 모두 지움\n  - kubectl get po   # pod 를 모두 지워도 결국 3개로 복원됨을 확인\n  - ( 제거하기 위해서는 deployment 를 제거해야만 함)\n  - kubectl delete deploy nginx\n- <b>Lab. Rollout & Back</b>\n  - (pwd 로 현 위치가 /container-orchestration/yaml/ 인지 확인)\n  - kubectl create -f nginx.yaml\n  - (아래 명령으로 배포 주석 추가, Rollback시 필요)\n  - <pre style=\"white-space: pre-wrap\">kubectl annotate deploy nginx-deployment kubernetes.io/change-cause=\"v1 is nginx:1.7.9\"</pre>\n  - Set image 명령을 통한 이미지 Rollout  및 확인\n  - kubectl set image deploy nginx-deployment nginx=nginx:1.9.1\n  - kubectl rollout history deploy nginx-deployment\n  - (배포주석 달기)\n  - <pre style=\"white-space: pre-wrap\">kubectl annotate deploy nginx-deployment kubernetes.io/change-cause=\"v2 is nginx:1.9.1\"</pre>\n  - kubectl describe po [해당 deployment 의 pod 중 하나의 이름]    # 내용의 image 부분이 1.9.1 인지 확인\n  - (무정지 재배포 히스토리 확인)\n  - kubectl rollout history deploy nginx-deployment\n  - (다음과 같이 출력됨을 확인)\n    \"REVISION  CHANGE-CAUSE\n    1         v1 is nginx:1.7.9\n    2         v2 is nginx:1.9.1\"\n  - (롤백하기)\n  - kubectl rollout undo deploy nginx-deployment\n  - kubectl rollout undo deploy nginx-deployment --to-revision 5\n- <b>Lab. Service</b>\n  - Basic YAML\n      <pre style=\"white-space: pre-wrap\">\n      apiVersion: v1\n      kind: Service\n      metadata:\n        name: my-service\n      spec:\n        selector:\n          app: MyApp\n        ports:\n          - protocol: TCP\n            port: 80\n            targetPort: 8080\n      </pre>\n  - kubectl delete service,deploy --all  # 기존 이력 삭제\n  - (다시 생성)\n  - kubectl create deploy nginx --image=nginx\n  - (서비스로 노출)\n  - <pre style=\"white-space: pre-wrap\">kubectl expose deploy nginx --type=\"LoadBalancer\" --port=80</pre>\n  - (웹 브라우저를 열고 생성된 external ip 로 접속, Nginx welcome 메시지 확인)\n  - kubectl exec -it (pod name) --/bin/bash   # 생성된 nginx 서버 linux 의 shell 에 접근\n- <b>Auto Scale-Out </b>\n  - pwd 로 현 위치가 /container-orchestration/yaml/ 인지 확인\n  - (모든 객체 지우기)\n  - kubectl delete deploy,service,pod --all\n  - (대상 서비스 배포 및 모니터링)\n  - kubectl apply -f https://k8s.io/examples/application/php-apache.yaml\n    - NOTE : 서비스가 Auto Scaling되기 위해서는 컨테이너 Spec에 Resources : 설정이 있어야 함\n      \"resources:\n            limits:\n              cpu: 500m\n            requests:\n              cpu: 200m\"\n  - (오토 스케일링 설정, hpa: HorizontalPodAutoscaler )\n    - kubectl autoscale deployment php-apache --cpu-percent=20 --min=1 --max=10\n      cpu-percent=50 : Pod 들의 요청 대비 평균 CPU 사용율 (여기서는  요청이 200 milli-cores이므로, 모든 Pod의 평균 CPU 사용율이 100 milli-cores(50%)를 넘게되면 HPA 발생)\"\n    - kubectl get hpa php-apache -o yaml\n  - 로드 제너레이터(siege)가 설치된 컨테이너 생성\n    - cat siege.yaml\n    - kubectl create -f siege.yaml\n    - kubectl exec -it siege --/bin/bash\n  - 로드 생성\n    - siege -c30 -t30S -v http://php-apache\n  - (오토 스케일링이 되지 않을 때 :  kubectl get hpa의 TARGETS 부분에 cpu 사용률이 <unknown>으로 출력될 때)\n    - metrics-server가 제대로 실행중인지 kubectl top pods 명령으로 포드 cpu 사용률이 모니터링 되는지 확인\n    - 디플로이먼트의 컨테이너 옵션에 cpu request 옵션이 제대로 걸려 있는지 확인\n    - cpu request옵션이 없으면 hpa가 cpu사용량에 필요한 계산을 할 수 없음\n- <b>Lab. Volume</b>\n  - (pwd 로 현 위치가 /container-orchestration/yaml/volume 인지 확인)\n  - (emptyDir 마운트)\n  - kubectl create -f volume-emptydir.yaml\n  - (GitRepository를 볼륨으로 마운트)\n  - kubectl create -f volume-gitrepo.yaml\n  - (PersistentVolumeClaim 생성)\n  - kubectl create -f volume-pvc.yaml\n  - kubectl get pvc\n  - kubectl describe pvc azure-managed-disk\n  - (생성된 PersistentVolueClaim으로 Pod 생성하기)\n  - kubectl create -f pod-with-pvc.yaml\n  - kubectl describe pod mypod\n  - (포드 접속)\n  - kubectl exec -it mypod --/bin/bash\n  - (마운트 및 사이즈 확인)\n  - df -k\n- <b>Lab. ConfigMap</b>\n  - (pwd 로 현 위치가 /container-orchestration/yaml/configmap/ 인지 확인)\n  - (컨피그 맵 생성)\n  - kubectl create configmap hello-cm --from-literal=language=java\n  - kubectl get cm\n  - kubectl get cm hello-cm -o yaml\n  - (도커라이징 & ACR Push)\n    - az acr build --registry [acr-레지트스리명] --image [acr레지스트리명].azurecr.io/cm-sandbox:v1 .\n  - (인증오류 발생 시, ACR 로그인)\n    - az acr login --name (Azure Container Registry 명) --expose-token\n    - Token Expose관련 오류 > expose-token 생략\n  - 클라우드에서 배포 이미지 확인\n  - nano cm-deployment.yaml 파일 편집(나의 Registry명으로 수정)\n  - (배포 및 서비스 생성)\n    - kubectl create -f cm-deployment.yaml\n    - kubectl create -f cm-service.yaml\n  - (서비스 확인)\n    - Service의 External-IP 접속\n- <b>Lab. Secret</b>\n  - (pwd 로 현 위치가 /container-orchestration/yaml/secret/ 인지 확인)\n  - (Pod에서 Secret 파일 마운트 사용하기 내용을 참고하여 배포 및 서비스 확인해 보기)\n- <b>Lab. Liveness & Readiness Probe</b>\n  - (pwd 로 현 위치가 /container-orchestration/yaml/liveness/ 인지 확인)\n  - (Liveness Command Probe 실습)\n    - kubectl create -f exec-liveness.yaml\n    - (컨테이너가 Running 상태로 보이나, Liveness Probe 실패로 계속 재시작)\n    - (kubectl describe로 실패 메시지 확인)\n    - kubectl describe po liveness-exec\n  - (Liveness HTTP Probe 실습)\n    - kubectl create -f http-liveness.yaml\n    - (kubectl describe로 실패 메시지 확인)\n    - kubectl describe po liveness-http\n  - (Liveness 와 readiness probe 동시 적용 실습)\n    - kubectl create -f tcp-liveness-readiness.yaml\n    - (8080포트에 대해 정상적으로 Liveness 와 readiness Probe를 통과해 서비스가 실행됨)\n    - kubectl describe po goproxy\n</details>\n\n<details>\n<summary><b>Kubernetes Advanced Hands-on</b></summary>\n\n- <b>Lab. Ingress</b>\n  - Helm 명령으로 설치 여부 확인\n  - Helm 이 설치되어 있지 않은 경우, Helm(패키지 인스톨러) 설치\n    - Helm 3.x 설치(권장)\n      - curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 > get_helm.sh\n      - chmod 700 get_helm.sh\n      - ./get_helm.sh\n    - Helm 2.x 설치\n      - curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash\n      - (설치 중, sudo를 위한 비밀번호 입력)\n      - kubectl --namespace kube-system create sa tiller\n      - kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller\n      - helm init --service-account tiller\n  - Helm으로 Ingress Controller 설치\n    - helm repo add stable https://charts.helm.sh/stable\n    - helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n    - helm repo update\n    - kubectl create namespace ingress-basic\n    - helm version 확인\n    - (helm version 2.x 일때)\n      - helm install --name nginx-ingress stable/nginx-ingress  --namespace=ingress-basic\n    - (helm version 3.x 일때)\n      - helm install nginx-ingress ingress-nginx/ingress-nginx --namespace=ingress-basic\n    - (설치확인)\n      - kubectl get all --namespace=ingress-basic\n      - (Ingress Controller의 EXTERNAL-IP가 API Gateway 엔드포인트: 메모 必)\n  - Ingress 대상 서비스(BLUE, GREEN) 생성\n    - (pwd 로 현 위치가 /container-orchestration/yaml/ingress/blue-svc/ 인지 확인)\n      - (도커라이징 & 이미지 Push)\n      - az acr build --registry [acr-레지트스리명] --image [acr레지스트리명].azurecr.io/nginx-blue:latest .\n      - (배포 전 yaml을 열어 image URL을 나의 ACR이름으로 수정)\n      - nano nginx-blue-deployment.yaml\n      - (저장 ctrl + X)\n      - (배포 및 서비스 생성)\n      - kubectl create -f nginx-blue-deployment.yaml\n    - (pwd 로 현 위치가 /container-orchestration/yaml/ingress/green-svc/ 인지 확인)\n      - (도커라이징 & 이미지 Push)\n      - az acr build --registry [acr-레지트스리명] --image [acr레지스트리명].azurecr.io/nginx-green:latest .\n      - (배포 전 yaml을 열어 image URL을 나의 ACR이름으로 수정)\n      - nano nginx-green-deployment.yaml\n      - (저장 ctrl + X)\n      - (배포 및 서비스 생성)\n      - kubectl create -f nginx-green-deployment.yaml\n    - (서비스 생성 확인)\n      - kubectl get deploy,service -n ingress-basic\n  - Ingress Routing Rule 생성\n    - (pwd 로 현 위치가 /container-orchestration/yaml/ingress/ 인지 확인)\n    - kubectl create -f path-based-ingress.yaml\n    - kubectl get ingress -n ingress-basic\n  - Ingress 테스트\n    - 인그레이스 리소스 삭제\n    - kubectl delete namespace ingress-basic\n- <b>Azure AKS 모니터링</b>\n  - admin-user 서비스 계정 및 클러스터 롤 바인딩 생성\n     <pre style=\"white-space: pre-wrap\">\n        cat &#60;&#60;EOF | kubectl apply -f -\n          &#45;&#45;&#45;\n          apiVersion: v1\n          kind: ServiceAccount\n          metadata:\n            name: admin-user\n            namespace: kube-system\n          &#45;&#45;&#45;\n          apiVersion: rbac.authorization.k8s.io/v1beta1\n          kind: ClusterRoleBinding\n          metadata:\n            name: admin-user\n          roleRef:\n            apiGroup: rbac.authorization.k8s.io\n            kind: ClusterRole\n            name: cluster-admin\n          subjects:\n          - kind: ServiceAccount\n            name: admin-user\n            namespace: kube-system\n        EOF\n       </pre>\n  - 인증 토큰 조회\n    - <pre style=\"white-space: pre-wrap\">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk \"{print $1}\")</pre>\n  - az aks browse --resource-group (resource-group-name) --name (cluster-name)\n  - 복사된 토큰값 붙여넣기 및 로그인\n\n </details>\n\n <details>\n<summary><b>Real MSA Application Deployment</b></summary>\n\n  - <b>사전작업</b>\n    - az acr login --name [acr-repository-name] --expose-token\n    - cd  ~\n    - mkdir MSA-Sample\n    - cd MSA-Sample\n\n  - <b>마이크로서비스 배포</b>\n    - 상품(Product) 서비스\n      <pre style=\"white-space: pre-wrap\">\n      git clone https://github.com/event-storming/reqres_products.git\n      cd reqres_products\n\n      mvn package -Dmaven.test.skip=true\n\n      az acr build --registry (myregistry) --image (myregistry).azurecr.io/products:latest .\n\n      kubectl create deploy products --image=(myregistry).azurecr.io/products:latest\n      kubectl expose deploy products --type=ClusterIP\" --port=8080\n\n      cd ..\n      </pre>\n    - 여러 서비스를 편하게 배포하기 위해 Container Registry를 환경변수로 설정\n      - export CRNAME=(myregistry)\n      - export ACR=${CRNAME}.azurecr.io\n    - 주문(Order) 서비스\n      <pre style=\"white-space: pre-wrap\">\n      git clone https://github.com/event-storming/reqres_orders.git\n      cd reqres_orders\n      export IMAGENAME=orders\n\n      mvn package -Dmaven.test.skip=true\n      az acr build --registry ${CRNAME} --image ${ACR}/${IMAGENAME}:latest .\n      kubectl create deploy ${IMAGENAME} --image=${ACR}/${IMAGENAME}:latest\n      kubectl expose deploy ${IMAGENAME} --type=ClusterIP\" --port=8080\n\n      cd ..\n      </pre>\n    - 배송(Delivery) 서비스\n      <pre style=\"white-space: pre-wrap\">\n      git clone https://github.com/event-storming/reqres_delivery.git\n      cd reqres_delivery\n      export IMAGENAME=delivery\n\n      mvn package -Dmaven.test.skip=true\n      az acr build --registry ${CRNAME} --image ${ACR}/${IMAGENAME}:latest .\n      kubectl create deploy ${IMAGENAME} --image=${ACR}/${IMAGENAME}:latest\n      kubectl expose deploy ${IMAGENAME} --type=ClusterIP\" --port=8080\n\n      cd ..\n      </pre>\n    - 인증(Oauth) 서비스\n      <pre style=\"white-space: pre-wrap\">\n      git clone https://github.com/event-storming/oauth.git\n      cd oauth\n      export IMAGENAME=oauth\n\n      mvn package -Dmaven.test.skip=true\n      az acr build --registry ${CRNAME} --image ${ACR}/${IMAGENAME}:latest .\n      kubectl create deploy ${IMAGENAME} --image=${ACR}/${IMAGENAME}:latest\n      kubectl expose deploy ${IMAGENAME} --type=ClusterIP\" --port=8080\n\n      cd ..\n      </pre>\n    - 게이트웨이(Gateway) 서비스\n      <pre style=\"white-space: pre-wrap\">\n      git clone https://github.com/event-storming/gateway.git\n      cd gateway\n      export IMAGENAME=gateway\n\n      mvn package -Dmaven.test.skip=true\n      az acr build --registry ${CRNAME} --image ${ACR}/${IMAGENAME}:latest .\n      kubectl create deploy ${IMAGENAME} --image=${ACR}/${IMAGENAME}:latest\n      kubectl expose deploy ${IMAGENAME} --type=\"LoadBalancer\" --port=8080\n\n      cd ..\n      </pre>\n    - 프론트-엔드(UI) 서비스 빌드를 위한 npm 설치 (Azure Cloud에서 실행시 Skip)\n      <pre style=\"white-space: pre-wrap\">\n      sudo apt-get update\n      sudo apt install build-essential\n      curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n      sudo apt install nodejs\n      </pre>\n    - 프론트-엔드(UI) 서비스: 배포 사전 작업\n      <pre style=\"white-space: pre-wrap\">\n      git clone https://github.com/event-storming/ui.git\n      cd ui\n      export IMAGENAME=ui\n\n      npm install\n      npm run build\n      az acr build --registry ${CRNAME} --image ${ACR}/${IMAGENAME}:latest .\n\n      _GATEWAY_IP=$(kubectl get -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\" svc gateway --ignore-not-found)\n      echo ${_GATEWAY_IP}\n      </pre>\n    - 프론트-엔드(UI) 서비스:  배포 및 서비스 생성\n      <pre style=\"white-space: pre-wrap\">\n      cat &#60;&#60;EOF | kubectl apply -f -\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: ${IMAGENAME}\n        labels:\n          app: ${IMAGENAME}\n      spec:\n        replicas: 1\n        selector:\n          matchLabels:\n            app: ${IMAGENAME}\n        template:\n          metadata:\n            labels:\n              app: ${IMAGENAME}\n          spec:\n            containers:\n              - name: ${IMAGENAME}\n                image: ${ACR}/${IMAGENAME}:latest\n                ports:\n                  - containerPort: 8080\n                env:\n                  - name: VUE_APP_API_HOST\n                    value: http://${_GATEWAY_IP}:8080\n      EOF\n\n      kubectl expose deploy ${IMAGENAME} --type=\"LoadBalancer\" --port=8080\n\n      cd ..\n      </pre>\n  - <b>서비스 확인</b>\n    - kubectl get svc ui\n    - 브라우저에서 접속 http://UI-Service-EXTERNAL-IP:8080\n\n </details>\n\n <details>\n<summary><b>Service Mesh, Istio Hands-on</b></summary>\n\n  - <b>Lab. Istio Install</b>\n    - Istio 설치\n    - curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.7.1 TARGET_ARCH=x86_64 sh -\n      \"(istio v1.7.1은 Kubernetes 1.16이상에서만 동작)\"\n    - cd istio-1.7.1\n    - <pre style=\"white-space: pre-wrap\">export PATH=$PWD/bin:$PATH</pre>\n    - istioctl install --set profile=demo --set hub=gcr.io/istio-release\n      \"note : there are other profiles for production or performance testing.\"\n    - Istio 모니터링 툴(Telemetry Applications) 설치\n      - vi samples/addons/kiali.yaml\n      - 4라인의 apiVersion: apiextensions.k8s.io/v1beta1을 apiVersion: apiextensions.k8s.io/v1으로 수정\n      - kubectl apply -f samples/addons\n      - kiali.yaml 오류발생시, 아래 명령어 실행\n        > kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.7/samples/addons/kiali.yaml\n\n      - 모니터링(Tracing & Monitoring) 툴 설정\n        - Monitoring Server - Kiali\n          - 기본 ServiceType 변경 : ClusterIP를 LoadBalancer 로..\n            - kubectl edit svc kiali -n istio-system\n            - :%s/ClusterIP/LoadBalancer/g\n            - :wq!\n          - 모니터링 시스템(kiali) 접속 : EXTERNAL-IP:20001 (admin/admin)\n        - Tracing Server - Jaeger\n          - 기본 ServiceType 변경 : ClusterIP를 LoadBalancer 로..\n            - kubectl edit svc tracing -n istio-system\n            - :%s/ClusterIP/LoadBalancer/g\n            - :wq!\n          - 분산추적 시스템(tracing) 접속 : EXTERNAL-IP:80\n    - 설치확인\n      - kubectl get all -n istio-system\n  - <b>How to enable Istio</b>\n    - 1. Whenever deploying to Cluster, Using pre-processing command \"Istio kube-inject\"\n      - kubectl apply -f <(istioctl kube-inject -f Deployment.yml) -n istio-test-ns\n    - 2. Using Istio-enabled Namespace.\n      - e.g. kubectl label namespace tutorial istio-injection=enabled\n  - <b>Lab. Istio Tutorial 셋업</b>\n    - Git repository에서 Tutorial 리소스 가져오기\n      - cd ~\n      - mkdir git\n      - cd git\n      - git clone https://github.com/redhat-developer-demos/istio-tutorial\n      - cd istio-tutorial\n    - 네임스페이스 생성\n      - kubectl create namespace tutorial\n    - Customer Service 배포\n      - kubectl apply -f <(istioctl kube-inject -f customer/kubernetes/Deployment.yml) -n tutorial\n        - kubectl describe pod (Customer Pod) -n tutorial 로 생성확인\n      - kubectl create -f customer/kubernetes/Service.yml -n tutorial\n    - Istio Gateway 설치 및 Customer 서비스 라우팅(VirtualService) 설정\n      - cat customer/kubernetes/Gateway.yml\n      - kubectl create -f customer/kubernetes/Gateway.yml -n tutorial\n      - (Istio-IngressGateway를 통한 Customer 서비스 확인)\n        - kubectl get service/istio-ingressgateway -n istio-system\n        - 해당 EXTERNAL-IP가 Istio Gateway 주소\n        - Customer 서비스 호출 :\n          <pre style=\"white-space: pre-wrap\">\"http://(istio-ingressgateway IP)/customer\"</pre>\n    - Preference, Recommendation-v1 Service 배포\n      - kubectl apply -f <(istioctl kube-inject -f preference/kubernetes/Deployment.yml)  -n tutorial\n      - kubectl create -f preference/kubernetes/Service.yml -n tutorial\n      - kubectl apply -f <(istioctl kube-inject -f recommendation/kubernetes/Deployment.yml) -n tutorial\n      - kubectl create -f recommendation/kubernetes/Service.yml -n tutorial\n  - <b>Lab. Istio - Traffic Routing</b>\n    - Simple Routing\n      - (pwd 로 현 위치가 /istio-tutorial/ 인지 확인)\n      - (recommendation 서비스 추가 배포: v2)\n        - kubectl apply -f <(istioctl kube-inject -f recommendation/kubernetes/Deployment-v2.yml) -n tutorial\n      - 서비스 호출\n        - 브라우저에서 Customer 서비스(Externl-IP:8080 접속) 호출\n        - F5(새로고침)를 10회 이상 클릭하여 다수의 요청 생성\n      - Routing 결과 확인 - Kiali(Externl-IP:20001) 접속\n      - (Recommendation v.2 서비스 Scale Out)\n      - (서비스의 v2 의 replica 를 2로 설정)\n        - kubectl scale --replicas=2 deployment/recommendation-v2 -n tutorial\n        - kubectl get po -n tutorial\n\n      - Customer 서비스를 10회 이상 F5(새로고침)하여 서비스 호출\n      - Routing 결과 확인 - Kiali(Externl-IP:20001) 접속\n    - Advanced Routing\n      - 정책(VirtualService, DestinationRule) 설정\n        - (현, 정책 확인)\n          - kubectl get VirtualService -n tutorial -o yaml\n          - kubectl get DestinationRule -n tutorial -o yaml\n        - (사용자 선호도에 따른 추천 서비스 라우팅 정책 설정)\n        - (VirtualService, DestinationRule 설정, v2로 100% 라우팅)\n          - kubectl create -f istiofiles/destination-rule-recommendation-v1-v2.yml -n tutorial\n          - kubectl create -f istiofiles/virtual-service-recommendation-v2.yml -n tutorial\n        - (설정정책 확인)\n          - kubectl get VirtualService -n tutorial -o yaml\n          - kubectl get DestinationRule -n tutorial -o yaml\n        - (서비스 확인)\n          - 브라우저에서 Customer 서비스(Externl-IP:8080 접속)호출\n          - Kiali(Externl-IP:20001), Jaeger(External-IP:80)에서 모니터링\n      - 가중치 기반 스마트 라우팅 (Canary Deployment)\n        - (recommendation 서비스 v1의 가중치를 100으로 변경)\n          - kubectl replace -f istiofiles/virtual-service-recommendation-v1.yml -n tutorial\n        - (서비스 호출 및 Kiali(Externl-IP:20001)에서 모니터링)\n        - (VirtualService 삭제 시, Round-Robin 방식으로 동작)\n          - kubectl delete -f istiofiles/virtual-service-recommendation-v1.yml -n tutorial\n        - Canary 라우팅 비율별 배포 정책 예시\n          - (90 : 10)\n          - kubectl apply -f istiofiles/virtual-service-recommendation-v1_and_v2.yml -n tutorial\n          - (75 : 25)\n          - kubectl replace -f istiofiles/virtual-service-recommendation-v1_and_v2_75_25.yml -n tutorial\n        - 삭제\n          - kubectl delete dr recommendation -n tutorial\n          - #kubectl delete vs recommendation -n tutorial\n          - kubectl scale --replicas=1 deployment/recommendation-v2 -n tutorial\n      - Client 브라우저 유형별 스마트 라우팅\n        - Firefox 브라우저로 접속 시, v2로 라우팅되도록 설정\n          - kubectl apply -f istiofiles/destination-rule-recommendation-v1-v2.yml -n tutorial\n          - kubectl apply -f istiofiles/virtual-service-firefox-recommendation-v2.yml -n tutorial\n        - (Firefox 브라우저와 다른 브라우저에서 접속 확인)\n        - (Browser 환경이 지원되지 않을 경우,)\n          - curl -A Safari Externl-IP:8080\n          - curl -A Firefox Externl-IP:8080\n        - 삭제\n          - kubectl delete dr recommendation -n tutorial\n          - kubectl delete vs recommendation -n tutorial\n\n  - <b>Lab. Istio - Timeout & Retry</b>\n    - Lab에 필요한 모듈(Message Queue) 설치\n      - kubectl get svc my-kafka -n kafka\n      - 미설치시, 설치 링크 (https://workflowy.com/s/msa/27a0ioMCzlpV04Ib#/a7018fb8c629)\n    - tutorial  네임스페이스에 Istio 기능 추가\n      - kubectl label namespace tutorial istio-injection=enabled --overwrite\n      - 네임스페이스가 없을 시, 생성 후 실행\n    - Lab. Timeout : Fail-Fast를 통한 서비스 Caller 자원 보호\n      - Timeout 테스트를 위해 CNA 과정에서 구현한 Order 마이크로서비스의  코드 보완 및 tutorial 네임스페이스에 배포\n        - Service time delay를 위해, Order Aggregate(Order.java)에 저장전 Thread.sleep 코드 삽입\n          <pre style=\"white-space: pre-wrap\">\n          @PrePersist\n\t          public void onPrePersist(){\n            try {\n                Thread.currentThread().sleep((long) (800 + Math.random() * 220));\n            } catch (InterruptedException e) {\n                e.printStackTrace();\n            }\n          }\n          </pre>\n        - Docker image Build & Push\n        - tutorial 네임스페이스에 Order 서비스 배포\n            <pre style=\"white-space: pre-wrap\">\n            kubectl apply -f - &#60;&#60;EOF\n            apiVersion: apps/v1\n            kind: Deployment\n            metadata:\n              name: order\n              namespace: tutorial\n              labels:\n                app: order\n            spec:\n              replicas: 1\n              selector:\n                matchLabels:\n                  app: order\n              template:\n                metadata:\n                  labels:\n                    app: order\n                spec:\n                  containers:\n                    - name: order\n                      image: IMAGE_FULL_REPOSITORY_URL/order:v2\n                      ports:\n                        - containerPort: 8080\n                      resources:\n                        limits:\n                          cpu: 500m\n                        requests:\n                          cpu: 200m\n            EOF\n            </pre>\n        - Order 서비스 생성\n          - kubectl expose deploy order --port=8080 -n tutorial\n        - Order 서비스 Timeout 설정 (Istio Gateway에서 Order 서비스로 라우팅 시)\n          - (pwd 로 현 위치가 /istio-tutorial/ 인지 확인)\n          - nano customer/kubernetes/Gateway.yaml 오픈 후 마지막 행 다음에 타임아웃 설정이 포함된 아래 내용 추가\n            <pre style=\"white-space: pre-wrap\">\n            - match:\n                - uri:\n                    prefix: /orders\n                route:\n                - destination:\n                    host: order\n                    port:\n                      number: 8080\n                timeout: 3s\n            </pre>\n          - (변경 내용 적용)\n          - kubectl apply -f customer/kubernetes/Gateway.yml -n tutorial\n        - Order 서비스 Timeout 설정 (클라우드 내에서 Order 서비스로 라우팅시)\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n              apiVersion: networking.istio.io/v1alpha3\n              kind: VirtualService\n              metadata:\n                name: vs-order-network-rule\n                namespace: tutorial\n              spec:\n                hosts:\n                - order\n                http:\n                - route:\n                  - destination:\n                      host: order\n                  timeout: 3s\n          EOF\n          </pre>\n        - 부하테스트 툴(Siege) 설치 및 Order 서비스 Load Testing\n          - kubectl run siege --image=apexacme/siege-nginx -n tutorial\n          - kubectl exec -it siege -c siege -n tutorial &#45;&#45; /bin/bash\n          - <pre style=\"white-space: pre-wrap\">siege -c30 -t20S -v --content-type \"application/json\" &#39;http://order:8080/orders POST {\"productId\": \"1001\", \"qty\":5}&#39;</pre>\n      - Order 서비스에 설정된 Timeout을 임계치를 초과하는 순간, Istio에서 서비스로의 연결을 자동 차단하는 것을 확인\n    - Lab. Retry : 5xx 오류를 리턴받게 되면, Envoy Proxy에서 설정한 횟수만큼 대상 서비스를 재호출하여 일시적인 장애였는지를 다시 확인하는 Rule\n      - Retry 테스트를 위해 CNA 과정에서 구현한 Delivery 마이크로서비스를 tutorial 네임스페이스에 배포\n      - Docker image Build & Push\n      - tutorial 네임스페이스에 Delivery 서비스 배포\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: apps/v1\n          kind: Deployment\n          metadata:\n            name: delivery\n            namespace: tutorial\n            labels:\n              app: delivery\n          spec:\n            replicas: 1\n            selector:\n              matchLabels:\n                app: delivery\n            template:\n              metadata:\n                labels:\n                  app: delivery\n              spec:\n                containers:\n                  - name: delivery\n                    image: IMAGE_FULL_REPOSITORY_URL/delivery:v1\n                    ports:\n                      - containerPort: 8080\n                    resources:\n                      limits:\n                        cpu: 500m\n                      requests:\n                        cpu: 200m\n        EOF\n        </pre>\n      - Delivery 서비스 생성\n        - kubectl expose deploy delivery --port=8080 -n tutorial\n      - Order 서비스에 Retry Rule 추가 적용\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: networking.istio.io/v1alpha3\n          kind: VirtualService\n          metadata:\n            name: vs-order-network-rule\n            namespace: tutorial\n          spec:\n            hosts:\n            - order\n            http:\n            - route:\n              - destination:\n                  host: order\n              timeout: 3s\n              retries:\n                attempts: 3\n                perTryTimeout: 2s\n                retryOn: 5xx,retriable-4xx,gateway-error,connect-failure,refused-stream\n        EOF\n        </pre>\n      - Delivery 서비스를 정지하고, 이를 동기호출하는 Order 서비스 API 호출\n        - kubectl scale deploy delivery --replicas=0 -n tutorial\n        - kubectl exec -it siege -c siege -n tutorial --/bin/bash\n        - http http://order:8080/orders/ productId=1001 qty=5\n          - httpie가 없을 시,\n          - apt-get update\n          - apt-get install httpie\n        - http DELETE http://order:8080/orders/1\n      - Jaeger 접속(http://tracing svc EXTERNAL-IP :80) 후, Retry 횟수 확인하기\n        \"< 검색조건 >\n          Service : order.tutorial, Operation : delivery.tutorial.svc.cluster.local:8080/*\n          검색결과 : 총 Retry 횟수 + 1 의 Requests 로깅\"\n  - <b>Lab. Istio - Circuit Breaker</b>\n    - Circuit Breaker : 장애 인스턴스를 회피하는 기능으로 5xx 오류를 리턴한 인스턴스를  라우팅 대상에서 일정시간 만큼 제외 (Pool Ejection)\n    - Namespace 생성 및 Istio 활성\n      - kubectl create namespace istio-cb-ns\n      - kubectl label namespace istio-cb-ns istio-injection=enabled\n    - Istio Retry 디폴트 동작 확인\n      - 테스트 어플리케이션 배포\n        - hello-server-1, hello-server-2 Pods, Service\n        - hello-server 앱은 env:RANDOM_ERROR 값의 확률로 랜덤하게 503 에러를 발생하는 로직이 포함\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n              apiVersion: v1\n              kind: Pod\n              metadata:\n                name: hello-server-1\n                namespace: istio-cb-ns\n                labels:\n                  app: hello\n              spec:\n                containers:\n                - name: hello-server-1\n                  image: docker.io/honester/hello-server:latest\n                  imagePullPolicy: IfNotPresent\n                  env:\n                  - name: VERSION\n                    value: \"v1\"\n                  - name: LOG\n                    value: \"1\"\n              &#45;&#45;&#45;\n              apiVersion: v1\n              kind: Pod\n              metadata:\n                name: hello-server-2\n                namespace: istio-cb-ns\n                labels:\n                  app: hello\n              spec:\n                containers:\n                - name: hello-server-2\n                  image: docker.io/honester/hello-server:latest\n                  imagePullPolicy: IfNotPresent\n                  env:\n                  - name: VERSION\n                    value: \"v2\"\n                  - name: LOG\n                    value: \"1\"\n                  - name: RANDOM_ERROR\n                    value: \"0.2\"\n              &#45;&#45;&#45;\n              apiVersion: v1\n              kind: Service\n              metadata:\n                name: svc-hello\n                namespace: istio-cb-ns\n                labels:\n                  app: hello\n              spec:\n                selector:\n                  app: hello\n                ports:\n                - name: http\n                  protocol: TCP\n                  port: 8080\n          EOF\n          </pre>\n        - 클라이언트용 서비스(httpbin) 배포\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n              apiVersion: apps/v1\n              kind: Deployment\n              metadata:\n                name: httpbin\n                namespace: istio-cb-ns\n              spec:\n                replicas: 1\n                selector:\n                  matchLabels:\n                    app: httpbin\n                template:\n                  metadata:\n                    labels:\n                      app: httpbin\n                  spec:\n                    containers:\n                    - name: httpbin\n                      image: docker.io/honester/httpbin:latest\n                      imagePullPolicy: IfNotPresent\n                      ports:\n                      - containerPort: 80\n              &#45;&#45;&#45;\n              apiVersion: v1\n              kind: Service\n              metadata:\n                name: httpbin\n                namespace: istio-cb-ns\n                labels:\n                  app: httpbin\n              spec:\n                selector:\n                  app: httpbin\n                ports:\n                - name: http\n                  port: 8000\n                  targetPort: 80\n          EOF\n          </pre>\n      - Retry 디폴트 동작 테스트\n        - hello-server-2의 로그 모니터 걸기\n          - kubectl logs -f hello-server-2 -c hello-server-2 -n istio-cb-ns\n        - 클라이언트에서 svc-hello 서비스 10번 호출하기\n          - for i in {1..10}; do kubectl exec -it httpbin -c httpbin -n istio-cb-ns --curl http://svc-hello.istio-cb-ns:8080; sleep 0.1; done\n      - 결과 확인/분석\n        \"1) 서비스 호출은 Round Robin으로 로드 밸런싱되나, 프로세싱 시간에 따라 동일한 서비스가 연속 2회 로깅 될 수 있음\n          2) 핵심포인트는, Server-2가 5xx 오류를 리턴할 경우, 자동으로 Retry되어 Server-1 로그가 연달아 출력된다는 점임. (Default Retry : 2회)\"\n    - Circuit Breaker 설정\n      - 대기 쓰레드수 기반 Circuit Breaker\n        - 클라이언트용 서비스(httpbin)에 쓰레드 기반 Circuit Breaker 설정\n        - (Pending Thread가 많을수록 경우, 오랫동안 큐잉된 요청은 Response time이 증가하게 되므로, 적절한 대기 쓰레드를 풀을 적용하여 Circuit Breaking)\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n            apiVersion: networking.istio.io/v1alpha3\n            kind: DestinationRule\n            metadata:\n              name: dr-httpbin\n              namespace: istio-cb-ns\n            spec:\n              host: httpbin\n              trafficPolicy:\n                connectionPool:\n                  http:\n                    http1MaxPendingRequests: 1\n                    maxRequestsPerConnection: 1\n          EOF\n          </pre>\n        - Circuit Breaker 동작 확인\n          - 부하테스트 툴(Siege) 설치 및  Load Testing\n            - kubectl run siege --image=apexacme/siege-nginx -n istio-cb-ns\n            - kubectl exec -it siege -c siege -n istio-cb-ns --/bin/bash\n              - siege -c1 -t10S -v http://httpbin:8000/get  # 100% availability\n              - siege -c2 -t10S -v http://httpbin:8000/get  # 87% availability\n          - Kiali(Externl-IP:20001) 모니터링\n      - 로드 밸런싱 풀(pool) 인스턴스의 Health Status 기반 Circuit Breaker\n        - Hello 서비스의 로드 밸런싱 풀(pool)의 인스턴스 상태기반 Circuit Breaker 설정\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n            apiVersion: networking.istio.io/v1alpha3\n            kind: DestinationRule\n            metadata:\n              name: dr-hello-server\n              namespace: istio-cb-ns\n            spec:\n              host: svc-hello\n              trafficPolicy:\n                outlierDetection:\n                  interval: 1s\n                  consecutiveErrors: 1\n                  baseEjectionTime: 3m\n                  maxEjectionPercent: 100\n          EOF\n          </pre>\n        - Circuit Breaker 동작 확인\n          - 클라이언트(httpbin Pod)에서 svc-hello 호출\n            - hello-server-2의 로그 모니터 걸기\n              - kubectl logs -f hello-server-2 -c hello-server-2 -n istio-cb-ns\n            - 클라이언트에서 svc-hello 서비스 10번 호출하기\n              - for i in {1..10}; do kubectl exec -it httpbin -c httpbin -n istio-cb-ns --curl http://svc-hello.istio-cb-ns:8080; sleep 0.1; done\n            - 결과 확인/분석\n              \"1) 5초 동안 5xx 에러가 2번 발생할 경우, Server-2로는 5분 동안 트래픽이 라우팅 되지 않는다.\n                2) 모니터링 시스템(Kiali) : EXTERNAL-IP:20001 에서 Circuit Breaker 뱃지 발생 확인\"\n    - 동기호출 Target인 배송(Delivery) 서비스에 Circuit Breaker 설정하기\n      - Thread 부하 및 5XX 오류에 대해 서비스를 차단하는 Circuit Breaker 생성\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: networking.istio.io/v1alpha3\n          kind: DestinationRule\n          metadata:\n            name: dr-delivery\n            namespace: tutorial\n          spec:\n            host: delivery\n            trafficPolicy:\n              connectionPool:\n                http:\n                  http1MaxPendingRequests: 1\n                  maxRequestsPerConnection: 1\n              outlierDetection:\n                interval: 5s\n                consecutiveErrors: 1\n                baseEjectionTime: 5m\n                maxEjectionPercent: 100\n        EOF\n        </pre>\n      - 설정 내용\n        - 최대 활성 연결 갯수 30개와 최대 요청 대기 수를 100개로 지정하고, 이 임계점을 넘어가는 추가 요청은 거부(circuit break)\n        - 5초 동안 2번 5xx을 리턴한 서비스는 5분 동안 라우팅 대상에서 제외(Ejection)\n        - 또한, 모든 대상 서비스 인스턴스가 방출(제외)될 수 있음\n    - Clean-up\n      - kubectl delete pod/hello-server-1 pod/hello-server-2 pod/httpbin service/svc-hello dr/dr-hello -n istio-cb-ns\n  - <b>Clear Istio </b>\n    - kubectl delete ns tutorial istio-cb-ns istio-system\n\n </details>\n\n <details>\n<summary><b>Backup</b></summary>\n\n  - <b>Container로부터 이미지 생성</b>\n    - 이미지 생성\n      - docker run --name my-nginx -d -p 80:80 nginx\n      - docker exec -it my-nginx /bin/bash\n        - apt-get update\n        - apt-get install curl\n        - cd /usr/share/nginx/html\n        - <pre style=\"white-space: pre-wrap\">echo \"Hello my name is PYJ.\" >> index.html</pre>\n        - exit\n      - docker commit my-nginx my-nginx:1.0 # 컨테이너를 이미지로 생성\n      - docker diff [실행중인 Container ID] #원본 이미지와의 차이점 확인\n      - docker commit -a \"apex@naver.com\" -m \"update nginx\" my-nginx my-nginx:1.0\n      - docker images\n      - docker stop my-nginx\n      - docker run --name my-nginx2 -p 80:80 -d my-nginx:1.0\n      - http://localhost 확인\n      - docker stop my-nginx2\n\n    - 이미지 푸시\n      - docker tag my-nginx:1.0 apexacme/my-nginx:1.0\n      - docker images\n      - docker push apexacme/my-nginx:1.0\n      - http://hub.docker.com 에서 이미지 확인\n    - 도커허브 이미지로부터 컨테이너 실행\n      - docker run --name new-nginx -d -p 80:80 apexacme/my-nginx:1.0\n  - <b>샘플 자바 애플리케이션 패키징과 배포 </b>\n    - (pwd 로 현 위치가 /container-orchestration/ 인지 확인)\n    - git clone https://github.com/event-storming/monolith.git\n    - cd monolith/\n    - ls\n    - (skip) mvn spring-boot:run #Maven으로 App. 실행\n    - mvn package -B -Dmaven.test.skip=true\n    - (skip) java -jar target/monolith-0.0.1.BUILD-SNAPSHOT.jar #Java로 App. 실행\n    - cat Dockerfile # 도커파일 내용 확인\n    - (도커라이징)\n      - docker build -t (Azure container registry명).azurecr.io/monolith:v1 .\n        - #주의1 : 명령  맨끝에 \" .\" 빼먹으면 안됨.   Dockerfile 의 위치인\n        - #주의2 : project id 부분을 자신의 GCP project id 로 변경!!\n        - #주의3 : 현재 연결된 kubernetes 클러스터와 동일한 프로젝트 id 여야만 gcr registry 접근이 가능함\n      - docker images\n      - (skip) docker run (Azure container registry명).azurecr.io/monolith:v1 #Docker로 App. 실행\n      - docker push (azure container registry명).azurecr.io/monolith:v1\n    - kubectl create deploy monolith --image=(azure container registry명).azurecr.io/monolith:v1\n    - kubectl get po -l app=monolith\n    - kubectl expose deploy monolith --type=\"LoadBalancer\" --port=8080\n    - kubectl get svc -w\n    - 자바 애플리케이션 접속\n      - http://(Service_Extern-IP):8080\n  - <b>Lab. Circuit Breaker function</b>\n    - Istio가 활성화된 네임스페이스 생성\n      - kubectl create namespace istio-cb-ns\n      - kubectl label namespace istio-cb-ns istio-injection=enabled\n    - [CB 유스케이스] #1. Connection Max & Pending 수에 따른 Circuit Breaker\n      - 테스트 어플리케이션 배포 (Deployment, Service)\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: apps/v1\n          kind: Deployment\n          metadata:\n            name: httpbin\n            namespace: istio-cb-ns\n          spec:\n            replicas: 1\n            selector:\n              matchLabels:\n                app: httpbin\n            template:\n              metadata:\n                labels:\n                  app: httpbin\n              spec:\n                containers:\n                - name: httpbin\n                  image: docker.io/honester/httpbin:latest\n                  imagePullPolicy: IfNotPresent\n                  ports:\n                  - containerPort: 80\n          &#45;&#45;&#45;\n          apiVersion: v1\n          kind: Service\n          metadata:\n            name: httpbin\n            namespace: istio-cb-ns\n            labels:\n              app: httpbin\n          spec:\n            selector:\n              app: httpbin\n            ports:\n            - name: http\n              port: 8000\n              targetPort: 80\n        EOF\n        </pre>\n      - 로드 테스트 툴(siege) 배포\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: v1\n          kind: Pod\n          metadata:\n            name: siege\n            namespace: istio-cb-ns\n          spec:\n            containers:\n            - name: siege\n              image: apexacme/siege-nginx\n        EOF\n        </pre>\n      - siege를 통한 서비스(httpbin) 부하 생성\n        - kubectl exec -it siege --container siege -n istio-cb-ns --/bin/bash\n        - siege -c1 -t10S -v http://httpbin:8000/get\n        - siege -c1 -t10S -v http://httpbin.istio-cb-ns:8000/get\n        - siege -c1 -t10S -v http://httpbin.istio-cb-ns.svc.cluster.local:8000/get\n        - 서비스 모니터링 (Kiali) : EXTERNAL-IP:20001 (admin/admin)\n      - DestinationRule 를 생성하여 CB가 발생할 수 있도록 Connection pool 설정\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: networking.istio.io/v1alpha3\n          kind: DestinationRule\n          metadata:\n            name: dr-httpbin\n            namespace: istio-cb-ns\n\n          spec:\n            host: httpbin\n            trafficPolicy:\n              connectionPool:\n                http:\n                  http1MaxPendingRequests: 1\n                  maxRequestsPerConnection: 1\n        EOF\n        </pre>\n        - http1MaxPendingRequests=1 : Queue에서 Connection pool 에 연결을 기다리는 request 수를 1개로 제한\n        - maxRequestsPerConnection=1 : keep alive 기능 disable\n      - siege를 통한 서비스(httpbin) 부하 재생성 및 CB 확인\n        - siege -c1 -t10S -v http://httpbin:8000/get  # 100% Availability\n        - siege -c2 -t10S -v http://httpbin:8000/get  # 87% availability\n          - Envoy will return HTTP 503. It is the responsibility of the application to implement any fallback logic that is needed to handle the HTTP 503 error code from an upstream service. (https://istio-releases.github.io/v0.1/docs/concepts/traffic-management/handling-failures.html)\n        - 모니터링 시스템(Kiali) : EXTERNAL-IP:20001 에서 Circuit Breaker 발생 확인 (뱃지)\n      - Circuit Breaker 제거 후, 동일 로드 생성 후, Availability 100% 확인\n        - kubectl delete dr/dr-httpbin -n istio-cb-ns\n        - siege -c2 -t10S -v http://httpbin:8000/get  # 100% availability\n      - Clean-up\n        - kubectl delete deployment.apps/httpbin service/httpbin -n istio-cb-ns\n    - [CB 유스케이스] #2. Load balancing pool의 인스턴스 상태에 기반한 Circuit Breaker\n      - 테스트 어플리케이션 배포 (hello-server-1, hello-server-2 Pods, Service)\n      - (hello-server:latest 이미지는 env:RANDOM_ERROR 값의 확률로 랜덤하게 503 에러를 발생하는 로직이 포함)\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: v1\n          kind: Pod\n          metadata:\n            name: hello-server-1\n            namespace: istio-cb-ns\n            labels:\n              app: hello\n          spec:\n            containers:\n            - name: hello-server-1\n              image: docker.io/honester/hello-server:latest\n              imagePullPolicy: IfNotPresent\n              env:\n              - name: VERSION\n                value: \"v1\"\n              - name: LOG\n                value: \"1\"\n          &#45;&#45;&#45;\n          apiVersion: v1\n          kind: Pod\n          metadata:\n            name: hello-server-2\n            namespace: istio-cb-ns\n            labels:\n              app: hello\n          spec:\n            containers:\n            - name: hello-server-2\n              image: docker.io/honester/hello-server:latest\n              imagePullPolicy: IfNotPresent\n              env:\n              - name: VERSION\n                value: \"v2\"\n              - name: LOG\n                value: \"1\"\n              - name: RANDOM_ERROR\n                value: \"0.2\"\n          &#45;&#45;&#45;\n          apiVersion: v1\n          kind: Service\n          metadata:\n            name: svc-hello\n            namespace: istio-cb-ns\n            labels:\n              app: hello\n          spec:\n            selector:\n              app: hello\n            ports:\n            - name: http\n              protocol: TCP\n              port: 8080\n        EOF\n        </pre>\n      - 클라이언트용 Pod 설치\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: v1\n          kind: Pod\n          metadata:\n            name: httpbin\n            namespace: istio-cb-ns\n            labels:\n              app: httpbin\n          spec:\n            containers:\n            - name: httpbin\n              image: docker.io/honester/httpbin:latest\n              imagePullPolicy: IfNotPresent\n        EOF\n        </pre>\n      - 클라이언트(httpbin Pod)에서 svc-hello 호출(default, Round-Robin)\n        - (hello-server-2의 로그 모니터링)\n        - kubectl logs -f hello-server-2 -c hello-server-2 -n istio-cb-ns\n        - (클라이언트에서 svc-hello 서비스 10번 호출하기)\n        - for i in {1..10}; do kubectl exec -it httpbin -c httpbin -n istio-cb-ns --curl http://svc-hello.istio-cb-ns:8080; sleep 0.1; done\n      - 5XX 오류에 대해 해당 서비스 차단 및 Thread 부하에 따른 DestinationRule 생성\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: networking.istio.io/v1alpha3\n          kind: DestinationRule\n          metadata:\n            name: dr-hello\n            namespace: istio-cb-ns\n          spec:\n            host: svc-hello\n            trafficPolicy:\n              connectionPool:\n                http:\n                  http1MaxPendingRequests: 5\n                  maxRequestsPerConnection: 10\n              outlierDetection:\n                interval: 1s\n                consecutiveErrors: 1\n                baseEjectionTime: 3m\n                maxEjectionPercent: 100\n        EOF\n\n        설명 : 1초 주기로 이상징후를 체크하며, 1번이라도 실패한 서비스는 3분동안 라우팅 대상에서 제외된다. 또한 모든 대상 서비스 인스턴스가 방출(제외)될 수 있다.\"\n        </pre>\n      - 클라이언트(httpbin Pod)에서 svc-hello 호출 및 CB 확인\n        - (hello-server-2의 로그 모니터링)\n        - kubectl logs -f hello-server-2 -c hello-server-2 -n istio-cb-ns\n        - (클라이언트에서 svc-hello 서비스 10번 호출하기)\n        - for i in {1..10}; do kubectl exec -it httpbin -c httpbin -n istio-cb-ns --curl http://svc-hello.istio-cb-ns:8080; sleep 0.1; done\n        - 모니터링 시스템(Kiali) : EXTERNAL-IP:20001 에서 Circuit Breaker 발생 확인 (뱃지)\n      - 배송(delivery) 서비스에 Circuit Breaker 설치\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: networking.istio.io/v1alpha3\n          kind: DestinationRule\n          metadata:\n            name: dr-delivery\n            namespace: cna-shop\n          spec:\n            host: delivery\n            trafficPolicy:\n              connectionPool:\n                http:\n                  http1MaxPendingRequests: 5\n                  maxRequestsPerConnection: 10\n              outlierDetection:\n                interval: 1s\n                consecutiveErrors: 1\n                baseEjectionTime: 3m\n                maxEjectionPercent: 100\n        EOF\n        </pre>\n     - Clean-up\n      - kubectl delete pod/hello-server-1 pod/hello-server-2 pod/httpbin service/svc-hello dr/dr-hello -n istio-cb-ns\n\n  - <b>Lab. Istio Egress</b>\n    - 외부 도메인을 호출하는 v3 버전을 배포\n      - kubectl apply -f <(istioctl kube-inject -f recommendation/kubernetes/Deployment-v3.yml) -n tutorial\n    - 브라우저에서 Customer 서비스(Externl-IP:8080 접속)\n    - v3에서 날짜정보가 추가로 출력됨을 확인\n    - Istio 트래픽을 등록된 것만 허용하도록 변경\n      - <pre style=\"white-space: pre-wrap\">kubectl get configmap istio -n istio-system -o yaml | sed \"s/mode: ALLOW_ANY/mode: REGISTRY_ONLY/g\" | kubectl replace -n istio-system -f -</pre>\n    - 브라우저에서 Customer 서비스(Externl-IP:8080 접속)\n      - v3 은 서비스 오류로 인해 브라우저 확인 불가, Kiali 에서 확인\n    - 트래픽을 모두 v3 (weigh 100)로 라우팅하고 에러 화면 확인\n      - kubectl create -f istiofiles/destination-rule-recommendation-v1-v2-v3.yml -n tutorial\n      - kubectl create -f istiofiles/virtual-service-recommendation-v3.yml -n tutorial\n    - 브라우저에서 Customer 서비스(Externl-IP:8080 접속)\n      - 화면에 Error Log 출력 : “customer => Error: 503 - preference => Error: 500”\n    - 외부 도메인을 허용해 주는 ServiceEntry 를 생성하여 정상 접속 허용\n      - kubectl create -f istiofiles/service-entry-egress-worldclockapi.yml -n tutorial\n      - 브라우저에서 Customer 서비스(Externl-IP:8080 접속) - 정상 출력\n    - (테스트 후, 설정 복구)\n    - <pre style=\"white-space: pre-wrap\">kubectl get configmap istio -n istio-system -o yaml | sed \"s/mode: REGISTRY_ONLY/mode: ALLOW_ANY/g\" | kubectl replace -n istio-system -f -</pre>\n\n</details>\n\n</p>\n</details>\n<hr />\n\n\n### 아마존 AWS\n<details>\n<summary>AWS Cloud 기반의 Container Orchestration Lab. Scripts</summary>\n<p>\n\n<details>\n<summary><b>자주 사용되는 AWS Cloud 명령어</b></summary>\n\n  - <b>Cloud Configuration</b>\n    - aws configure\n\n  - <b>AWS 클러스터 생성</b>\n    - eksctl create cluster --name (Cluster-Name) --version 1.17 --nodegroup-name standard-workers --node-type t3.medium --nodes 3 --nodes-min 1 --nodes-max 3\n  - <b>AWS 클러스터 토큰 가져오기</b>\n    - aws eks --region (Region-Code) update-kubeconfig --name (Cluster-Name)\n  - <b>AWS 컨테이너 레지스트리에 이미지 리파지토리 생성</b>\n    - aws ecr create-repository --repository-name (Image-Repository-Name) --image-scanning-configuration scanOnPush=true --region (Region-Code)\n  - <b>AWS 컨테이너 레지스트리 로그인</b>\n    - aws ecr get-login-password --region (Region-Code) | docker login --username AWS --password-stdin (Account-Id).dkr.ecr.(Region-Code).amazonaws.com\n    - 오류(unknown flag: --password-stdin) 발생 시,\n      - docker login --username AWS -p $(aws ecr get-login-password --region (Region-Code)) (Account-Id).dkr.ecr.(Region-Code).amazonaws.com/\n  - <b>AWS 레지스트리에 도커 이미지 푸시하기</b>\n    - aws ecr create-repository --repository-name (IMAGE_NAME) --region ap-northeast-2\n    - docker push 283210891307.dkr.ecr.ap-northeast-2.amazonaws.com/(IMAGE_NAME):latest\n  - <b>AWS 클러스터 삭제</b>\n    - eksctl delete cluster --name (PROD_CLUSTER)\n  - <b>AWS EKS 모니터링</b>\n    - Metric-Server 설치\n      - kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yaml\n    - Kubernetes DashBoard 설치\n      - kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml\n    - eks-admin 서비스 계정 및 클러스터 롤 바인딩 생성\n      <pre style=\"white-space: pre-wrap\">\n      cat &#60;&#60;EOF | kubectl apply -f -\n        apiVersion: v1\n        kind: ServiceAccount\n        metadata:\n          name: eks-admin\n          namespace: kube-system\n        &#45;&#45;&#45;\n        apiVersion: rbac.authorization.k8s.io/v1beta1\n        kind: ClusterRoleBinding\n        metadata:\n          name: eks-admin\n        roleRef:\n          apiGroup: rbac.authorization.k8s.io\n          kind: ClusterRole\n          name: cluster-admin\n        subjects:\n        - kind: ServiceAccount\n          name: eks-admin\n          namespace: kube-system\n      EOF\n      </pre>\n    - 인증 토큰 조회\n      - <pre style=\"white-space: pre-wrap\">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep eks-admin | awk \"{print $1}\")</pre>\n    - Proxy 설정 및 DashBoard 연결\n      - kubectl proxy\n      - http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#!/login\n      - 복사한 토큰 정보로 로그인\n</details>\n\n### Docker\n\n<details>\n<summary><b>Setup</b></summary>\n\n- <b>관리자 권한으로 PowerShell 실행</b>\n  - Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\n- <b>리눅스설치 및 실행</b>\n  - (설치전 확인 사항)\n    - Windows 기능 켜기/끄기에서 \"Linux용 Windows 하위시스템 활성화\" 확인\n    - 개발자 기능사용에서 \"개발자 모드\" 활성화 확인\n  - Ubuntu의 Archive Repository Server를 (빠른 패키지 설치를 위해) 국내로 설정\n    - sudo vi /etc/apt/sources.list\n    - :%s/archive.ubuntu.com/ftp.daumkakao.com/g\n    - :wq!\n    - sudo apt-get update\n\n- <b>Linux에 JDK 설치</b>\n  - (설치 명령)\n\n  - sudo apt-get update\n  - sudo apt install default-jdk\n  - (bash에 환경변수 추가)\n  -  cd ~\n  - nano .bashrc\n  - (맨아래로 이동)\n  - (JAVA_HOME 설정 및 실행 Path 추가)\n    <pre style=\"white-space: pre-wrap\">\n    export JAVA_HOME=\"/usr/lib/jvm/java-11-openjdk-amd64\"\n    export PATH=$PATH:$JAVA_HOME/bin:.\n    </pre>\n  - (수정사항 반영)\n    - ctrl + x, y 입력, 종료\n    - source ~/.bashrc\n  - (설치 확인)\n  - echo $JAVA_HOME\n  - java -version\n- <b>Windows에 도커 데몬 설치</b>\n  - https://www.docker.com/products/docker-desktop\n- <b>도커허브 계정생성</b>\n  -  http://hub.docker.com 접속 후, Sign Up (회원가입)\n- <b>리눅스에 도커 Client 설치</b>\n  - sudo apt-get update\n  - 비밀번호 입력창에 skadmin1234\n  - sudo apt install apt-transport-https ca-certificates curl software-properties-common\n  - curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add\n  - sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable\"\n  - sudo apt update\n  - sudo apt install docker-ce\n  - #리눅스 설치시 생성한 사용자 명 입력\n  - sudo usermod -aG docker skccadmin\n- <b>도커 데몬과 도커 Client 연결 </b>\n  - cd\n  - nano .bashrc\n  - 맨아래 줄에 아래 환경변수 추가\n    - 방향키로 맨 아래까지 내린 다음, 새로운 행에 아래 내용 입력\n    - export DOCKER_HOST=tcp://0.0.0.0:2375\n    - 저장 & 종료 : Ctrl + x, 입력 후, y 입력  후 엔터\n  - source ~/.bashrc\n  - 연결 확인\n    - docker images\n    - docker run --name nginx -d -p 80:80 nginx\n    - docker images\n</details>\n\n<details>\n<summary><b>Docker Hands-on</b></summary>   </b>\n\n- <b>Lab. image</b>\n\n  - 이미지 Pull\n    - docker pull hello-world\n    - docker images\n    - docker pull nginx\n    - docker pull nginx:latest\n    - docker pull docker.io/library/nginx:latest\n    - docker pull nginx:1.16.1\n    - docker images\n\n  - 도커허브 (Docker Hub)\n    - http://hub.docker.com # 접속 후, nginx 검색\n  - 이미지 Tagging\n    - docker image tag nginx my-nginx # Create 태그\n  - 이미지 삭제\n    - docker image rm my-nginx\n    - docker image rm hello-world\n    - docker image rm $(docker images -q) # 한번에 모든 도커 이미지 지우기\n- <b>Lab. container</b>\n  - 컨테이너 생성\n    - docker run hello-world # 컨테이너 만들기\n    - docker run --name hello hello-world # 이름 지정, 미지정시 임의의 이름으로 생성\n    - docker run --name my-nginx -d -p 80:80 nginx\n    - docker ps\n  - 컨테이너 시작/종료\n    - docker stop my-nginx\n    - docker start my-nginx\n  - 컨테이너 포트 노출\n    - http://localhost 에서 nginx index.html 확인\n    - docker container rm my-nginx\n    - docker run --name my-nginx -d -p 8080:80 nginx\n    - http://localhost:8080 에서 nginx index.html 확인\n  - 컨테이너 접근\n    - docker exec my-nginx cat /usr/share/nginx/html/index.html #실행 중 컨테이너 접근\n    - docker exec -i -t  my-nginx /bin/bash\n      - apt-get update\n      - apt-get install curl\n      - curl localhost\n      - exit\n  - 컨테이너 삭제\n    - docker container rm my-nginx # 실행 중 컨테이너  삭제 시, 오류\n    - docker container rm $(docker ps -a -q) # 한번에 모든 컨테이너 지우기\n- <b>Lab. Docker Build & Push</b>\n  - Dockerfile로부터 이미지 생성\n    - Dockerfile & 리소스 생성\n      - mkdir Dockerfile\n      - cd Dockerfile\n      - nano index.html\n        <pre style=\"white-space: pre-wrap\">\"Hi~ My name is Park Yong Joo..\"</pre>\n        - 저장 및 종료 (Ctrl + X, y 입력 후 엔터)\n        - nano Dockerfile\n          <pre style=\"white-space: pre-wrap\">\n          FROM nginx\n          COPY index.html /usr/share/nginx/html/\n          </pre>\n      - 저장 및 종료 (Ctrl + x, Y 입력 후 엔터)\n    - 도커라이징 & Push\n      - docker build -t Docker-ID/my-nginx:v1 .\n      - docker images\n      - docker push Docker-ID/my-nginx:v1\n        \"denied: 권한오류 생성 시, docker login 명령으로 Docker Hub에 로그인해 준다.\"\n    - http://hub.docker.com 에서 이미지 확인\n    - Docker Hub 이미지로부터 컨테이너 실행\n      - docker run --name new-nginx -d -p 80:80 Docker-ID/my-nginx:v1\n    - Browser에서 실행 애플리케이션 확인\n      - http://localhost:8080\n- <b>Clear</b>\n  - docker container rm $(docker ps -a -q)\n    - container 삭제 전, 실행 중인 컨테이너를 정지시켜 준다.\n    - docker container stop new-nginx\n  - docker image rm -f $(docker images -q)\n</details>\n\n### Kubernetes\n\n<details>\n<summary><b>Cloud Client Setup</b></summary>\n\n- <b>Kubectl 설치 (ubuntu 18.04)</b>\n  - sudo apt-get update && sudo apt-get install -y apt-transport-https\n  - curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n  - echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list\n  - sudo apt-get update\n  - sudo apt-get install -y kubectl\n\n- <b>AWS-Cli v2 설치</b>\n  - curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n  - unzip awscliv2.zip\n  - (unzip이 없을 경우, 설치)\n    - sudo apt-get install unzip\n  - sudo ./aws/install\n- <b>AWS Configure</b>\n  - (AWS 관리콘솔)\n  - 1. 부여받은 교육 계정으로  AWS 콘솔 접속\n  - 2. IAM 서비스 접속\n  - 3. 왼쪽 메뉴에서 \"엑세스 관리\" > \"사용자\" 클릭\n  - 4. 나의 계정정보 클릭\n  - 5. 메인화면에서 \"보안자격증명\" 클릭\n  - 6. \"액세스 키 만들기\" 클릭\n  - 7. Access Key ID와 Secret Access key를 복사\n  - (클라이언트 Tool)\n  - aws configure 입력\n  - 관리콘솔에 복사한 Access Key ID와 Secret Access key 입력\n  - region 정보에 ap-northeast-2 입력\n  - default output format에 json 입력\n- <b>EKS Client (eksctl) 설치</b>\n  - curl --location \"https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\n  - sudo mv /tmp/eksctl /usr/local/bin\n- <b>Amazon EKS 생성</b>\n  - eksctl create cluster --name user15-sk-Cluster --version 1.15 --nodegroup-name standard-workers --node-type t3.medium --nodes 2 --nodes-min 1 --nodes-max 3\n- <b>Local에 EKS 클러스터 접속정보 설정</b>\n  - aws eks --region eu-west-1 update-kubeconfig --name admin00-Cluster\n  - kubectl config current-context\n  - kubectl get all\n- <b>Local에 ECR(Elastic Container Registry) 인증 및 토큰 설정</b>\n  - aws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin (Account-ID).dkr.ecr.ap-northeast-2.amazonaws.com\n  - 오류(unknown flag: --password-stdin) 발생 시,\n    - docker login --username AWS -p $(aws ecr get-login-password --region ca-central-1) 052937454741.dkr.ecr.ca-central-1.amazonaws.com/\n</details>\n\n<details>\n<summary><b>Kubernetes Hands-on</b></summary>\n\n- <b>Lab에 필요한 리소스 내려받기</b>\n  - git clone https://github.com/acmexii/mall.git\n  - git clone https://github.com/event-storming/container-orchestration.git\n  - cd container-orchestration\n  - cd yaml_aws\n\n- <b>Lab. K8s Sample App 생성</b>\n  - 어플리케이션 생성/ 확인\n    - Docker hub에 올린 이미지를 통한 컨테이너 생성\n      kubectl create deploy my-nginx --image=apexacme/my-nginx:v1\"\n    - 클러스터 외부에 노출하기\n      kubectl expose deploy my-nginx --type=LoadBalancer --port=80\"\n  - 서비스 확인하기\n    - kubectl get svc의 EXTERNAL-IP 복사\n    - Browser에서 EXTERNAL-IP:80 접속\n\n- <b>Lab. Pod & 기본명령</b>\n  - kubectl get nodes\n    - 쿠버네티스에 제대로 접속했는지 확인\n    - 현 클러스터의 워크노드를 리스트업\n    - 접속 결과 안나오는 경우\n      - kubectl config current-context 명령으로 Cluster 접속 확인\n  - 객체의 검색\n    - kubectl get [object type]\n    - kubectl get pods   # pods = pod = po\n    - kubectl get deployments   # deploy\n    - kubectl get services    # svc\n    - kubectl get replicaset    # rs\n  - 객체의 모니터링\n    - watch kubectl get all\n    - kubectl get pod -w\n    - watch kubectl get pod\n  - 객체의 유형\n    - Service\n      - types\n        - LoadBalancer\n          - 클라우드 제공자에 의해 제공된 Loadbalancer 로 노출\n          - front-end 혹은 ingress (api gateway)\n        - ClusterIP(default) / NodePort\n          - 클러스터 내부 IP\n          - 내부 마이크로 서비스\n    - Deployment\n      - ReplicaSet (하나이상 생성)\n        - Pod (하나이상 생성)\n          - Container (docker ) 하나이상.\n    - Pod\n    - ReplicaSet\n    - Ingress\n    - Secret\n    - ConfigMap\n    - ServiceAccount = sa\n    - statefulset\n    - daemonset\n  - 설정파일(YAML)을 통한 Pod 배포 (직접 타이핑)\n    - nano declarative-pod.yaml\n      <pre style=\"white-space: pre-wrap\">\n      apiVersion: v1\n      kind: Pod\n      metadata:\n        name: declarative-pod\n        labels:\n          env: test\n      spec:\n        containers:\n        - name: memory-demo-ctr\n          image: nginx\n      </pre>\n    - 저장 및 종료 (ctrl + X, Y, 엔터)\n    - kubectl create -f declarative-pod.yaml\n    - kubectl get pods\n  - 원하는 Node 타입에 Pod 생성\n    - #pwd 로 현 위치가 /container-orchestration/yaml_aws/pod 인지 확인\n    - kubectl create -f pod-with-nodeselector.yaml\n    - kubectl get po -o wide\n      - Pod가 찾는 노드가 없어 pending 상태\n    - 노드에 라벨 추가\n      - kubectl label nodes <your-node-name> disktype=ssd\n      - kubectl get nodes --show-labels | grep ssd\n    - kubectl get po -o wide\n  - Pod 생성 전 초기화\n    - kubectl create -f pod-initialize.yaml\n    - kubectl get po\n    - #생성된 Pod 내로 접근\n    - kubectl exec -it init-demo --/bin/bash\n    - cd /usr/share/nginx/html\n    - ls\n  - 생성된 Pod, 및 오브젝트 삭제\n    - kubectl delete pod [pod명]\n    - kubectl delete service,deploy --all\n\n- <b>Lab. Label </b>\n  - 2개의 Pod 생성\n  - (1. pod 폴더로 이동하여, 아래 명령어 실행)\n  - kubectl create -f pod-with-nodeselector.yaml\n  - (2. 아래 nginx 컨테이너 생성)\n  - kubectl run nginx2 --image=nginx\n  - kubectl get pods -l run=nginx2\n  - kubectl get pods --selector run=nginx2\n  - <pre style=\"white-space: pre-wrap\">kubectl get pods --selector \"run in (nginx2, test)\"</pre>\n- <b>Lab. Rollout & RollBack</b>\n  - (pwd 로 현 위치가 /container-orchestration/yaml_aws/ 인지 확인)\n  - kubectl create -f nginx.yaml\n  - (아래 명령으로 배포 주석 추가, Rollback시 필요)\n  - <pre style=\"white-space: pre-wrap\">kubectl annotate deploy nginx-deployment kubernetes.io/change-cause=\"v1 is nginx:1.7.9\"</pre>\n  - Set image 명령을 통한 이미지 Rollout  및 확인\n  - kubectl set image deploy nginx-deployment nginx=nginx:1.9.1\n  - kubectl rollout history deploy nginx-deployment\n  - (배포주석 달기)\n  - <pre style=\"white-space: pre-wrap\">kubectl annotate deploy nginx-deployment kubernetes.io/change-cause=\"v2 is revisioned nginx:1.9.1\"</pre>\n  - kubectl describe po <해당 deployment 의 pod 중 하나의 이름>    # 내용의 image 부분이 1.9.1 인지 확인\n  - (무정지 재배포 히스토리 확인)\n  - kubectl rollout history deploy nginx-deployment\n  - (다음과 같이 출력됨을 확인)\n    \"REVISION  CHANGE-CAUSE\n      1         v1 is nginx:1.7.9\n      2         v2 is nginx:1.9.1\"\n  - (롤백하기)\n  - kubectl rollout undo deploy nginx-deployment\n  - kubectl rollout undo deploy nginx-deployment --to-revision 1\n- <b>Lab. ReplicaSet</b>\n  - pwd 로 현 위치가 /container-orchestration/yaml_aws/replicaset 인지 확인\n  - kubectl create -f replicaset.yaml\n  - kubectl get all\n  - #replica 개수 조정\n    - kubectl scale replicaset/frontend --replicas=5\n    - kubectl get po\n- <b>Lab. Deployment & 기본명령 </b>\n  - 기본 nginx 서버의 배포\n  - kubectl create deploy nginx --image=nginx\n  - kubectl get deploy nginx\n  - kubectl get replicaset -l app=nginx\n  - kubectl get po -l app=nginx  # \"-l\" 옵션은 label의 key/value 로 객체를 필터링\n  - kubectl get pods --selector app=nginx\n  - <pre style=\"white-space: pre-wrap\">kubectl get pods --selector \"app in (nginx, test)\"</pre>\n  - kubectl describe po (검색한 pod name)\n  - (pod 제거)\n  - kubectl delete po --all   #\n  - (pod 를 제거해도 재생됨을 확인)\n  - kubectl get po\n  - (scale out)\n  - kubectl scale deploy nginx --replicas=3\n  - kubectl get po   # pod 개수가 3개로 늘어남을 확인\n  - kubectl delete po --all   # pod 를 모두 지움\n  - kubectl get po   # pod 를 모두 지워도 결국 3개로 복원됨을 확인\n  - ( 제거하기 위해서는 deployment 를 제거해야만 함)\n  - kubectl delete deploy nginx\n- <b>Auto Scale-Out </b>\n  - pwd 로 현 위치가 /container-orchestration/yaml_aws/ 인지 확인\n  - (모든 객체 지우기)\n  - kubectl delete deploy,service --all\n  - (대상 서비스 배포 및 모니터링)\n  - kubectl apply -f https://k8s.io/examples/application/php-apache.yaml\n    - NOTE : 서비스가 Auto Scaling되기 위해서는 컨테이너 Spec에 Resources : 설정이 있어야 함\n      \" resources:\n            limits:\n              cpu: 500m\n            requests:\n              cpu: 200m\"\n  - (오토 스케일링 설정, hpa: HorizontalPodAutoscaler )\n\n    - kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10\n      cpu-percent=50 : Pod 들의 요청 대비 평균 CPU 사용율 (여기서는  요청이 200 milli-cores이므로, 모든 Pod의 평균 CPU 사용율이 100 milli-cores(50%)를 넘게되면 HPA 발생)\"\n    - kubectl get hpa php-apache -o yaml\n  - 로드 제너레이터(siege)가 설치된 컨테이너 생성\n    - cat siege.yaml\n    - kubectl create -f siege.yaml\n    - kubectl exec -it siege --/bin/bash\n  - 로드 생성\n    - siege -c30 -t30S -v http://php-apache\n  - (오토 스케일링이 되지 않을 때 :  kubectl get hpa의 TARGETS 부분에 cpu 사용률이 <unknown>으로 출력될 때)\n    - metrics-server가 제대로 실행중인지 kubectl top pods 명령으로 포드 cpu 사용률이 모니터링 되는지 확인\n    - 디플로이먼트의 컨테이너 옵션에 cpu request 옵션이 제대로 걸려 있는지 확인\n    - cpu request옵션이 없으면 hpa가 cpu사용량에 필요한 계산을 할 수 없음\n- <b>Lab. Service</b>\n  - Basic YAML\n      <pre style=\"white-space: pre-wrap\">\n      apiVersion: v1\n      kind: Service\n      metadata:\n        name: my-service\n      spec:\n        selector:\n          app: MyApp\n        ports:\n          - protocol: TCP\n            port: 80\n            targetPort: 8080\n      </pre>\n  - kubectl delete service,deploy --all  # 기존 이력 삭제\n  - (다시 생성)\n  - kubectl create deploy nginx --image=nginx\n  - (서비스로 노출)\n  - kubectl expose deploy nginx --type=\"LoadBalancer\" --port=80\n  - (웹 브라우저를 열고 생성된 external ip 로 접속, Nginx welcome 메시지 확인)\n  - kubectl exec -it (pod name) --/bin/bash   # 생성된 nginx 서버 linux 의 shell 에 접근\n- <b>Lab. Volume</b>\n  - (pwd 로 현 위치가 /container-orchestration/yaml_aws/volume 인지 확인)\n  - (emptyDir 마운트)\n  - kubectl create -f volume-emptydir.yaml\n  - (GitRepository를 볼륨으로 마운트)\n  - kubectl create -f volume-gitrepo.yaml\n  - (EFS - Elastic File System 생성하기)\n    - 관리콘솔에서 EFS 생성\n    - EFS Service Account 생성 : efs-sa.yaml 실행\n    - EFS Provisioner 배포 :\n      - 관리콘솔에서 생성한 efs ID와 DNS 정보로 수정\n      - nano efs-provisioner-deploy.yaml\n      - efs-provisioner-deploy.yaml 실행\n    - SA 권한 설정 :  efs-rbac.yaml 실행\n    - StorageClass 생성 :  efs-storageclass.yaml 실행\n  - (PersistentVolumeClaim 생성)\n  - kubectl create -f volume-pvc.yaml\n  - kubectl get pvc\n  - kubectl describe pvc aws-efs\n  - (생성된 PersistentVolueClaim으로 Pod 생성하기)\n  - kubectl create -f pod-with-pvc.yaml\n  - kubectl describe pod mypod\n  - (포드 접속)\n  - kubectl exec -it mypod --/bin/bash\n  - (마운트 및 사이즈 확인)\n  - df -k\n- <b>Lab. ConfigMap</b>\n  - (pwd 로 현 위치가 /container-orchestration/yaml_aws/configmap/ 인지 확인)\n  - (컨피그 맵 생성)\n  - kubectl create configmap hello-cm --from-literal=language=java\n  - kubectl get cm\n  - kubectl get cm hello-cm -o yaml\n  - (도커라이징 & ECR Push)\n    - docker build -t 052937454741.dkr.ecr.ap-northeast-2.amazonaws.com/user01-cm-sandbox:v1 .\n    - aws ecr create-repository --repository-name user22-cm-sandbox --region ap-northeast-2\n    - docker push 052937454741.dkr.ecr.ap-northeast-2.amazonaws.com/user01-cm-sandbox:v1\n  - (인증오류 발생 시, ECR 로그인)\n    - aws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin 052937454741.dkr.ecr.ap-northeast-2.amazonaws.com/\n    - 오류(unknown flag: --password-stdin) 발생 시,\n      - docker login --username AWS -p $(aws ecr get-login-password --region (Region-Code)) (Account-Id).dkr.ecr.(Region-Code).amazonaws.com/\n  - 클라우드에서 배포 이미지 확인\n  - nano cm-deployment.yaml 파일 편집(나의 ECR Registry 정보로 수정)\n  - (배포 및 서비스 생성)\n    - kubectl create -f cm-deployment.yaml\n    - kubectl create -f cm-service.yaml\n  - (서비스 확인)\n    - Service의 External-IP 접속\n- <b>Lab. Secret</b>\n  - (pwd 로 현 위치가 /container-orchestration/yaml_aws/secret/ 인지 확인)\n  - kubectl create secret generic my-password --from-literal=password=mysqlpassword\n  - (Pod에서 Secret 파일 마운트 사용하기 내용을 참고하여 배포 및 서비스 확인해 보기)\n- <b>Lab. Liveness & Readiness Probe</b>\n  - (pwd 로 현 위치가 /container-orchestration/yaml_aws/liveness/ 인지 확인)\n  - (Liveness Command Probe 실습)\n    - kubectl create -f exec-liveness.yaml\n    - (컨테이너가 Running 상태로 보이나, Liveness Probe 실패로 계속 재시작)\n    - (kubectl describe로 실패 메시지 확인)\n    - kubectl describe po liveness-exec\n  - (Liveness HTTP Probe 실습)\n    - kubectl create -f http-liveness.yaml\n    - (kubectl describe로 실패 메시지 확인)\n    - kubectl describe po liveness-http\n  - (Liveness 와 readiness probe 동시 적용 실습)\n    - kubectl create -f tcp-liveness-readiness.yaml\n    - (8080포트에 대해 정상적으로 Liveness 와 readiness Probe를 통과해 서비스가 실행됨)\n    - kubectl describe po goproxy\n</details>\n\n<details>\n<summary><b>Kubernetes Advanced Hands-on</b></summary>\n\n- <b>Lab. Ingress</b>\n  - Helm 명령으로 설치 여부 확인\n  - Helm 이 설치되어 있지 않은 경우, Helm(패키지 인스톨러) 설치\n    - Helm 3.x 설치(권장)\n      - curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 > get_helm.sh\n      - chmod 700 get_helm.sh\n      - ./get_helm.sh\n    - Helm 2.x 설치\n      - curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash\n      - (설치 중, sudo를 위한 비밀번호 입력)\n      - kubectl --namespace kube-system create sa tiller\n      - kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller\n      - helm init --service-account tiller\n  - Helm으로 Ingress Controller 설치\n    - helm repo add stable https://charts.helm.sh/stable\n    - helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n    - helm repo update\n    - kubectl create namespace ingress-basic\n    - helm version 확인\n    - (helm version 2.x 일때)\n      - helm install --name nginx-ingress stable/nginx-ingress  --namespace=ingress-basic\n    - (helm version 3.x 일때)\n      - helm install nginx-ingress ingress-nginx/ingress-nginx --namespace=ingress-basic\n    - (설치확인)\n      - kubectl get all --namespace=ingress-basic\n      - (Ingress Controller의 EXTERNAL-IP가 API Gateway 엔드포인트: 메모 必)\n    - Ingress 대상 서비스(BLUE, GREEN) 생성\n      - (pwd 로 현 위치가 /container-orchestration/yaml_aws/ingress/blue-svc/ 인지 확인)\n        - (도커라이징 & 이미지 Push)\n        - docker build -t 052937454741.dkr.ecr.ap-northeast-2.amazonaws.com/user30-nginx-blue:latest  .\n        - aws ecr create-repository --repository-name user30-nginx-blue --region ap-northeast-2\n        - docker push 052937454741.dkr.ecr.ap-northeast-2.amazonaws.com/user30-nginx-blue:latest\n        - (배포 전 yaml을 열어 image URL을 나의 ACR이름으로 수정)\n        - nano nginx-blue-deployment.yaml\n        - (저장 ctrl + X)\n        - (배포 및 서비스 생성)\n        - kubectl create -f nginx-blue-deployment.yaml\n      - (pwd 로 현 위치가 /container-orchestration/yaml_aws/ingress/green-svc/ 인지 확인)\n        - (도커라이징 & 이미지 Push)\n        - docker build -t (283210891307).dkr.ecr.ap-northeast-2.amazonaws.com/nginx-green:latest  .\n        - aws ecr create-repository --repository-name nginx-green --region ap-northeast-2\n        - docker push (283210891307).dkr.ecr.ap-northeast-2.amazonaws.com/nginx-green:latest\n        - (배포 전 yaml을 열어 image URL을 나의 ACR이름으로 수정)\n        - nano nginx-green-deployment.yaml\n        - (저장 ctrl + X)\n        - (배포 및 서비스 생성)\n        - kubectl create -f nginx-green-deployment.yaml\n      - (서비스 생성 확인)\n        - kubectl get deploy,service -n ingress-basic\n    - Ingress Routing Rule 생성\n      - (pwd 로 현 위치가 /container-orchestration/yaml_aws/ingress/ 인지 확인)\n      - kubectl apply -f path-based-ingress.yaml\n      - kubectl get ingress -n ingress-basic\n    - Ingress 테스트\n      - API Gateway 주소를 Local 시스템에 등록\n      - Windows - hosts 파일 맨 하단에 Ingress Controller의 External-IP 등록\n  - <b>Metric Server 설치</b>\n    - kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.7/components.yaml\n    - kubectl get deployment metrics-server -n kube-system\n  - <b>AWS EKS 모니터링</b>\n    - Metric-Server 설치\n      - kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.5.0/components.yaml\n    - Kubernetes DashBoard 설치\n      - kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml\n    - eks-admin 서비스 계정 및 클러스터 롤 바인딩 생성\n      <pre style=\"white-space: pre-wrap\">\n      cat &#60;&#60;EOF | kubectl apply -f -\n        apiVersion: v1\n        kind: ServiceAccount\n        metadata:\n          name: eks-admin\n          namespace: kube-system\n        &#45;&#45;&#45;\n        apiVersion: rbac.authorization.k8s.io/v1beta1\n        kind: ClusterRoleBinding\n        metadata:\n          name: eks-admin\n        roleRef:\n          apiGroup: rbac.authorization.k8s.io\n          kind: ClusterRole\n          name: cluster-admin\n        subjects:\n        - kind: ServiceAccount\n          name: eks-admin\n          namespace: kube-system\n      EOF\n      </pre>\n    - 인증 토큰 조회\n      - <pre style=\"white-space: pre-wrap\">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep eks-admin | awk \"{print $1}\")</pre>\n    - Proxy 설정 및 DashBoard 연결\n      - kubectl proxy\n      - http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#!/login\n      - 복사한 토큰 정보로 로그인\n</details>\n\n<details>\n<summary><b>Real MSA Application Deployment</b></summary>\n\n  - <b>사전작업</b>\n    - aws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin (Account-ID).dkr.ecr.ap-northeast-2.amazonaws.com\n    - cd  ~\n    - mkdir MSA-Sample\n    - cd MSA-Sample\n  - <b>마이크로서비스 배포</b>\n    - 상품(Product) 서비스\n      <pre style=\"white-space: pre-wrap\">\n      export ECR=[AWS_ACCOUNT_ID].dkr.ecr.ap-northeast-2.amazonaws.com\n      git clone https://github.com/event-storming/reqres_products.git\n      cd reqres_products\n      mvn package -Dmaven.test.skip=true\n\n      docker build -t ${ECR}/products:latest .\n      aws ecr create-repository --repository-name products --region ap-northeast-2\n      docker push ${ECR}/products:latest\n\n      kubectl create deploy products --image=${ECR}/products:latest\n      kubectl expose deploy products --type=ClusterIP\" --port=8080\n      cd ..\n      </pre>\n    - 주문(Order) 서비스\n      <pre style=\"white-space: pre-wrap\">\n      export ECR=[AWS_ACCOUNT_ID].dkr.ecr.ap-northeast-2.amazonaws.com\n      git clone https://github.com/event-storming/reqres_orders.git\n\n      cd reqres_orders\n      export IMAGENAME=orders\n      mvn package -Dmaven.test.skip=true\n      docker build -t ${ECR}/${IMAGENAME}:latest .\n      aws ecr create-repository --repository-name ${IMAGENAME} --region ap-northeast-2\n      docker push ${ECR}/${IMAGENAME}:latest\n      kubectl create deploy ${IMAGENAME} --image=${ECR}/${IMAGENAME}:latest\n      kubectl expose deploy ${IMAGENAME} --type=ClusterIP\" --port=8080\n\n      cd ..\n      </pre>\n    - 배송(Delivery) 서비스\n      <pre style=\"white-space: pre-wrap\">\n      git clone https://github.com/event-storming/reqres_delivery.git\n      cd reqres_delivery\n      export IMAGENAME=delivery\n\n      mvn package -Dmaven.test.skip=true\n      docker build -t ${ECR}/${IMAGENAME}:latest .\n      aws ecr create-repository --repository-name ${IMAGENAME} --region ap-northeast-2\n      docker push ${ECR}/${IMAGENAME}:latest\n      kubectl create deploy ${IMAGENAME} --image=${ECR}/${IMAGENAME}:latest\n      kubectl expose deploy ${IMAGENAME} --type=ClusterIP\" --port=8080\n\n      cd ..\n      </pre>\n    - 인증(Oauth) 서비스\n      <pre style=\"white-space: pre-wrap\">\n      git clone https://github.com/event-storming/oauth.git\n      cd oauth\n      export IMAGENAME=oauth\n\n      mvn package -Dmaven.test.skip=true\n      docker build -t ${ECR}/${IMAGENAME}:latest .\n      aws ecr create-repository --repository-name ${IMAGENAME} --region ap-northeast-2\n      docker push ${ECR}/${IMAGENAME}:latest\n      kubectl create deploy ${IMAGENAME} --image=${ECR}/${IMAGENAME}:latest\n      kubectl expose deploy ${IMAGENAME} --type=ClusterIP\" --port=8080\n\n      cd ..\n      </pre>\n    - 게이트웨이(Gateway) 서비스\n      <pre style=\"white-space: pre-wrap\">\n      git clone https://github.com/event-storming/gateway.git\n      cd gateway\n      export IMAGENAME=gateway\n\n      mvn package -Dmaven.test.skip=true\n      docker build -t ${ECR}/${IMAGENAME}:latest .\n      aws ecr create-repository --repository-name ${IMAGENAME} --region ap-northeast-2\n      docker push ${ECR}/${IMAGENAME}:latest\n      kubectl create deploy ${IMAGENAME} --image=${ECR}/${IMAGENAME}:latest\n      kubectl expose deploy ${IMAGENAME} --type=\"LoadBalancer\" --port=8080\n\n      cd ..\n      </pre>\n    - 프론트-엔드(UI) 서비스: 빌드환경 설정(npm 설치)\n      <pre style=\"white-space: pre-wrap\">\n      sudo apt-get update\n      sudo apt install build-essential\n      curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -\n      sudo apt install nodejs\n      </pre>\n    - 프론트-엔드(UI) 서비스: 배포 사전 작업\n      <pre style=\"white-space: pre-wrap\">\n      git clone https://github.com/event-storming/ui.git\n      cd ui\n      export IMAGENAME=ui\n\n      npm install\n      npm run build\n      docker build -t ${ECR}/${IMAGENAME}:latest .\n      aws ecr create-repository --repository-name ${IMAGENAME} --region ap-northeast-2\n      docker push ${ECR}/${IMAGENAME}:latest\n\n      _GATEWAY_IP=$(kubectl get -o jsonpath=\"{.status.loadBalancer.ingress[0].hostname}\" svc gateway --ignore-not-found)\n      echo ${_GATEWAY_IP}\n      </pre>\n    - 프론트-엔드(UI) 서비스: 배포 및 서비스 생성\n      <pre style=\"white-space: pre-wrap\">\n      cat &#60;&#60;EOF | kubectl apply -f -\n      apiVersion: apps/v1\n      kind: Deployment\n      metadata:\n        name: ${IMAGENAME}\n        labels:\n          app: ${IMAGENAME}\n      spec:\n        replicas: 1\n        selector:\n          matchLabels:\n            app: ${IMAGENAME}\n        template:\n          metadata:\n            labels:\n              app: ${IMAGENAME}\n          spec:\n            containers:\n              - name: ${IMAGENAME}\n                image: ${ACR}/${IMAGENAME}:latest\n                ports:\n                  - containerPort: 8080\n                env:\n                  - name: VUE_APP_API_HOST\n                    value: http://${_GATEWAY_IP}:8080\n      EOF\n\n      kubectl expose deploy ${IMAGENAME} --type=\"LoadBalancer\" --port=8080\n\n      cd ..\n      </pre>\n\n  - <b>서비스 확인</b>\n    - kubectl get svc ui\n    - 브라우저에서 접속 http://UI-Service-EXTERNAL-IP:8080\n\n</details>\n\n<details>\n<summary><b>Service Mesh, Istio Hands-on</b></summary>\n\n  - <b>Lab. Istio Install</b>\n    - Istio 설치\n    - curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.7.1 TARGET_ARCH=x86_64 sh -\n      \"(istio v1.7.1은 Kubernetes 1.16이상에서만 동작)\"\n    - cd istio-1.7.1\n    - <pre style=\"white-space: pre-wrap\">export PATH=$PWD/bin:$PATH</pre>\n    - istioctl install --set profile=demo --set hub=gcr.io/istio-release\n      \"note : there are other profiles for production or performance testing.\"\n    - Istio 모니터링 툴(Telemetry Applications) 설치\n      - vi samples/addons/kiali.yaml\n      - 4라인의 apiVersion: apiextensions.k8s.io/v1beta1을 apiVersion: apiextensions.k8s.io/v1으로 수정\n      - kubectl apply -f samples/addons\n      - kiali.yaml 오류발생시, 아래 명령어 실행\n        > kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.7/samples/addons/kiali.yaml\n\n      - 모니터링(Tracing & Monitoring) 툴 설정\n        - Monitoring Server - Kiali\n          - 기본 ServiceType 변경 : ClusterIP를 LoadBalancer 로..\n            - kubectl edit svc kiali -n istio-system\n            - :%s/ClusterIP/LoadBalancer/g\n            - :wq!\n          - 모니터링 시스템(kiali) 접속 : EXTERNAL-IP:20001 (admin/admin)\n        - Tracing Server - Jaeger\n          - 기본 ServiceType 변경 : ClusterIP를 LoadBalancer 로..\n            - kubectl edit svc tracing -n istio-system\n            - :%s/ClusterIP/LoadBalancer/g\n            - :wq!\n          - 분산추적 시스템(tracing) 접속 : EXTERNAL-IP:80\n    - 설치확인\n      - kubectl get all -n istio-system\n  - <b>How to enable Istio</b>\n    - 1. Whenever deploying to Cluster, Using pre-processing command \"Istio kube-inject\"\n      - kubectl apply -f <(istioctl kube-inject -f Deployment.yml) -n istio-test-ns\n    - 2. Using Istio-enabled Namespace.\n      - e.g. kubectl label namespace tutorial istio-injection=enabled\n  - <b>Lab. Istio Tutorial 셋업</b>\n    - Git repository에서 Tutorial 리소스 가져오기\n      - cd ~\n      - git clone https://github.com/redhat-developer-demos/istio-tutorial\n      - cd istio-tutorial\n    - 네임스페이스 생성\n      - kubectl create namespace tutorial\n    - Customer Service 배포\n      - kubectl apply -f <(istioctl kube-inject -f customer/kubernetes/Deployment.yml) -n tutorial\n        - kubectl describe pod (Customer Pod) -n tutorial 로 생성확인\n      - kubectl create -f customer/kubernetes/Service.yml -n tutorial\n    - Istio Gateway 설치 및 Customer 서비스 라우팅(VirtualService) 설정\n      - cat customer/kubernetes/Gateway.yml\n      - kubectl create -f customer/kubernetes/Gateway.yml -n tutorial\n      - (Istio-IngressGateway를 통한 Customer 서비스 확인)\n        - kubectl get service/istio-ingressgateway -n istio-system\n        - 해당 EXTERNAL-IP가 Istio Gateway 주소\n        - Customer 서비스 호출 :\n          <pre style=\"white-space: pre-wrap\">\"http://(istio-ingressgateway IP)/customer\"</pre>\n    - Preference, Recommendation-v1 Service 배포\n      - kubectl apply -f <(istioctl kube-inject -f preference/kubernetes/Deployment.yml)  -n tutorial\n      - kubectl create -f preference/kubernetes/Service.yml -n tutorial\n      - kubectl apply -f <(istioctl kube-inject -f recommendation/kubernetes/Deployment.yml) -n tutorial\n      - kubectl create -f recommendation/kubernetes/Service.yml -n tutorial\n  - <b>Lab. Istio - Traffic Routing</b>\n    - Simple Routing\n      - (pwd 로 현 위치가 /istio-tutorial/ 인지 확인)\n      - (recommendation 서비스 추가 배포: v2)\n        - kubectl apply -f <(istioctl kube-inject -f recommendation/kubernetes/Deployment-v2.yml) -n tutorial\n      - 서비스 호출\n        - 브라우저에서 Customer 서비스(Externl-IP:8080 접속) 호출\n        - F5(새로고침)를 10회 이상 클릭하여 다수의 요청 생성\n      - Routing 결과 확인 - Kiali(Externl-IP:20001) 접속\n    - (Recommendation v.2 서비스 Scale Out)\n    - (서비스의 v2 의 replica 를 2로 설정)\n      - kubectl scale --replicas=2 deployment/recommendation-v2 -n tutorial\n      - kubectl get po -n tutorial\n    - Customer 서비스를 10회 이상 F5(새로고침)하여 서비스 호출\n    - Routing 결과 확인 - Kiali(Externl-IP:20001) 접속\n    - Advanced Routing\n      - 정책(VirtualService, DestinationRule) 설정\n        - (현, 정책 확인)\n          - kubectl get VirtualService -n tutorial -o yaml\n          - kubectl get DestinationRule -n tutorial -o yaml\n        - (사용자 선호도에 따른 추천 서비스 라우팅 정책 설정)\n        - (VirtualService, DestinationRule 설정, v2로 100% 라우팅)\n          - kubectl create -f istiofiles/destination-rule-recommendation-v1-v2.yml -n tutorial\n          - kubectl create -f istiofiles/virtual-service-recommendation-v2.yml -n tutorial\n        - (설정정책 확인)\n          - kubectl get VirtualService -n tutorial -o yaml\n          - kubectl get DestinationRule -n tutorial -o yaml\n        - (서비스 확인)\n          - 브라우저에서 Customer 서비스(Externl-IP:8080 접속)호출\n          - Kiali(Externl-IP:20001), Jaeger(External-IP:80)에서 모니터링\n      - 가중치 기반 스마트 라우팅\n        - (recommendation 서비스 v1의 가중치를 100으로 변경)\n          - kubectl replace -f istiofiles/virtual-service-recommendation-v1.yml -n tutorial\n        - (서비스 호출 및 Kiali(Externl-IP:20001)에서 모니터링)\n        - (VirtualService 삭제 시, Round-Robin 방식으로 동작)\n          - kubectl delete -f istiofiles/virtual-service-recommendation-v1.yml -n tutorial\n        - Canary 라우팅 비율별 배포 정책 예시\n          - (90 : 10)\n          - kubectl apply -f istiofiles/virtual-service-recommendation-v1_and_v2.yml -n tutorial\n          - (75 : 25)\n          - kubectl replace -f istiofiles/virtual-service-recommendation-v1_and_v2_75_25.yml -n tutorial\n        - 삭제\n          - kubectl delete dr recommendation -n tutorial\n          - #kubectl delete vs recommendation -n tutorial\n          - kubectl scale --replicas=1 deployment/recommendation-v2 -n tutorial\n      - Client 브라우저 유형별 스마트 라우팅\n        - Firefox 브라우저로 접속 시, v2로 라우팅되도록 설정\n          - kubectl apply -f istiofiles/destination-rule-recommendation-v1-v2.yml -n tutorial\n          - kubectl apply -f istiofiles/virtual-service-firefox-recommendation-v2.yml -n tutorial\n        - (Firefox 브라우저와 다른 브라우저에서 접속 확인)\n        - (Browser 환경이 지원되지 않을 경우,)\n          - curl -A Safari Externl-IP:8080\n          - curl -A Firefox Externl-IP:8080\n        - 삭제\n          - kubectl delete dr recommendation -n tutorial\n          - kubectl delete vs recommendation -n tutorial\n  - <b>Lab. Istio - Timeout & Retry</b>\n    - Lab에 필요한 모듈(Message Queue) 설치\n      - kubectl get svc my-kafka -n kafka\n      - 미설치시, 설치 링크 (https://workflowy.com/s/msa/27a0ioMCzlpV04Ib#/a7018fb8c629)\n    - tutorial  네임스페이스에 Istio 기능 추가\n      - kubectl label namespace tutorial istio-injection=enabled --overwrite\n      - 네임스페이스가 없을 시, 생성 후 실행\n    - Lab. Timeout : Fail-Fast를 통한 서비스 Caller 자원 보호\n      - Timeout 테스트를 위해 CNA 과정에서 구현한 Order 마이크로서비스의  코드 보완 및 tutorial 네임스페이스에 배포\n        - Service time delay를 위해, Order Aggregate(Order.java)에 저장전 Thread.sleep 코드 삽입\n          <pre style=\"white-space: pre-wrap\">\n          @PrePersist\n            public void onPrePersist(){\n              try {\n                  Thread.currentThread().sleep((long) (800 + Math.random() * 220));\n              } catch (InterruptedException e) {\n                  e.printStackTrace();\n              }\n            }\n          </pre>\n        - Docker image Build & Push\n        - tutorial 네임스페이스에 Order v2 서비스 재배포\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n            apiVersion: apps/v1\n            kind: Deployment\n            metadata:\n              name: order\n              namespace: tutorial\n              labels:\n                app: order\n            spec:\n              replicas: 1\n              selector:\n                matchLabels:\n                  app: order\n              template:\n                metadata:\n                  labels:\n                    app: order\n                spec:\n                  containers:\n                    - name: order\n                      image: IMAGE_FULL_REPOSITORY_URL/order:v2\n                      ports:\n                        - containerPort: 8080\n                      resources:\n                        limits:\n                          cpu: 500m\n                        requests:\n                          cpu: 200m\n          EOF\n          </pre>\n        - Order 서비스 생성\n          - kubectl expose deploy order --port=8080 -n tutorial\n        - Order 서비스 Timeout 설정 (Istio Gateway에서 Order 서비스로 라우팅 시)\n          - (pwd 로 현 위치가 /istio-tutorial/ 인지 확인)\n          - nano customer/kubernetes/Gateway.yaml 오픈 후 마지막 행 다음에 타임아웃 설정이 포함된 아래 내용 추가\n            <pre style=\"white-space: pre-wrap\">\n            - match:\n                - uri:\n                    prefix: /orders\n                route:\n                - destination:\n                    host: order\n                    port:\n                      number: 8080\n                timeout: 3s\n            </pre>\n          - (변경 내용 적용)\n          - kubectl apply -f customer/kubernetes/Gateway.yml -n tutorial\n        - Order 서비스 Timeout 설정 (클라우드 내에서 Order 서비스로 라우팅시)\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n              apiVersion: networking.istio.io/v1alpha3\n              kind: VirtualService\n              metadata:\n                name: vs-order-network-rule\n                namespace: tutorial\n              spec:\n                hosts:\n                - order\n                http:\n                - route:\n                  - destination:\n                      host: order\n                  timeout: 3s\n          EOF\n          </pre>\n        - 부하테스트 툴(Siege) 설치 및 Order 서비스 Load Testing\n          - kubectl run siege --image=apexacme/siege-nginx -n tutorial\n          - kubectl exec -it siege -c siege -n tutorial &#45;&#45; /bin/bash\n          - <pre style=\"white-space: pre-wrap\">siege -c30 -t20S -v --content-type \"application/json\" &#39;http://order:8080/orders POST {\"productId\": \"1001\", \"qty\":5}&#39;</pre>\n      - Order 서비스에 설정된 Timeout을 임계치를 초과하는 순간, Istio에서 서비스로의 연결을 자동 차단하는 것을 확인\n    - Lab. Retry : 5xx 오류를 리턴받게 되면, Envoy Proxy에서 설정한 횟수만큼 대상 서비스를 재호출하여 일시적인 장애였는지를 다시 확인하는 Rule\n      - Retry 테스트를 위해 CNA 과정에서 구현한 Delivery 마이크로서비스를 tutorial 네임스페이스에 배포\n      - Docker image Build & Push\n      - tutorial 네임스페이스에 Delivery 서비스 배포\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: apps/v1\n          kind: Deployment\n          metadata:\n            name: delivery\n            namespace: tutorial\n            labels:\n              app: delivery\n          spec:\n            replicas: 1\n            selector:\n              matchLabels:\n                app: delivery\n            template:\n              metadata:\n                labels:\n                  app: delivery\n              spec:\n                containers:\n                  - name: delivery\n                    image: IMAGE_FULL_REPOSITORY_URL/delivery:v2\n                    ports:\n                      - containerPort: 8080\n                    resources:\n                      limits:\n                        cpu: 500m\n                      requests:\n                        cpu: 200m\n        EOF\n        </pre>\n      - Delivery 서비스 생성\n        - kubectl expose deploy delivery --port=8080 -n tutorial\n      - Order 서비스에 Retry Rule 추가 적용\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: networking.istio.io/v1alpha3\n          kind: VirtualService\n          metadata:\n            name: vs-order-network-rule\n            namespace: tutorial\n          spec:\n            hosts:\n            - order\n            http:\n            - route:\n              - destination:\n                  host: order\n              timeout: 3s\n              retries:\n                attempts: 3\n                perTryTimeout: 2s\n                retryOn: 5xx,retriable-4xx,gateway-error,connect-failure,refused-stream\n        EOF\n        </pre>\n      - Delivery 서비스를 정지하고, 이를 동기호출하는 Order 서비스 API 호출\n        - kubectl scale deploy delivery --replicas=0 -n tutorial\n        - kubectl exec -it siege -c siege -n tutorial --/bin/bash\n        - http http://order:8080/orders/ productId=1001 qty=5\n          - httpie가 없을 시,\n          - apt-get update\n          - apt-get install httpie\n        - http DELETE http://order:8080/orders/1\n      - Jaeger 접속(http://tracing svc EXTERNAL-IP :80) 후, Retry 횟수 확인하기\n        \"< 검색조건 >\n          Service : order.tutorial, Operation : delivery.tutorial.svc.cluster.local:8080/*\n          검색결과 : 총 Retry 횟수 + 1 의 Requests 로깅\"\n  - <b>Lab. Istio - Circuit Breaker</b>\n    - Circuit Breaker : 장애 인스턴스를 회피하는 기능으로 5xx 오류를 리턴한 인스턴스를  라우팅 대상에서 일정시간 만큼 제외 (Pool Ejection)\n    - Namespace 생성 및 Istio 활성\n      - kubectl create namespace istio-cb-ns\n      - kubectl label namespace istio-cb-ns istio-injection=enabled\n    - Istio Retry 디폴트 동작 확인\n      - 테스트 어플리케이션 배포\n        - hello-server-1, hello-server-2 Pods, Service\n        - hello-server 앱은 env:RANDOM_ERROR 값의 확률로 랜덤하게 503 에러를 발생하는 로직이 포함\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60; EOF\n              apiVersion: v1\n              kind: Pod\n              metadata:\n                name: hello-server-1\n                namespace: istio-cb-ns\n                labels:\n                  app: hello\n              spec:\n                containers:\n                - name: hello-server-1\n                  image: docker.io/honester/hello-server:latest\n                  imagePullPolicy: IfNotPresent\n                  env:\n                  - name: VERSION\n                    value: \"v1\"\n                  - name: LOG\n                    value: \"1\"\n              &#45;&#45;&#45;\n              apiVersion: v1\n              kind: Pod\n              metadata:\n                name: hello-server-2\n                namespace: istio-cb-ns\n                labels:\n                  app: hello\n              spec:\n                containers:\n                - name: hello-server-2\n                  image: docker.io/honester/hello-server:latest\n                  imagePullPolicy: IfNotPresent\n                  env:\n                  - name: VERSION\n                    value: \"v2\"\n                  - name: LOG\n                    value: \"1\"\n                  - name: RANDOM_ERROR\n                    value: \"0.2\"\n              &#45;&#45;&#45;\n              apiVersion: v1\n              kind: Service\n              metadata:\n                name: svc-hello\n                namespace: istio-cb-ns\n                labels:\n                  app: hello\n              spec:\n                selector:\n                  app: hello\n                ports:\n                - name: http\n                  protocol: TCP\n                  port: 8080\n          EOF\n          </pre>\n        - 클라이언트용 서비스(httpbin) 배포\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n              apiVersion: apps/v1\n              kind: Deployment\n              metadata:\n                name: httpbin\n                namespace: istio-cb-ns\n              spec:\n                replicas: 1\n                selector:\n                  matchLabels:\n                    app: httpbin\n                template:\n                  metadata:\n                    labels:\n                      app: httpbin\n                  spec:\n                    containers:\n                    - name: httpbin\n                      image: docker.io/honester/httpbin:latest\n                      imagePullPolicy: IfNotPresent\n                      ports:\n                      - containerPort: 80\n              &#45;&#45;&#45;\n              apiVersion: v1\n              kind: Service\n              metadata:\n                name: httpbin\n                namespace: istio-cb-ns\n                labels:\n                  app: httpbin\n              spec:\n                selector:\n                  app: httpbin\n                ports:\n                - name: http\n                  port: 8000\n                  targetPort: 80\n          EOF\n          </pre>\n      - Retry 디폴트 동작 테스트\n        - hello-server-2의 로그 모니터 걸기\n          - kubectl logs -f hello-server-2 -c hello-server-2 -n istio-cb-ns\n        - 클라이언트에서 svc-hello 서비스 10번 호출하기\n          - for i in {1..10}; do kubectl exec -it httpbin -c httpbin -n istio-cb-ns --curl http://svc-hello.istio-cb-ns:8080; sleep 0.1; done\n      - 결과 확인/분석\n        \"1) 서비스 호출은 Round Robin으로 로드 밸런싱되나, 프로세싱 시간에 따라 동일한 서비스가 연속 2회 로깅 될 수 있음\n          2) 핵심포인트는, Server-2가 5xx 오류를 리턴할 경우, 자동으로 Retry되어 Server-1 로그가 연달아 출력된다는 점임. (Default Retry : 2회)\"\n    - Circuit Breaker 설정\n      - 대기 쓰레드수 기반 Circuit Breaker\n        - 클라이언트용 서비스(httpbin)에 쓰레드 기반 Circuit Breaker 설정\n        - (Pending Thread가 많을수록 경우, 오랫동안 큐잉된 요청은 Response time이 증가하게 되므로, 적절한 대기 쓰레드를 풀을 적용하여 Circuit Breaking)\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n            apiVersion: networking.istio.io/v1alpha3\n            kind: DestinationRule\n            metadata:\n              name: dr-httpbin\n              namespace: istio-cb-ns\n            spec:\n              host: httpbin\n              trafficPolicy:\n                connectionPool:\n                  http:\n                    http1MaxPendingRequests: 1\n                    maxRequestsPerConnection: 1\n          EOF\n          </pre>\n        - Circuit Breaker 동작 확인\n          - 부하테스트 툴(Siege) 설치 및  Load Testing\n            - kubectl run siege --image=apexacme/siege-nginx -n istio-cb-ns\n            - kubectl exec -it siege -c siege -n istio-cb-ns --/bin/bash\n              - siege -c1 -t10S -v http://httpbin:8000/get  # 100% availability\n              - siege -c2 -t10S -v http://httpbin:8000/get  # 87% availability\n          - Kiali(Externl-IP:20001) 모니터링\n      - 로드 밸런싱 풀(pool) 인스턴스의 Health Status 기반 Circuit Breaker\n        - Hello 서비스의 로드 밸런싱 풀(pool)의 인스턴스 상태기반 Circuit Breaker 설정\n          <pre style=\"white-space: pre-wrap\">\n          kubectl apply -f - &#60;&#60;EOF\n            apiVersion: networking.istio.io/v1alpha3\n            kind: DestinationRule\n            metadata:\n              name: dr-hello-server\n              namespace: istio-cb-ns\n            spec:\n              host: svc-hello\n              trafficPolicy:\n                outlierDetection:\n                  interval: 1s\n                  consecutiveErrors: 1\n                  baseEjectionTime: 3m\n                  maxEjectionPercent: 100\n          EOF\n          </pre>\n        - Circuit Breaker 동작 확인\n          - 클라이언트(httpbin Pod)에서 svc-hello 호출\n            - hello-server-2의 로그 모니터 걸기\n              - kubectl logs -f hello-server-2 -c hello-server-2 -n istio-cb-ns\n            - 클라이언트에서 svc-hello 서비스 10번 호출하기\n              - for i in {1..10}; do kubectl exec -it httpbin -c httpbin -n istio-cb-ns --curl http://svc-hello.istio-cb-ns:8080; sleep 0.1; done\n            - 결과 확인/분석\n              \"1) 5초 동안 5xx 에러가 2번 발생할 경우, Server-2로는 5분 동안 트래픽이 라우팅 되지 않는다.\n                2) 모니터링 시스템(Kiali) : EXTERNAL-IP:20001 에서 Circuit Breaker 뱃지 발생 확인\"\n    - 동기호출 Target인 배송(Delivery) 서비스에 Circuit Breaker 설정하기\n      - Thread 부하 및 5XX 오류에 대해 서비스를 차단하는 Circuit Breaker 생성\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: networking.istio.io/v1alpha3\n          kind: DestinationRule\n          metadata:\n            name: dr-delivery\n            namespace: tutorial\n          spec:\n            host: delivery\n            trafficPolicy:\n              connectionPool:\n                http:\n                  http1MaxPendingRequests: 30\n                  maxRequestsPerConnection: 100\n              outlierDetection:\n                interval: 5s\n                consecutiveErrors: 1\n                baseEjectionTime: 5m\n                maxEjectionPercent: 100\n        EOF\n        </pre>\n      - 설정 내용\n        - 최대 활성 연결 갯수 30개와 최대 요청 대기 수를 100개로 지정하고, 이 임계점을 넘어가는 추가 요청은 거부(circuit break)\n        - 5초 동안 2번 5xx을 리턴한 서비스는 5분 동안 라우팅 대상에서 제외(Ejection)\n        - 또한, 모든 대상 서비스 인스턴스가 방출(제외)될 수 있음\n    - Clean-up\n      - kubectl delete pod/hello-server-1 pod/hello-server-2 pod/httpbin service/svc-hello dr/dr-hello -n istio-cb-ns\n  - <b>Clear Istio </b>\n    - kubectl delete ns tutorial istio-cb-ns istio-system\n\n</details>\n\n\n<details>\n<summary><b>Backup</b></summary>\n\n  - <b>Container로부터 이미지 생성</b>\n    - 이미지 생성\n      - docker run --name my-nginx -d -p 80:80 nginx\n      - docker exec -it my-nginx /bin/bash\n        - apt-get update\n        - apt-get install curl\n        - cd /usr/share/nginx/html\n        - <pre style=\"white-space: pre-wrap\">echo \"Hello my name is PYJ.\" >> index.html</pre>\n        - exit\n      - docker commit my-nginx my-nginx:1.0 # 컨테이너를 이미지로 생성\n      - docker diff [실행중인 Container ID] #원본 이미지와의 차이점 확인\n      - <pre style=\"white-space: pre-wrap\">docker commit -a \"apex@naver.com\" -m \"update nginx\" my-nginx my-nginx:1.0 </pre>\n      - docker images\n      - docker stop my-nginx\n      - docker run --name my-nginx2 -p 80:80 -d my-nginx:1.0\n      - http://localhost 확인\n      - docker stop my-nginx2\n    - 이미지 푸시\n      - docker tag my-nginx:1.0 apexacme/my-nginx:1.0\n      - docker images\n      - docker push apexacme/my-nginx:1.0\n      - http://hub.docker.com 에서 이미지 확인\n    - 도커허브 이미지로부터 컨테이너 실행\n      - docker run --name new-nginx -d -p 80:80 apexacme/my-nginx:1.0\n\n  - <b>샘플 자바 애플리케이션 패키징과 배포 </b>\n    - (pwd 로 현 위치가 /container-orchestration_aws/ 인지 확인)\n    - git clone https://github.com/event-storming/monolith.git\n    - cd monolith/\n    - ls\n    - (skip) mvn spring-boot:run #Maven으로 App. 실행\n    - mvn package -B -Dmaven.test.skip=true\n    - (skip) java -jar target/monolith-0.0.1.BUILD-SNAPSHOT.jar #Java로 App. 실행\n    - cat Dockerfile # 도커파일 내용 확인\n    - (도커라이징)\n      - docker build -t (Azure container registry명).azurecr.io/monolith:v1 .\n        - #주의1 :   명령  맨끝에 \" .\" 빼먹으면 안됨.   Dockerfile 의 위치인\n        - #주의2 :   project id 부분을 자신의 GCP project id 로 변경!!\n        - #주의3 :   현재 연결된 kubernetes 클러스터와 동일한 프로젝트 id 여야만 gcr registry 접근이 가능함\n      - docker images\n      - (skip) docker run (Azure container registry명).azurecr.io/monolith:v1 #Docker로 App. 실행\n      - docker push (azure container registry명).azurecr.io/monolith:v1\n    - kubectl create deploy monolith --image=(azure container registry명).azurecr.io/monolith:v1\n    - kubectl get po -l app=monolith\n    - <pre style=\"white-space: pre-wrap\">kubectl expose deploy monolith --type=\"LoadBalancer\" --port=8080</pre>\n    - kubectl get svc -w\n    - 자바 애플리케이션 접속\n      - http://(Service_Extern-IP):8080\n  - <b>Lab. Circuit Breaking</b>\n    - Istio가 활성화된 네임스페이스 생성\n      - kubectl create namespace istio-cb-ns\n      - kubectl label namespace istio-cb-ns istio-injection=enabled\n    - [CB 유스케이스] #1. Connection Max & Pending 수에 따른 Circuit Breaker\n      - 테스트 어플리케이션 배포 (Deployment, Service)\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: apps/v1\n          kind: Deployment\n          metadata:\n            name: httpbin\n            namespace: istio-cb-ns\n          spec:\n            replicas: 1\n            selector:\n              matchLabels:\n                app: httpbin\n            template:\n              metadata:\n                labels:\n                  app: httpbin\n              spec:\n                containers:\n                - name: httpbin\n                  image: docker.io/honester/httpbin:latest\n                  imagePullPolicy: IfNotPresent\n                  ports:\n                  - containerPort: 80\n          &#45;&#45;&#45;\n          apiVersion: v1\n          kind: Service\n          metadata:\n            name: httpbin\n            namespace: istio-cb-ns\n            labels:\n              app: httpbin\n          spec:\n            selector:\n              app: httpbin\n            ports:\n            - name: http\n              port: 8000\n              targetPort: 80\n        EOF\n        </pre>\n      - 로드 테스트 툴(siege) 배포\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: v1\n          kind: Pod\n          metadata:\n            name: siege\n            namespace: istio-cb-ns\n          spec:\n            containers:\n            - name: siege\n              image: apexacme/siege-nginx\n        EOF\n        </pre>\n      - siege를 통한 서비스(httpbin) 부하 생성\n        - kubectl exec -it siege --container siege -n istio-cb-ns --/bin/bash\n        - siege -c1 -t10S -v http://httpbin:8000/get\n        - siege -c1 -t10S -v http://httpbin.istio-cb-ns:8000/get\n        - siege -c1 -t10S -v http://httpbin.istio-cb-ns.svc.cluster.local:8000/get\n        - 서비스 모니터링 (Kiali) : EXTERNAL-IP:20001 (admin/admin)\n      - DestinationRule 를 생성하여 CB가 발생할 수 있도록 Connection pool 설정\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: networking.istio.io/v1alpha3\n          kind: DestinationRule\n          metadata:\n            name: dr-httpbin\n            namespace: istio-cb-ns\n\n          spec:\n            host: httpbin\n            trafficPolicy:\n              connectionPool:\n                http:\n                  http1MaxPendingRequests: 1\n                  maxRequestsPerConnection: 1\n        EOF\n        </pre>\n        - http1MaxPendingRequests=1 : Queue에서 Connection pool 에 연결을 기다리는 request 수를 1개로 제한\n        - maxRequestsPerConnection=1 : keep alive 기능 disable\n      - siege를 통한 서비스(httpbin) 부하 재생성 및 CB 확인\n        - siege -c1 -t10S -v http://httpbin:8000/get  # 100% Availability\n        - siege -c2 -t10S -v http://httpbin:8000/get  # 87% availability\n          - Envoy will return HTTP 503. It is the responsibility of the application to implement any fallback logic that is needed to handle the HTTP 503 error code from an upstream service. (https://istio-releases.github.io/v0.1/docs/concepts/traffic-management/handling-failures.html)\n        - 모니터링 시스템(Kiali) : EXTERNAL-IP:20001 에서 Circuit Breaker 발생 확인 (뱃지)\n      - Circuit Breaker 제거 후, 동일 로드 생성 후, Availability 100% 확인\n        - kubectl delete dr/dr-httpbin -n istio-cb-ns\n        - siege -c2 -t10S -v http://httpbin:8000/get  # 100% availability\n      - Clean-up\n        - kubectl delete deployment.apps/httpbin service/httpbin -n istio-cb-ns\n    - Istio - Circuit Breaker\n      - 테스트 어플리케이션 배포 (hello-server-1, hello-server-2 Pods, Service)\n      - (hello-server:latest 이미지는 env:RANDOM_ERROR 값의 확률로 랜덤하게 503 에러를 발생하는 로직이 포함)\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: v1\n          kind: Pod\n          metadata:\n            name: hello-server-1\n            namespace: istio-cb-ns\n            labels:\n              app: hello\n          spec:\n            containers:\n            - name: hello-server-1\n              image: docker.io/honester/hello-server:latest\n              imagePullPolicy: IfNotPresent\n              env:\n              - name: VERSION\n                value: \"v1\"\n              - name: LOG\n                value: \"1\"\n          &#45;&#45;&#45;\n          apiVersion: v1\n          kind: Pod\n          metadata:\n            name: hello-server-2\n            namespace: istio-cb-ns\n            labels:\n              app: hello\n          spec:\n            containers:\n            - name: hello-server-2\n              image: docker.io/honester/hello-server:latest\n              imagePullPolicy: IfNotPresent\n              env:\n              - name: VERSION\n                value: \"v2\"\n              - name: LOG\n                value: \"1\"\n              - name: RANDOM_ERROR\n                value: \"0.2\"\n          &#45;&#45;&#45;\n          apiVersion: v1\n          kind: Service\n          metadata:\n            name: svc-hello\n            namespace: istio-cb-ns\n            labels:\n              app: hello\n          spec:\n            selector:\n              app: hello\n            ports:\n            - name: http\n              protocol: TCP\n              port: 8080\n        EOF\n        </pre>\n      - 클라이언트용 Pod 설치\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: v1\n          kind: Pod\n          metadata:\n            name: httpbin\n            namespace: istio-cb-ns\n            labels:\n              app: httpbin\n          spec:\n            containers:\n            - name: httpbin\n              image: docker.io/honester/httpbin:latest\n              imagePullPolicy: IfNotPresent\n        EOF\n        </pre>\n      - 클라이언트(httpbin Pod)에서 svc-hello 호출(default, Round-Robin)\n        - (hello-server-2의 로그 모니터링)\n        - kubectl logs -f hello-server-2 -c hello-server-2 -n istio-cb-ns\n        - (클라이언트에서 svc-hello 서비스 10번 호출하기)\n        - for i in {1..10}; do kubectl exec -it httpbin -c httpbin -n istio-cb-ns --curl http://svc-hello.istio-cb-ns:8080; sleep 0.1; done\n      - 5XX 오류에 대해 해당 서비스 차단 및 Thread 부하에 따른 Circuit Breaker 생성\n        <pre style=\"white-space: pre-wrap\">\n        kubectl apply -f - &#60;&#60;EOF\n          apiVersion: networking.istio.io/v1alpha3\n          kind: DestinationRule\n          metadata:\n            name: dr-delivery\n            namespace: istio-cb-ns\n          spec:\n            host: delivery\n            trafficPolicy:\n              connectionPool:\n                http:\n                  http1MaxPendingRequests: 1\n                  maxRequestsPerConnection: 1\n              outlierDetection:\n                interval: 1s\n                consecutiveErrors: 1\n                baseEjectionTime: 3m\n                maxEjectionPercent: 100\n        EOF\n        \n\n        설명 : 1초 주기로 이상징후를 체크하며, 1번이라도 실패한 서비스는 3분동안 라우팅 대상에서 제외된다. 또한 모든 대상 서비스 인스턴스가 방출(제외)될 수 있다.\"\n        </pre>\n      - 클라이언트(httpbin Pod)에서 svc-hello 호출 및 CB 확인\n        - (hello-server-2의 로그 모니터링)\n        - kubectl logs -f hello-server-2 -c hello-server-2 -n istio-cb-ns\n        - (클라이언트에서 svc-hello 서비스 10번 호출하기)\n        - for i in {1..10}; do kubectl exec -it httpbin -c httpbin -n istio-cb-ns --curl http://svc-hello.istio-cb-ns:8080; sleep 0.1; done\n        - 모니터링 시스템(Kiali) : EXTERNAL-IP:20001 에서 Circuit Breaker 발생 확인 (뱃지)\n      - Clean-up\n        - kubectl delete pod/hello-server-1 pod/hello-server-2 pod/httpbin service/svc-hello dr/dr-hello -n istio-cb-ns\n\n  - <b>Lab. Istio Egress</b>\n    - 외부 도메인을 호출하는 v3 버전을 배포\n      - kubectl apply -f <(istioctl kube-inject -f recommendation/kubernetes/Deployment-v3.yml) -n tutorial\n    - 브라우저에서 Customer 서비스(Externl-IP:8080 접속)\n    - v3에서 날짜정보가 추가로 출력됨을 확인\n    - Istio 트래픽을 등록된 것만 허용하도록 변경\n      - <pre style=\"white-space: pre-wrap\">kubectl get configmap istio -n istio-system -o yaml | sed \"s/mode: ALLOW_ANY/mode: REGISTRY_ONLY/g\" | kubectl replace -n istio-system -f -</pre>\n    - 브라우저에서 Customer 서비스(Externl-IP:8080 접속)\n      - v3 은 서비스 오류로 인해 브라우저 확인 불가, Kiali 에서 확인\n    - 트래픽을 모두 v3 (weigh 100)로 라우팅하고 에러 화면 확인\n      - kubectl create -f istiofiles/destination-rule-recommendation-v1-v2-v3.yml -n tutorial\n      - kubectl create -f istiofiles/virtual-service-recommendation-v3.yml -n tutorial\n    - 브라우저에서 Customer 서비스(Externl-IP:8080 접속)\n      - 화면에 Error Log 출력 : “customer => Error: 503 - preference => Error: 500”\n    - 외부 도메인을 허용해 주는 ServiceEntry 를 생성하여 정상 접속 허용\n      - kubectl create -f istiofiles/service-entry-egress-worldclockapi.yml -n tutorial\n      - 브라우저에서 Customer 서비스(Externl-IP:8080 접속) - 정상 출력\n    - (테스트 후, 설정 복구)\n    - <pre style=\"white-space: pre-wrap\">kubectl get configmap istio -n istio-system -o yaml | sed \"s/mode: REGISTRY_ONLY/mode: ALLOW_ANY/g\" | kubectl replace -n istio-system -f -</pre>\n    \n</details>\n\n\n\n</p>\n</details>\n<hr />\n\n<br />\n\n'>\n        </mark-down>\n    </div>\n</template>\n\n\n<script>\n    // @group 02_06_08\n    export default {\n        name: 'OperationSeven',\n        props: {\n            \"실습 스크립트\": {\n                type: String\n            },\n        },\n\n    }\n</script>","path":"/operation/operation/operation-seven/","props":[{"name":"실습 스크립트"}],"componentDesc":{"group":["02_06_08"]},"fileInfo":{"name":"OperationSeven","path":"operation/operation/OperationSeven.vue","directory":"operation/operation"}},"allPagesByName":{"edges":[{"node":{"id":"455485b89fc07a2011135c71b3e5b06d","name":"index","path":"/operation/introduction/","props":[{"name":"교육과정 소개"}],"componentDesc":{"group":[]},"fileInfo":{"name":"index","path":"operation/introduction/index.vue","directory":"operation/introduction"}}},{"node":{"id":"f18f6749bc56709e8e73e001478bcb46","name":"CourseInfo","path":"/operation/introduction/schedule/","props":[{"name":"전체 교육과정 맵"}],"componentDesc":{"group":["01_01_02"]},"fileInfo":{"name":"schedule","path":"operation/introduction/schedule.vue","directory":"operation/introduction"}}},{"node":{"id":"2925affe20d6f52082da2ed5deba81bd","name":"ai-curriculum","path":"/operation/introduction/ai-curriculum/","props":[{"name":"🆕 AI 기반 CNA&MSA 플립러닝 마스터 과정"}],"componentDesc":{"group":["01_01_03"]},"fileInfo":{"name":"AI-Curriculum","path":"operation/introduction/AI-Curriculum.vue","directory":"operation/introduction"}}},{"node":{"id":"1e338f81867fab541087ebe83b959719","name":"fundamental","path":"/operation/introduction/fundamental/","props":[{"name":"Fundamental 과정"}],"componentDesc":{"group":["01_01_04"]},"fileInfo":{"name":"Fundamental","path":"operation/introduction/Fundamental.vue","directory":"operation/introduction"}}},{"node":{"id":"be00513bd89ffad07cc0e3cbdcde62c9","name":"curriculum","path":"/operation/introduction/curriculum/","props":[{"name":"Intermediate 과정"}],"componentDesc":{"group":["01_01_05"]},"fileInfo":{"name":"Curriculum","path":"operation/introduction/Curriculum.vue","directory":"operation/introduction"}}},{"node":{"id":"1bade8d0fcc428cbdb1cfb76e5eba996","name":"quick-understanding-cna","path":"/operation/introduction/quick-understanding-cna/","props":[{"name":"┣ Quick Understanding"}],"componentDesc":{"group":["01_01_06"]},"fileInfo":{"name":"QuickUnderstandingCNA","path":"operation/introduction/QuickUnderstandingCNA.vue","directory":"operation/introduction"}}},{"node":{"id":"c622030c4095d41bae8f79f418f549c9","name":"flipped-learning4-days-cna-course","path":"/operation/introduction/flipped-learning4-days-cna-course/","props":[{"name":"┣ Flipped Learning"}],"componentDesc":{"group":["01_01_07"]},"fileInfo":{"name":"FlippedLearning4DaysCNACourse","path":"operation/introduction/FlippedLearning4DaysCNACourse.vue","directory":"operation/introduction"}}},{"node":{"id":"a0d8f3caef345dd5d492b270b1960383","name":"standard-cna","path":"/operation/introduction/standard-cna/","props":[{"name":"┣ Standard Learning"}],"componentDesc":{"group":["01_01_08"]},"fileInfo":{"name":"StandardCNA","path":"operation/introduction/StandardCNA.vue","directory":"operation/introduction"}}},{"node":{"id":"cafbbcc8251047b22a56bef64e81b063","name":"enterprise-full-day14-days-course","path":"/operation/introduction/enterprise-full-day14-days-course/","props":[{"name":"┗ Enterprise Full-day"}],"componentDesc":{"group":["01_01_09"]},"fileInfo":{"name":"EnterpriseFull-day14DaysCourse","path":"operation/introduction/EnterpriseFull-day14DaysCourse.vue","directory":"operation/introduction"}}},{"node":{"id":"acef2a7532bd3000d50c662d773dbf5b","name":"advanced","path":"/operation/introduction/advanced/","props":[{"name":"Advanced 과정"}],"componentDesc":{"group":["01_01_10"]},"fileInfo":{"name":"Advanced","path":"operation/introduction/Advanced.vue","directory":"operation/introduction"}}},{"node":{"id":"2a1d0cbdc49425fb011d0c252f06495d","name":"one-point-lesson","path":"/operation/introduction/one-point-lesson/","props":[{"name":"MSA 컨설팅"}],"componentDesc":{"group":["01_01_11"]},"fileInfo":{"name":"OnePointLesson","path":"operation/introduction/OnePointLesson.vue","directory":"operation/introduction"}}},{"node":{"id":"eeaab9630dc4711635747aa9fda862d7","name":"index","path":"/operation/planning/","props":[{"name":"계획"}],"componentDesc":{"group":["01_02_01"]},"fileInfo":{"name":"index","path":"operation/planning/index.vue","directory":"operation/planning"}}},{"node":{"id":"43941ed8331160d458de8af0ed8c55a0","name":"StepByStepGoal","path":"/operation/planning/step-by-step-goal/","props":[{"name":"단계별 수행목표"}],"componentDesc":{"group":["01_02_02"]},"fileInfo":{"name":"StepByStepGoal","path":"operation/planning/StepByStepGoal.vue","directory":"operation/planning"}}},{"node":{"id":"eaf523fa39f4a074e77055d6d5c89514","name":"DesignMSA","path":"/operation/planning/design-msa/","props":[{"name":"마이크로서비스 설계원칙"}],"componentDesc":{"group":["01_02_03"]},"fileInfo":{"name":"DesignMSA","path":"operation/planning/DesignMSA.vue","directory":"operation/planning"}}},{"node":{"id":"fea996d53387462e8d43e38f46f84ab1","name":"AvatarPatten","path":"/operation/planning/avatar-patten/","props":[{"name":"구현 패턴"}],"componentDesc":{"group":["01_02_04"]},"fileInfo":{"name":"AvatarPatten","path":"operation/planning/AvatarPatten.vue","directory":"operation/planning"}}},{"node":{"id":"58164a2c54b07d6052fda331fec0f758","name":"Planning","path":"/operation/planning/planning/","props":[{"name":"최종목표 수립"}],"componentDesc":{"group":["01_02_05"]},"fileInfo":{"name":"Planning","path":"operation/planning/Planning.vue","directory":"operation/planning"}}},{"node":{"id":"f7715041966f522014e526ecf22aad61","name":"SystemSecurity","path":"/operation/planning/system-security/","props":[{"name":"시스템 보안"}],"componentDesc":{"group":["01_02_06"]},"fileInfo":{"name":"SystemSecurity","path":"operation/planning/SystemSecurity.vue","directory":"operation/planning"}}},{"node":{"id":"abf384f6ffc7520780adece4dfe537ac","name":"PerformanceMeasures","path":"/operation/planning/performance-measures/","props":[{"name":"성능 확보 방안"}],"componentDesc":{"group":["01_02_07"]},"fileInfo":{"name":"PerformanceMeasures","path":"operation/planning/PerformanceMeasures.vue","directory":"operation/planning"}}},{"node":{"id":"b31eb669d439e2c8533478ddf58c3a3e","name":"CloudIq","path":"/operation/planning/cloud-iq/","props":[{"name":"클라우드 네이티브 전환 가이드"}],"componentDesc":{"group":["01_02_08"]},"fileInfo":{"name":"CloudIq","path":"operation/planning/CloudIq.vue","directory":"operation/planning"}}},{"node":{"id":"10730aa73947978d675e7c8daf2416e4","name":"TestMeasures","path":"/operation/planning/test-measures/","props":[{"name":"테스트 방안"}],"componentDesc":{"group":["01_02_09_true"]},"fileInfo":{"name":"TestMeasures","path":"operation/planning/TestMeasures.vue","directory":"operation/planning"}}},{"node":{"id":"21d97888270a338620632c524d256291","name":"index","path":"/operation/analysis/","props":[{"name":"분석"}],"componentDesc":{"group":["02_01_01"]},"fileInfo":{"name":"index","path":"operation/analysis/index.vue","directory":"operation/analysis"}}},{"node":{"id":"a9ee193c6ff3869677568b484bc66534","name":"AnalysisDecomposition","path":"/operation/analysis/analysis-decomposition/","props":[{"name":"마이크로서비스 분해 전략"}],"componentDesc":{"group":["02_01_02"]},"fileInfo":{"name":"AnalysisDecomposition","path":"operation/analysis/AnalysisDecomposition.vue","directory":"operation/analysis"}}},{"node":{"id":"415e97d88f108384b5d54275fd20e0b0","name":"AnalysisOne","path":"/operation/analysis/analysis-one/","props":[{"name":"관심사 분리 필요성"}],"componentDesc":{"group":["02_01_03"]},"fileInfo":{"name":"AnalysisOne","path":"operation/analysis/AnalysisOne.vue","directory":"operation/analysis"}}},{"node":{"id":"39d48b66f14d620fe41a006bb7936fa0","name":"AnalysisTwo","path":"/operation/analysis/analysis-two/","props":[{"name":"애자일 필요성"}],"componentDesc":{"group":["02_01_04"]},"fileInfo":{"name":"AnalysisTwo","path":"operation/analysis/AnalysisTwo.vue","directory":"operation/analysis"}}},{"node":{"id":"8dff280938d055bec5f88b9567c42371","name":"AnalysisThree","path":"/operation/analysis/analysis-three/","props":[{"name":"레가시 모노리식의 한계점"}],"componentDesc":{"group":["02_01_05"]},"fileInfo":{"name":"AnalysisThree","path":"operation/analysis/AnalysisThree.vue","directory":"operation/analysis"}}},{"node":{"id":"f8c489692b996c65fa72641c291c45de","name":"index","path":"/operation/design/","props":[{"name":"설계"}],"componentDesc":{"group":["02_02_01"]},"fileInfo":{"name":"index","path":"operation/design/index.vue","directory":"operation/design"}}},{"node":{"id":"bc4f780de48b4491c32b9aaab8409ef8","name":"DesignCohesion","path":"/operation/design/design-cohesion/","props":[{"name":"응집도와 결합도"}],"componentDesc":{"group":["02_02_02"]},"fileInfo":{"name":"DesignCohesion","path":"operation/design/DesignCohesion.vue","directory":"operation/design"}}},{"node":{"id":"a2953ad4f2aaa393654dcc9f26a1edf6","name":"DesignOne","path":"/operation/design/design-one/","props":[{"name":"접근법과 분석패턴"}],"componentDesc":{"group":["02_02_03"]},"fileInfo":{"name":"DesignOne","path":"operation/design/DesignOne.vue","directory":"operation/design"}}},{"node":{"id":"715d7a6a67e1618c6f4a53bbe2ee693e","name":"DesignTwo","path":"/operation/design/design-two/","props":[{"name":"도메인 주도 설계(DDD)"}],"componentDesc":{"group":["02_02_04"]},"fileInfo":{"name":"DesignTwo","path":"operation/design/DesignTwo.vue","directory":"operation/design"}}},{"node":{"id":"d7a2a0541b3b683fd126b13f50f7fdd5","name":"DesignTradeOff","path":"/operation/design/design-trade-off/","props":[{"name":"어그리게이트와 이벤추얼 컨시스턴시"}],"componentDesc":{"group":["02_02_05"]},"fileInfo":{"name":"DesignTradeOff","path":"operation/design/DesignTradeOff.vue","directory":"operation/design"}}},{"node":{"id":"7b3ddc10cb5fe68380052c5fc4154691","name":"DesignAggregate","path":"/operation/design/design-aggregate/","props":[{"name":"어그리게이트 모범 사례"}],"componentDesc":{"group":["02_02_06"]},"fileInfo":{"name":"DesignAggregate","path":"operation/design/DesignAggregate.vue","directory":"operation/design"}}},{"node":{"id":"5d06067d6cb5f6d93bb81adfbdea1435","name":"DesignThree","path":"/operation/design/design-three/","props":[{"name":"이벤트스토밍"}],"componentDesc":{"group":["02_02_07"]},"fileInfo":{"name":"DesignThree","path":"operation/design/DesignThree.vue","directory":"operation/design"}}},{"node":{"id":"de3aeab5e834de266cb1e81f8c14e7e3","name":"DesignThreeOne","path":"/operation/design/design-three-one/","props":[{"name":"이벤트스토밍과 DDD의 관계"}],"componentDesc":{"group":["02_02_08"]},"fileInfo":{"name":"DesignThreeOne","path":"operation/design/DesignThreeOne.vue","directory":"operation/design"}}},{"node":{"id":"b4368bda16baebbfd2226f9c5adf0d42","name":"DesignFour","path":"/operation/design/design-four/","props":[{"name":"서비스 서열과 역학 관계"}],"componentDesc":{"group":["02_02_09"]},"fileInfo":{"name":"DesignFour","path":"operation/design/DesignFour.vue","directory":"operation/design"}}},{"node":{"id":"066b87fab26841fc32908a973c58bcbf","name":"DesignFive","path":"/operation/design/design-five/","props":[{"name":"아키텍처 설계"}],"componentDesc":{"group":["02_02_10"]},"fileInfo":{"name":"DesignFive","path":"operation/design/DesignFive.vue","directory":"operation/design"}}},{"node":{"id":"f1b1f93080994d9852bc52eca704267c","name":"DesignSix","path":"/operation/design/design-six/","props":[{"name":"서비스 디스커버리 패턴"}],"componentDesc":{"group":["02_02_11"]},"fileInfo":{"name":"DesignSix","path":"operation/design/DesignSix.vue","directory":"operation/design"}}},{"node":{"id":"9ae9ab1f6575feb602bf9a57b7818772","name":"DesignSeven","path":"/operation/design/design-seven/","props":[{"name":"마이크로서비스 보안설계"}],"componentDesc":{"group":["02_02_12_true"]},"fileInfo":{"name":"DesignSeven","path":"operation/design/DesignSeven.vue","directory":"operation/design"}}},{"node":{"id":"813f435401e4c85b9a53b0b0590c4675","name":"DesignEight","path":"/operation/design/design-eight/","props":[{"name":"프론트엔드 설계"}],"componentDesc":{"group":["02_02_13"]},"fileInfo":{"name":"DesignEight","path":"operation/design/DesignEight.vue","directory":"operation/design"}}},{"node":{"id":"e5f92b9f6556884b1a433342751f9c4b","name":"index","path":"/operation/implementation/","props":[{"name":"구현"}],"componentDesc":{"group":["02_03_01"]},"fileInfo":{"name":"index","path":"operation/implementation/index.vue","directory":"operation/implementation"}}},{"node":{"id":"a71dc4b521d90d7a5fd328ca6485cf2c","name":"ImplementationTwo","path":"/operation/implementation/implementation-two/","props":[{"name":"CNA 구현 프레임워크"}],"componentDesc":{"group":["02_03_03"]},"fileInfo":{"name":"ImplementationTwo","path":"operation/implementation/ImplementationTwo.vue","directory":"operation/implementation"}}},{"node":{"id":"118b27a63a6495e33fdda6ba920bb72f","name":"ImplementationThree","path":"/operation/implementation/implementation-three/","props":[{"name":"일반적인 CNA 구현"}],"componentDesc":{"group":["02_03_04"]},"fileInfo":{"name":"ImplementationThree","path":"operation/implementation/ImplementationThree.vue","directory":"operation/implementation"}}},{"node":{"id":"d4ce467fe9b110cf3d9204a7c87c2d31","name":"ImplementationFour","path":"/operation/implementation/implementation-four/","props":[{"name":"MSA 기반 CNA구현"}],"componentDesc":{"group":["02_03_05_true"]},"fileInfo":{"name":"ImplementationFour","path":"operation/implementation/ImplementationFour.vue","directory":"operation/implementation"}}},{"node":{"id":"d848f0a1719ff09b14fae915a95f0f1b","name":"ImplementationFive","path":"/operation/implementation/implementation-five/","props":[{"name":"모노리스 to MSA전환"}],"componentDesc":{"group":["02_03_06_true"]},"fileInfo":{"name":"ImplementationFive","path":"operation/implementation/ImplementationFive.vue","directory":"operation/implementation"}}},{"node":{"id":"ad6bff45538535a3bd2887a82ebe29d3","name":"ImplementationSix","path":"/operation/implementation/implementation-six/","props":[{"name":"게이트웨이(Gateway)"}],"componentDesc":{"group":["02_03_07_true"]},"fileInfo":{"name":"ImplementationSix","path":"operation/implementation/ImplementationSix.vue","directory":"operation/implementation"}}},{"node":{"id":"f12c036cfa3fb9f28b0a1696e9afb5b0","name":"ImplementationSeven","path":"/operation/implementation/implementation-seven/","props":[{"name":"이벤트기반 메세지채널"}],"componentDesc":{"group":["02_03_08_true"]},"fileInfo":{"name":"ImplementationSeven","path":"operation/implementation/ImplementationSeven.vue","directory":"operation/implementation"}}},{"node":{"id":"0a87f2e4004042d6d0b4ba4458b19543","name":"index","path":"/operation/integration/","props":[{"name":"통합"}],"componentDesc":{"group":["02_04_01"]},"fileInfo":{"name":"index","path":"operation/integration/index.vue","directory":"operation/integration"}}},{"node":{"id":"4fc98863b3f01e7301e7eb51038eda20","name":"IntegrationOne","path":"/operation/integration/integration-one/","props":[{"name":"Front-End에서의 통합"}],"componentDesc":{"group":["02_04_02_true"]},"fileInfo":{"name":"IntegrationOne","path":"operation/integration/IntegrationOne.vue","directory":"operation/integration"}}},{"node":{"id":"6309cd44f104ea41564b644aaebb2563","name":"IntegrationTwo","path":"/operation/integration/integration-two/","props":[{"name":"동기호출에 의한 통합"}],"componentDesc":{"group":["02_04_03_true"]},"fileInfo":{"name":"IntegrationTwo","path":"operation/integration/IntegrationTwo.vue","directory":"operation/integration"}}},{"node":{"id":"391739994a94024e60b94f45131831cc","name":"IntegrationThree","path":"/operation/integration/integration-three/","props":[{"name":"Event-driven 기반 통합"}],"componentDesc":{"group":["02_04_04_true"]},"fileInfo":{"name":"IntegrationThree","path":"operation/integration/IntegrationThree.vue","directory":"operation/integration"}}},{"node":{"id":"cc5a7890ef0e7692a6a62142bd9665b4","name":"IntegrationFour","path":"/operation/integration/integration-four/","props":[{"name":"이벤추얼 트랜잭션"}],"componentDesc":{"group":["02_04_05_true"]},"fileInfo":{"name":"IntegrationFour","path":"operation/integration/IntegrationFour.vue","directory":"operation/integration"}}},{"node":{"id":"376b545a9bee05e0e0edb1394a838af7","name":"IntegrationFive","path":"/operation/integration/integration-five/","props":[{"name":"데이터 프로젝션"}],"componentDesc":{"group":["02_04_06_true"]},"fileInfo":{"name":"IntegrationFive","path":"operation/integration/IntegrationFive.vue","directory":"operation/integration"}}},{"node":{"id":"eae94a205296c8c586ca7e896180fe50","name":"IntegrationSix","path":"/operation/integration/integration-six/","props":[{"name":"CQRS"}],"componentDesc":{"group":["02_04_07_true"]},"fileInfo":{"name":"IntegrationSix","path":"operation/integration/IntegrationSix.vue","directory":"operation/integration"}}},{"node":{"id":"8351b072e264a8a0fe00edaefaff73d7","name":"index","path":"/operation/deployment/","props":[{"name":"배포"}],"componentDesc":{"group":["02_05_01"]},"fileInfo":{"name":"index","path":"operation/deployment/index.vue","directory":"operation/deployment"}}},{"node":{"id":"ceaa91041ecececd893ac50a0ac98232","name":"DeploymentOne","path":"/operation/deployment/deployment-one/","props":[{"name":"지속적인 통합"}],"componentDesc":{"group":["02_05_02"]},"fileInfo":{"name":"DeploymentOne","path":"operation/deployment/DeploymentOne.vue","directory":"operation/deployment"}}},{"node":{"id":"c08d003b92f6195a9c3d9220876cfcc6","name":"DeploymentTwo","path":"/operation/deployment/deployment-two/","props":[{"name":"파이프라인(Pipeline)"}],"componentDesc":{"group":["02_05_03"]},"fileInfo":{"name":"DeploymentTwo","path":"operation/deployment/DeploymentTwo.vue","directory":"operation/deployment"}}},{"node":{"id":"2f5d16b6a07e14c5cf5ccfba84168a58","name":"DeploymentThree","path":"/operation/deployment/deployment-three/","props":[{"name":"배포 전략"}],"componentDesc":{"group":["02_05_04"]},"fileInfo":{"name":"DeploymentThree","path":"operation/deployment/DeploymentThree.vue","directory":"operation/deployment"}}},{"node":{"id":"b00195f0569e3f550742aadd3c72b5c6","name":"DeploymentFour","path":"/operation/deployment/deployment-four/","props":[{"name":"실습 스크립트"}],"componentDesc":{"group":["02_05_05"]},"fileInfo":{"name":"DeploymentFour","path":"operation/deployment/DeploymentFour.vue","directory":"operation/deployment"}}},{"node":{"id":"70dcb5649cee1dc9e6f986fa06a30e37","name":"index","path":"/operation/operation/","props":[{"name":"운영"}],"componentDesc":{"group":["02_06_01"]},"fileInfo":{"name":"index","path":"operation/operation/index.vue","directory":"operation/operation"}}},{"node":{"id":"a6367070dbcf4005b9749f36b12ce957","name":"OperationOne","path":"/operation/operation/operation-one/","props":[{"name":"도커 이미지"}],"componentDesc":{"group":["02_06_02_true"]},"fileInfo":{"name":"OperationOne","path":"operation/operation/OperationOne.vue","directory":"operation/operation"}}},{"node":{"id":"5c605f462cb5af809e8c01591411c8a8","name":"OperationTwo","path":"/operation/operation/operation-two/","props":[{"name":"Kubernetes 오브젝트 모델"}],"componentDesc":{"group":["02_06_03_true"]},"fileInfo":{"name":"OperationTwo","path":"operation/operation/OperationTwo.vue","directory":"operation/operation"}}},{"node":{"id":"571dfcc524308562cfbe688635a934b1","name":"OperationThree","path":"/operation/operation/operation-three/","props":[{"name":"Kubernetes Advanced 객체"}],"componentDesc":{"group":["02_06_04_true"]},"fileInfo":{"name":"OperationThree","path":"operation/operation/OperationThree.vue","directory":"operation/operation"}}},{"node":{"id":"df7d03483f8157c6354a82c325a43539","name":"OperationFour","path":"/operation/operation/operation-four/","props":[{"name":"Kubernetes 아키텍처"}],"componentDesc":{"group":["02_06_05"]},"fileInfo":{"name":"OperationFour","path":"operation/operation/OperationFour.vue","directory":"operation/operation"}}},{"node":{"id":"05165bc39ff0940d8503c5e13cdf8279","name":"OperationFive","path":"/operation/operation/operation-five/","props":[{"name":"서비스메시 이스티오"}],"componentDesc":{"group":["02_06_06_true"]},"fileInfo":{"name":"OperationFive","path":"operation/operation/OperationFive.vue","directory":"operation/operation"}}},{"node":{"id":"3e40606f453cbda5f32f9f6f7d998c7e","name":"OperationSix","path":"/operation/operation/operation-six/","props":[{"name":"마이크로서비스 모니터링"}],"componentDesc":{"group":["02_06_07"]},"fileInfo":{"name":"OperationSix","path":"operation/operation/OperationSix.vue","directory":"operation/operation"}}},{"node":{"id":"fab130dfedfaab931b1fa670a960c27f","name":"OperationSeven","path":"/operation/operation/operation-seven/","props":[{"name":"실습 스크립트"}],"componentDesc":{"group":["02_06_08"]},"fileInfo":{"name":"OperationSeven","path":"operation/operation/OperationSeven.vue","directory":"operation/operation"}}},{"node":{"id":"1fd099929393e781c266bc8994c71520","name":"index","path":"/operation/strategy/","props":[{"name":"MSA 전환전략"}],"componentDesc":{"group":["03_03_01"]},"fileInfo":{"name":"index","path":"operation/strategy/index.vue","directory":"operation/strategy"}}},{"node":{"id":"434f50ea02cbdbd2ae7abc51bee9b7c9","name":"index","path":"/operation/tool/","props":[{"name":"MSA 도구"}],"componentDesc":{"group":["03_03_01"]},"fileInfo":{"name":"index","path":"operation/tool/index.vue","directory":"operation/tool"}}},{"node":{"id":"847a9951a2b7a3ee7731f019fe5423d0","name":"StrategyOne","path":"/operation/strategy/strategy-one/","props":[{"name":"전환대상 식별"}],"componentDesc":{"group":["03_03_02"]},"fileInfo":{"name":"StrategyOne","path":"operation/strategy/StrategyOne.vue","directory":"operation/strategy"}}},{"node":{"id":"2e0dbc98f33cf40c7a0d4ea6697de88d","name":"ToolOne","path":"/operation/tool/tool-one/","props":[{"name":"이벤트스토밍 도구"}],"componentDesc":{"group":["03_03_02"]},"fileInfo":{"name":"ToolOne","path":"operation/tool/ToolOne.vue","directory":"operation/tool"}}},{"node":{"id":"115c7f167bcf04682abeb72ec7282e2e","name":"StrategyTwo","path":"/operation/strategy/strategy-two/","props":[{"name":"전환방식 선정"}],"componentDesc":{"group":["03_03_03"]},"fileInfo":{"name":"StrategyTwo","path":"operation/strategy/StrategyTwo.vue","directory":"operation/strategy"}}},{"node":{"id":"5356b3b016ca28c7fe3d5d1069077806","name":"StrategyThree","path":"/operation/strategy/strategy-three/","props":[{"name":"전환이슈 및 솔루션"}],"componentDesc":{"group":["03_03_04"]},"fileInfo":{"name":"StrategyThree","path":"operation/strategy/StrategyThree.vue","directory":"operation/strategy"}}},{"node":{"id":"906edca02a2c2e37d9566ec0918770a7","name":"ToolThree","path":"/operation/tool/tool-three/","props":[{"name":"배포 다이어그래밍 도구"}],"componentDesc":{"group":["03_03_04"]},"fileInfo":{"name":"ToolThree","path":"operation/tool/ToolThree.vue","directory":"operation/tool"}}},{"node":{"id":"906e7d4891ca3ee147d67c15cd15f0a5","name":"ArchitectureOne","path":"/operation/architecture/architecture-one/","props":[{"name":"API Gateway"}],"componentDesc":{"group":["03_04_01"]},"fileInfo":{"name":"ArchitectureOne","path":"operation/architecture/ArchitectureOne.vue","directory":"operation/architecture"}}},{"node":{"id":"b183fd861b323503a21a80ef7c01091e","name":"index","path":"/operation/architecture/","props":[{"name":"MSA 아우터 아키텍처"}],"componentDesc":{"group":["03_04_01"]},"fileInfo":{"name":"index","path":"operation/architecture/index.vue","directory":"operation/architecture"}}},{"node":{"id":"031e653a03b40e6918c87598eebd340f","name":"ArchitectureTwo","path":"/operation/architecture/architecture-two/","props":[{"name":"Service Mesh"}],"componentDesc":{"group":["03_04_03"]},"fileInfo":{"name":"ArchitectureTwo","path":"operation/architecture/ArchitectureTwo.vue","directory":"operation/architecture"}}},{"node":{"id":"87f88aa6bec4824bb88f99284f3edcd1","name":"ArchitectureThree","path":"/operation/architecture/architecture-three/","props":[{"name":"Container Management"}],"componentDesc":{"group":["03_04_04"]},"fileInfo":{"name":"ArchitectureThree","path":"operation/architecture/ArchitectureThree.vue","directory":"operation/architecture"}}},{"node":{"id":"139588c1bc0cabeacbeff3b0a4e813a7","name":"ArchitectureFour","path":"/operation/architecture/architecture-four/","props":[{"name":"Backing Service"}],"componentDesc":{"group":["03_04_05"]},"fileInfo":{"name":"ArchitectureFour","path":"operation/architecture/ArchitectureFour.vue","directory":"operation/architecture"}}},{"node":{"id":"a3220fe8675b5f4015b7e9344842edfc","name":"ArchitectureFive","path":"/operation/architecture/architecture-five/","props":[{"name":"Telemetry"}],"componentDesc":{"group":["03_04_06"]},"fileInfo":{"name":"ArchitectureFive","path":"operation/architecture/ArchitectureFive.vue","directory":"operation/architecture"}}}]},"allPages":{"edges":[{"node":{"name":"ToolThree","path":"/operation/tool/tool-three/","fileInfo":{"name":"ToolThree","directory":"operation/tool"}}},{"node":{"name":"index","path":"/operation/tool/","fileInfo":{"name":"index","directory":"operation/tool"}}},{"node":{"name":"ToolOne","path":"/operation/tool/tool-one/","fileInfo":{"name":"ToolOne","directory":"operation/tool"}}},{"node":{"name":"index","path":"/operation/strategy/","fileInfo":{"name":"index","directory":"operation/strategy"}}},{"node":{"name":"StrategyTwo","path":"/operation/strategy/strategy-two/","fileInfo":{"name":"StrategyTwo","directory":"operation/strategy"}}},{"node":{"name":"index","path":"/operation/planning/","fileInfo":{"name":"index","directory":"operation/planning"}}},{"node":{"name":"StrategyThree","path":"/operation/strategy/strategy-three/","fileInfo":{"name":"StrategyThree","directory":"operation/strategy"}}},{"node":{"name":"StrategyOne","path":"/operation/strategy/strategy-one/","fileInfo":{"name":"StrategyOne","directory":"operation/strategy"}}},{"node":{"name":"TestMeasures","path":"/operation/planning/test-measures/","fileInfo":{"name":"TestMeasures","directory":"operation/planning"}}},{"node":{"name":"StepByStepGoal","path":"/operation/planning/step-by-step-goal/","fileInfo":{"name":"StepByStepGoal","directory":"operation/planning"}}},{"node":{"name":"Planning","path":"/operation/planning/planning/","fileInfo":{"name":"Planning","directory":"operation/planning"}}},{"node":{"name":"SystemSecurity","path":"/operation/planning/system-security/","fileInfo":{"name":"SystemSecurity","directory":"operation/planning"}}},{"node":{"name":"CloudIq","path":"/operation/planning/cloud-iq/","fileInfo":{"name":"CloudIq","directory":"operation/planning"}}},{"node":{"name":"PerformanceMeasures","path":"/operation/planning/performance-measures/","fileInfo":{"name":"PerformanceMeasures","directory":"operation/planning"}}},{"node":{"name":"DesignMSA","path":"/operation/planning/design-msa/","fileInfo":{"name":"DesignMSA","directory":"operation/planning"}}},{"node":{"name":"AvatarPatten","path":"/operation/planning/avatar-patten/","fileInfo":{"name":"AvatarPatten","directory":"operation/planning"}}},{"node":{"name":"index","path":"/operation/operation/","fileInfo":{"name":"index","directory":"operation/operation"}}},{"node":{"name":"OperationTwo","path":"/operation/operation/operation-two/","fileInfo":{"name":"OperationTwo","directory":"operation/operation"}}},{"node":{"name":"OperationSix","path":"/operation/operation/operation-six/","fileInfo":{"name":"OperationSix","directory":"operation/operation"}}},{"node":{"name":"OperationThree","path":"/operation/operation/operation-three/","fileInfo":{"name":"OperationThree","directory":"operation/operation"}}},{"node":{"name":"OperationSeven","path":"/operation/operation/operation-seven/","fileInfo":{"name":"OperationSeven","directory":"operation/operation"}}},{"node":{"name":"OperationOne","path":"/operation/operation/operation-one/","fileInfo":{"name":"OperationOne","directory":"operation/operation"}}},{"node":{"name":"index","path":"/operation/introduction/","fileInfo":{"name":"index","directory":"operation/introduction"}}},{"node":{"name":"OperationFive","path":"/operation/operation/operation-five/","fileInfo":{"name":"OperationFive","directory":"operation/operation"}}},{"node":{"name":"CourseInfo","path":"/operation/introduction/schedule/","fileInfo":{"name":"schedule","directory":"operation/introduction"}}},{"node":{"name":"OperationFour","path":"/operation/operation/operation-four/","fileInfo":{"name":"OperationFour","directory":"operation/operation"}}},{"node":{"name":"standard-cna","path":"/operation/introduction/standard-cna/","fileInfo":{"name":"StandardCNA","directory":"operation/introduction"}}},{"node":{"name":"quick-understanding-cna","path":"/operation/introduction/quick-understanding-cna/","fileInfo":{"name":"QuickUnderstandingCNA","directory":"operation/introduction"}}},{"node":{"name":"one-point-lesson","path":"/operation/introduction/one-point-lesson/","fileInfo":{"name":"OnePointLesson","directory":"operation/introduction"}}},{"node":{"name":"fundamental","path":"/operation/introduction/fundamental/","fileInfo":{"name":"Fundamental","directory":"operation/introduction"}}},{"node":{"name":"enterprise-full-day14-days-course","path":"/operation/introduction/enterprise-full-day14-days-course/","fileInfo":{"name":"EnterpriseFull-day14DaysCourse","directory":"operation/introduction"}}},{"node":{"name":"advanced","path":"/operation/introduction/advanced/","fileInfo":{"name":"Advanced","directory":"operation/introduction"}}},{"node":{"name":"flipped-learning4-days-cna-course","path":"/operation/introduction/flipped-learning4-days-cna-course/","fileInfo":{"name":"FlippedLearning4DaysCNACourse","directory":"operation/introduction"}}},{"node":{"name":"curriculum","path":"/operation/introduction/curriculum/","fileInfo":{"name":"Curriculum","directory":"operation/introduction"}}},{"node":{"name":"ai-curriculum","path":"/operation/introduction/ai-curriculum/","fileInfo":{"name":"AI-Curriculum","directory":"operation/introduction"}}},{"node":{"name":"IntegrationTwo","path":"/operation/integration/integration-two/","fileInfo":{"name":"IntegrationTwo","directory":"operation/integration"}}},{"node":{"name":"index","path":"/operation/integration/","fileInfo":{"name":"index","directory":"operation/integration"}}},{"node":{"name":"IntegrationThree","path":"/operation/integration/integration-three/","fileInfo":{"name":"IntegrationThree","directory":"operation/integration"}}},{"node":{"name":"IntegrationSix","path":"/operation/integration/integration-six/","fileInfo":{"name":"IntegrationSix","directory":"operation/integration"}}},{"node":{"name":"IntegrationOne","path":"/operation/integration/integration-one/","fileInfo":{"name":"IntegrationOne","directory":"operation/integration"}}},{"node":{"name":"IntegrationFour","path":"/operation/integration/integration-four/","fileInfo":{"name":"IntegrationFour","directory":"operation/integration"}}},{"node":{"name":"IntegrationFive","path":"/operation/integration/integration-five/","fileInfo":{"name":"IntegrationFive","directory":"operation/integration"}}},{"node":{"name":"index","path":"/operation/implementation/","fileInfo":{"name":"index","directory":"operation/implementation"}}},{"node":{"name":"ImplementationTwo","path":"/operation/implementation/implementation-two/","fileInfo":{"name":"ImplementationTwo","directory":"operation/implementation"}}},{"node":{"name":"ImplementationThree","path":"/operation/implementation/implementation-three/","fileInfo":{"name":"ImplementationThree","directory":"operation/implementation"}}},{"node":{"name":"ImplementationFour","path":"/operation/implementation/implementation-four/","fileInfo":{"name":"ImplementationFour","directory":"operation/implementation"}}},{"node":{"name":"ImplementationSix","path":"/operation/implementation/implementation-six/","fileInfo":{"name":"ImplementationSix","directory":"operation/implementation"}}},{"node":{"name":"ImplementationSeven","path":"/operation/implementation/implementation-seven/","fileInfo":{"name":"ImplementationSeven","directory":"operation/implementation"}}},{"node":{"name":"ImplementationFive","path":"/operation/implementation/implementation-five/","fileInfo":{"name":"ImplementationFive","directory":"operation/implementation"}}},{"node":{"name":"index","path":"/operation/design/","fileInfo":{"name":"index","directory":"operation/design"}}},{"node":{"name":"DesignTwo","path":"/operation/design/design-two/","fileInfo":{"name":"DesignTwo","directory":"operation/design"}}},{"node":{"name":"DesignTradeOff","path":"/operation/design/design-trade-off/","fileInfo":{"name":"DesignTradeOff","directory":"operation/design"}}},{"node":{"name":"DesignThreeOne","path":"/operation/design/design-three-one/","fileInfo":{"name":"DesignThreeOne","directory":"operation/design"}}},{"node":{"name":"DesignThree","path":"/operation/design/design-three/","fileInfo":{"name":"DesignThree","directory":"operation/design"}}},{"node":{"name":"DesignSeven","path":"/operation/design/design-seven/","fileInfo":{"name":"DesignSeven","directory":"operation/design"}}},{"node":{"name":"DesignSix","path":"/operation/design/design-six/","fileInfo":{"name":"DesignSix","directory":"operation/design"}}},{"node":{"name":"DesignFour","path":"/operation/design/design-four/","fileInfo":{"name":"DesignFour","directory":"operation/design"}}},{"node":{"name":"DesignEight","path":"/operation/design/design-eight/","fileInfo":{"name":"DesignEight","directory":"operation/design"}}},{"node":{"name":"DesignOne","path":"/operation/design/design-one/","fileInfo":{"name":"DesignOne","directory":"operation/design"}}},{"node":{"name":"DesignFive","path":"/operation/design/design-five/","fileInfo":{"name":"DesignFive","directory":"operation/design"}}},{"node":{"name":"DesignAggregate","path":"/operation/design/design-aggregate/","fileInfo":{"name":"DesignAggregate","directory":"operation/design"}}},{"node":{"name":"DesignCohesion","path":"/operation/design/design-cohesion/","fileInfo":{"name":"DesignCohesion","directory":"operation/design"}}},{"node":{"name":"index","path":"/operation/deployment/","fileInfo":{"name":"index","directory":"operation/deployment"}}},{"node":{"name":"DeploymentTwo","path":"/operation/deployment/deployment-two/","fileInfo":{"name":"DeploymentTwo","directory":"operation/deployment"}}},{"node":{"name":"DeploymentThree","path":"/operation/deployment/deployment-three/","fileInfo":{"name":"DeploymentThree","directory":"operation/deployment"}}},{"node":{"name":"DeploymentFour","path":"/operation/deployment/deployment-four/","fileInfo":{"name":"DeploymentFour","directory":"operation/deployment"}}},{"node":{"name":"DeploymentOne","path":"/operation/deployment/deployment-one/","fileInfo":{"name":"DeploymentOne","directory":"operation/deployment"}}},{"node":{"name":"ArchitectureTwo","path":"/operation/architecture/architecture-two/","fileInfo":{"name":"ArchitectureTwo","directory":"operation/architecture"}}},{"node":{"name":"index","path":"/operation/architecture/","fileInfo":{"name":"index","directory":"operation/architecture"}}},{"node":{"name":"ArchitectureOne","path":"/operation/architecture/architecture-one/","fileInfo":{"name":"ArchitectureOne","directory":"operation/architecture"}}},{"node":{"name":"ArchitectureThree","path":"/operation/architecture/architecture-three/","fileInfo":{"name":"ArchitectureThree","directory":"operation/architecture"}}},{"node":{"name":"ArchitectureFour","path":"/operation/architecture/architecture-four/","fileInfo":{"name":"ArchitectureFour","directory":"operation/architecture"}}},{"node":{"name":"ArchitectureFive","path":"/operation/architecture/architecture-five/","fileInfo":{"name":"ArchitectureFive","directory":"operation/architecture"}}},{"node":{"name":"index","path":"/operation/analysis/","fileInfo":{"name":"index","directory":"operation/analysis"}}},{"node":{"name":"AnalysisDecomposition","path":"/operation/analysis/analysis-decomposition/","fileInfo":{"name":"AnalysisDecomposition","directory":"operation/analysis"}}},{"node":{"name":"AnalysisOne","path":"/operation/analysis/analysis-one/","fileInfo":{"name":"AnalysisOne","directory":"operation/analysis"}}},{"node":{"name":"AnalysisTwo","path":"/operation/analysis/analysis-two/","fileInfo":{"name":"AnalysisTwo","directory":"operation/analysis"}}},{"node":{"name":"AnalysisThree","path":"/operation/analysis/analysis-three/","fileInfo":{"name":"AnalysisThree","directory":"operation/analysis"}}}]}},"context":{"pathRegexp":"^$path.+$"}}